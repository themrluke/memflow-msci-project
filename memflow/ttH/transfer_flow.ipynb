{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f754e5-5310-4717-8700-8dc85ecf889f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7bc2a-42e5-48ff-8ddd-a01ed240d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import dask\n",
    "\n",
    "import vector\n",
    "import particle\n",
    "import hepunits\n",
    "\n",
    "import comet_ml\n",
    "import zuko\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from memflow.dataset.data import RootData,ParquetData\n",
    "from memflow.dataset.dataset import CombinedDataset\n",
    "from memflow.ttH.ttH_dataclasses import ttHHardDataset, ttHRecoDataset\n",
    "from memflow.models.transfer_flow_model import TransferFlow\n",
    "from memflow.models.custom_flows import *\n",
    "from memflow.callbacks.transfer_flow_callbacks import SamplingCallback, BiasCallback\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Change \"<n>\" to the index of the GPU you want to use on node\n",
    "\n",
    "print (f\"Running on GPU : {torch.cuda.is_available()}\")\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "print (f\"Accelerator : {accelerator}\")\n",
    "torch.set_float32_matmul_precision('medium')  \n",
    "if accelerator =='gpu':\n",
    "    torch.cuda.empty_cache()\n",
    "    print (torch.cuda.memory_summary(device=None, abbreviated=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c50d80-9342-47c6-9efd-87c39a91bc5b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a68956-c2a9-4fbd-b11a-9025f8339cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_hard = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "        #'all_jets_fullRun2_ttHbb_forTraining_allyears_spanetprov_part1_validation.parquet',\n",
    "        #'all_jets_fullRun2_ttHTobb_forTraining_2016_PreVFP_v3.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    N = int(1e5),\n",
    ")\n",
    "\n",
    "print (data_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35dfc61-246f-444b-930a-e5cdd77fd3c9",
   "metadata": {},
   "source": [
    "# Hard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd35299-f40b-4864-9132-67a95e03acd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hard_dataset = ttHHardDataset(\n",
    "    data = data_hard,\n",
    "    selection = [\n",
    "        # 'higgs',\n",
    "        # 'tops',\n",
    "        # 'bottoms',\n",
    "        # 'Ws',\n",
    "        # 'Zs',\n",
    "        'quarks',\n",
    "        'neutrinos',\n",
    "    ],\n",
    "    build = True,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "print(hard_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19d008-7658-4b9e-b3c1-463ec3401a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Before preprocessing')\n",
    "hard_dataset.plot(selection=True,raw=True)\n",
    "print ('After preprocessing')\n",
    "hard_dataset.plot(selection=True,raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47776673-da9c-4c0d-b92d-39a73349df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not strictly necessary, but just to make sure loading works as expected\n",
    "# We will use later a combined dataset (hard+reco) below\n",
    "hard_loader = DataLoader(\n",
    "    hard_dataset,\n",
    "    batch_size = 32,\n",
    ")\n",
    "batch = next(iter(hard_loader))\n",
    "\n",
    "for obj,mask,sel in zip(batch['data'],batch['mask'],hard_loader.dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c82aff-8a32-4f83-b199-0c7725639eda",
   "metadata": {},
   "source": [
    "# Reco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc523344",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reco = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    N = data_hard.N,\n",
    ")\n",
    "\n",
    "print (data_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781dae8-05ad-462e-b91a-bc544337a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_dataset = ttHRecoDataset(\n",
    "    data = data_reco,\n",
    "    selection = [\n",
    "        'jets',\n",
    "        'met',\n",
    "    ],\n",
    "    build = True,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "print(reco_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0ac35-78bf-45a7-ab31-d1d8884f4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Before preprocessing')\n",
    "reco_dataset.plot(selection=True,raw=True,log=True)\n",
    "print ('After preprocessing')\n",
    "reco_dataset.plot(selection=True,raw=False,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290aa9f-1bed-4c0e-b461-73f2c465db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also not needed, just checking \n",
    "reco_loader = DataLoader(\n",
    "    reco_dataset,\n",
    "    batch_size = 32,\n",
    ")\n",
    "batch = next(iter(reco_loader))\n",
    "\n",
    "for obj,mask,sel in zip(batch['data'],batch['mask'],reco_loader.dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc702e-66b1-450a-86d0-a28192f99d98",
   "metadata": {},
   "source": [
    "# Combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a63d7-eafc-43ef-8147-d0887d4bceec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(reco_dataset.intersection_branch)\n",
    "print (hard_dataset.metadata.keys())\n",
    "print (reco_dataset.metadata.keys())\n",
    "\n",
    "combined_dataset = CombinedDataset(\n",
    "    hard_dataset=hard_dataset,\n",
    "    reco_dataset=reco_dataset,\n",
    ")\n",
    "print (combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acc7c1-02e8-4c7f-8354-69090de0a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_loader = DataLoader(\n",
    "    combined_dataset,\n",
    "    batch_size = 256,\n",
    ")\n",
    "batch = next(iter(combined_loader))\n",
    "\n",
    "print ('Reco')\n",
    "for obj,mask,sel in zip(batch['reco']['data'],batch['reco']['mask'],combined_loader.dataset.reco_dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)\n",
    "print ('Hard')\n",
    "for obj,mask,sel in zip(batch['hard']['data'],batch['hard']['mask'],combined_loader.dataset.hard_dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d4124-4353-4233-a521-81f2b94f9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation\n",
    "# Not randomly for reproducilibility, but just based on number\n",
    "\n",
    "train_frac = 0.8\n",
    "indices = torch.arange(len(combined_dataset))\n",
    "sep = int(train_frac*len(combined_dataset))\n",
    "train_indices = indices[:sep]\n",
    "valid_indices = indices[sep:]\n",
    "\n",
    "combined_dataset_train = torch.utils.data.Subset(combined_dataset,train_indices)\n",
    "combined_dataset_valid = torch.utils.data.Subset(combined_dataset,valid_indices)\n",
    "print (f'Dataset : training {len(combined_dataset_train)} / validation {len(combined_dataset_valid)}')\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "combined_loader_train = DataLoader(\n",
    "    combined_dataset_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    ")\n",
    "combined_loader_valid = DataLoader(\n",
    "    combined_dataset_valid,\n",
    "    batch_size = 10000,\n",
    "    shuffle = False,\n",
    ")\n",
    "print (f'Batching {len(combined_loader_train)} / Validation {len(combined_loader_valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01df4426-506d-4108-916a-27a2aff5ffd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Find some indices in the validation set with max number of jets\n",
    "# # To use in the sampling to see a maximum number of jets\n",
    "# count = ak.count(data_reco['jets'].pt,axis=1).to_numpy()\n",
    "# mask_max = count == ak.max(count)\n",
    "# mask_valid = np.full(len(count),fill_value=False)\n",
    "# mask_valid[valid_indices] = True\n",
    "# idx_max = np.where(\n",
    "#     np.logical_and(mask_max,mask_valid)\n",
    "# )[0]\n",
    "# for i in idx_max:\n",
    "#     prov = data_reco['jets'].prov[i]\n",
    "#     unique, counts = np.unique(prov, return_counts=True)\n",
    "#     print ('idx',i,', '.join([f'prov {u:.0f} x {c:.0f}' for u,c in zip(unique, counts)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e440a8-85ba-4044-8f44-e094e6a58347",
   "metadata": {},
   "source": [
    "# Transfer flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_dataset.reco_dataset.input_features) # Determine the available features that you can select in ` flow_input_features`\n",
    "# Note the length of this (2 elements) must match the length of  `flow_input_features`\n",
    "# But you dont have to select all the features in each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a9e23-d4b1-42d6-8577-645174eb6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer flow #\n",
    "model = TransferFlow(\n",
    "    # General args #\n",
    "    dropout = 0.,\n",
    "    # Embedding arguments #\n",
    "    embed_dims = [32,64],\n",
    "    embed_act = nn.GELU,\n",
    "    # Particle features, names, masks, and number for printouts and logging #\n",
    "    n_hard_particles_per_type = combined_dataset.hard_dataset.number_particles_per_type,\n",
    "    hard_particle_type_names = combined_dataset.hard_dataset.selection,\n",
    "    hard_input_features_per_type = combined_dataset.hard_dataset.input_features,\n",
    "    n_reco_particles_per_type = combined_dataset.reco_dataset.number_particles_per_type,\n",
    "    reco_particle_type_names = combined_dataset.reco_dataset.selection,\n",
    "    reco_input_features_per_type = combined_dataset.reco_dataset.input_features,\n",
    "    flow_input_features = [ # features to be used in the flows (different from the tranformer)\n",
    "        # ['pt','eta','phi'], # leptons\n",
    "        ['pt','eta','phi'], # jets\n",
    "        ['pt','phi'],        # met\n",
    "\n",
    "    ],\n",
    "    hard_mask_attn = None,\n",
    "    reco_mask_attn = combined_dataset.reco_dataset.attention_mask,\n",
    "    # Transformer arguments #\n",
    "    onehot_encoding = False, # add onehot encoded position vector to particles\n",
    "    transformer_args = { # to be passed to the Transformer pytorch class\n",
    "        'nhead' : 8,\n",
    "        'num_encoder_layers' : 8, \n",
    "        'num_decoder_layers' : 8, \n",
    "        'dim_feedforward' : 128, \n",
    "        'activation' : 'gelu', \n",
    "    },\n",
    "    # Flow args #\n",
    "    flow_common_args = { # common args for all flows\n",
    "        'bins' : 16,\n",
    "        'transforms' : 5,\n",
    "        'randperm' : True,\n",
    "        'passes' : None,\n",
    "        'hidden_features' : [128] * 2,   \n",
    "    },\n",
    "    flow_classes = { # classes for each feature\n",
    "        'pt'  : zuko.flows.NSF,\n",
    "        'eta' : UniformNSF,\n",
    "        'phi' : UniformNCSF,\n",
    "    },\n",
    "    flow_specific_args = { # specific args for each class above\n",
    "        'eta' : {'bound' : 1.},\n",
    "        'phi' : {'bound' : 1.},\n",
    "    },\n",
    "    flow_mode = 'global', # 'global', 'type' or 'particle'\n",
    ")\n",
    "model = model.cpu()\n",
    "\n",
    "# Just check the model before training #\n",
    "batch = next(iter(combined_loader_train))\n",
    "\n",
    "log_probs, mask, weights = model(batch)\n",
    "mask = mask > 0\n",
    "print ('log_probs',log_probs,log_probs.shape)\n",
    "print ('mask',mask,mask.shape)\n",
    "print ('weights',weights,weights.shape)\n",
    "\n",
    "log_probs_tot = model.shared_eval(batch,0,'test')\n",
    "print ('tot log probs',log_probs_tot)\n",
    "\n",
    "samples = model.sample(batch['hard']['data'],batch['hard']['mask'],batch['reco']['data'],batch['reco']['mask'],N=100)\n",
    "print ('samples')\n",
    "for sample in samples:\n",
    "    print ('\\t',sample.shape)\n",
    "\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e711b53c-0d71-4ad0-819a-90197bf4ad9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Preprocess the dataset to include only the first 2 jets\n",
    "# print(combined_dataset_valid.keys())\n",
    "# print(type(combined_dataset_valid['reco']))\n",
    "# print(combined_dataset_valid['reco'])\n",
    "# # Limit to the first 2 jets in the 'data' key\n",
    "# for i, tensor in enumerate(combined_dataset_valid['reco']['data']):\n",
    "#     combined_dataset_valid['reco']['data'][i] = tensor[:2]  # Keep only the first 2 jets\n",
    "# print(combined_dataset_valid['reco']['data'])\n",
    "# MODIFY EVENTS IN CLASS WHEN SELECTING EVENTS (EG EVENTS THAT HAVE 1 B JET OR 5 JETS, ORDER IN BTAG OR PT)\n",
    "\n",
    "# Callbacks to make plots within comet\n",
    "bias = BiasCallback(\n",
    "    dataset = combined_dataset_valid,               # dataset on which to evaluate bias\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing, # preprocessing pipeline to draw raw variables\n",
    "    N_sample = 100,                                 # number of samples to draw\n",
    "    frequency = 20,                                 # plotting frequency (epochs)\n",
    "    raw = True,\n",
    "    bins = 101,                                      # 1D/2D plot number of bins\n",
    "    points = 20,                                    # Number of points for the quantile\n",
    "    log_scale = True,                               # log\n",
    "    batch_size = 50000,                              # Batch size to evaluate the dataset (internally makes a loaded)\n",
    "    N_batch = 1,                                   # Stop after N batches (makes it faster)\n",
    "    suffix = 'ttH',                                 # name for plots\n",
    "    label_names = {                                 # makes nicer labels\n",
    "        'pt' : 'p_T',\n",
    "        'eta' : '\\eta',\n",
    "        'phi' : '\\phi',\n",
    "    },\n",
    ")\n",
    "figs = bias.make_bias_plots(model.cuda(),show=True) # show is to plot standalone \n",
    "# of course now they are bad, need to train first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "771869b9-5cb2-41f6-b482-781b8a39ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = SamplingCallback(\n",
    "    dataset = combined_dataset_valid,           # dataset to check sampling\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing, # preprocessing pipeline\n",
    "    idx_to_monitor = [0,2,],               # idx of events in dataset to make plots with\n",
    "    N_sample = 100000,                         # number of samples to draw\n",
    "    frequency = 10,                             # plotting frequency (epochs)\n",
    "    bins = 51,                                  # 1D/2D plot number of bins\n",
    "    log_scale = True,                           # log\n",
    "    label_names = {                             # makes nicer labels\n",
    "        'pt' : 'p_T',\n",
    "        'eta' : '\\eta',\n",
    "        'phi' : '\\phi',\n",
    "    },\n",
    ")\n",
    "figs = sampling.make_sampling_plots(model.cuda(),show=True) # show is to plot standalone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de57283-b337-4b46-b17f-e8aa68e7b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Parameters #####\n",
    "epochs = 40\n",
    "steps_per_epoch_train = math.ceil(len(combined_dataset_train)/combined_loader_train.batch_size)\n",
    "\n",
    "print (f'Training   : Batch size = {combined_loader_train.batch_size} => {steps_per_epoch_train} steps per epoch')\n",
    "##### Optimizer #####\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "##### Scheduler #####\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer = optimizer,\n",
    "    mode='min', \n",
    "    factor=0.1, \n",
    "    patience=10, \n",
    "    threshold=0.001, \n",
    "    threshold_mode='rel', \n",
    "    cooldown=0, \n",
    "    min_lr=1e-7\n",
    ")\n",
    "model.set_scheduler_config(\n",
    "    {\n",
    "        'scheduler' : scheduler,\n",
    "        'interval' : 'step' if isinstance(scheduler,optim.lr_scheduler.OneCycleLR) else 'epoch',\n",
    "        'frequency' : 1,\n",
    "        'monitor' : 'val/loss_tot',\n",
    "        'strict' : True,\n",
    "        'name' : 'scheduler',\n",
    "    }\n",
    ")\n",
    "\n",
    "##### Callbacks #####\n",
    "callbacks = [\n",
    "    L.pytorch.callbacks.LearningRateMonitor(logging_interval = 'epoch'),\n",
    "    L.pytorch.callbacks.ModelSummary(max_depth=2),\n",
    "    sampling,\n",
    "    bias,\n",
    "]\n",
    "\n",
    "##### Logger #####\n",
    "logger = pl_loggers.CometLogger(\n",
    "    save_dir = '../comet_logs',\n",
    "    project_name = 'mem-flow-ttH',\n",
    "    experiment_name = 'combined',\n",
    "    offline = False,\n",
    ") \n",
    "logger.log_graph(model)\n",
    "logger.experiment.log_notebook(filename='transfer_flow.ipynb',overwrite=True)\n",
    "\n",
    "##### Trainer #####\n",
    "trainer = L.Trainer(\n",
    "    min_epochs = 5,\n",
    "    max_epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    devices = 'auto',\n",
    "    accelerator = accelerator,\n",
    "    logger = logger,\n",
    "    log_every_n_steps = steps_per_epoch_train/100,\n",
    ")\n",
    "##### Fit #####\n",
    "trainer.fit(\n",
    "    model = model,\n",
    "    train_dataloaders = combined_loader_train,\n",
    "    val_dataloaders = combined_loader_valid,\n",
    ")\n",
    "\n",
    "logger.experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613cafb-022b-4f7a-8479-2ee0be57d854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figs = sampling.make_sampling_plots(model.cuda(),show=True) # show is to plot standalone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513012b9-4bad-490b-8b4a-f06e24928b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = bias.make_bias_plots(model.cuda(),show=True) # show is to plot standalone "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
