{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f754e5-5310-4717-8700-8dc85ecf889f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7bc2a-42e5-48ff-8ddd-a01ed240d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import dask\n",
    "\n",
    "import vector\n",
    "import particle\n",
    "import hepunits\n",
    "\n",
    "import comet_ml\n",
    "import zuko\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from memflow.dataset.data import ParquetData\n",
    "from memflow.dataset.dataset import CombinedDataset\n",
    "from memflow.ttH.ttH_dataclasses import ttHHardDataset, ttHRecoDataset\n",
    "from memflow.callbacks.transfer_flow_callbacks import SamplingCallback, BiasCallback\n",
    "\n",
    "from models.conditional_flow_matching import TransferCFM\n",
    "from models.utils import plot_trajectories, compare_distributions\n",
    "from models.callbacks import CFMSamplingCallback\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Change \"<n>\" to the index of the GPU you want to use on node\n",
    "\n",
    "print (f\"Running on GPU : {torch.cuda.is_available()}\")\n",
    "accelerator = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (f\"Accelerator : {accelerator}\")\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "if accelerator =='cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print (torch.cuda.memory_summary(device=None, abbreviated=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c50d80-9342-47c6-9efd-87c39a91bc5b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a68956-c2a9-4fbd-b11a-9025f8339cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_hard = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "        #'all_jets_fullRun2_ttHbb_forTraining_allyears_spanetprov_part1_validation.parquet',\n",
    "        #'all_jets_fullRun2_ttHTobb_forTraining_2016_PreVFP_v3.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    N = int(1e5),\n",
    ")\n",
    "\n",
    "print (data_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35dfc61-246f-444b-930a-e5cdd77fd3c9",
   "metadata": {},
   "source": [
    "# Hard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd35299-f40b-4864-9132-67a95e03acd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hard_dataset = ttHHardDataset(\n",
    "    data = data_hard,\n",
    "    selection = [\n",
    "        # 'higgs',\n",
    "        # 'tops',\n",
    "        'bottoms',\n",
    "        # 'Ws',\n",
    "        # 'Zs',\n",
    "        'quarks',\n",
    "        'neutrinos',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "\n",
    "obj = hard_dataset.objects['neutrinos']\n",
    "\n",
    "# 1) Check its type\n",
    "print(\"Type of neutrinos object:\", type(obj))\n",
    "\n",
    "# 2) If itâ€™s a tuple, print its length\n",
    "if isinstance(obj, tuple):\n",
    "    print(\"Length of tuple:\", len(obj))\n",
    "    for i, element in enumerate(obj):\n",
    "        print(f\"Element {i} has type {type(element)} and shape/length:\", end=\" \")\n",
    "        # If element is a torch.Tensor or numpy array:\n",
    "        if hasattr(element, 'shape'):\n",
    "            print(element.shape)\n",
    "        # If it's a list of fields, you might just print:\n",
    "        else:\n",
    "            print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19d008-7658-4b9e-b3c1-463ec3401a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Before preprocessing')\n",
    "hard_dataset.plot(selection=True,raw=True)\n",
    "print ('After preprocessing')\n",
    "hard_dataset.plot(selection=True,raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47776673-da9c-4c0d-b92d-39a73349df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not strictly necessary, but just to make sure loading works as expected\n",
    "# We will use later a combined dataset (hard+reco) below\n",
    "hard_loader = DataLoader(\n",
    "    hard_dataset,\n",
    "    batch_size = 32,\n",
    ")\n",
    "batch = next(iter(hard_loader))\n",
    "\n",
    "for obj,mask,sel in zip(batch['data'],batch['mask'],hard_loader.dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c82aff-8a32-4f83-b199-0c7725639eda",
   "metadata": {},
   "source": [
    "# Reco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc523344",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reco = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    N = data_hard.N,\n",
    ")\n",
    "\n",
    "print (data_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781dae8-05ad-462e-b91a-bc544337a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_dataset = ttHRecoDataset(\n",
    "    data = data_reco,\n",
    "    selection = [\n",
    "        'jets',\n",
    "        'met',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "print(reco_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253911cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Dataset data keys:\")\n",
    "# print(reco_dataset.data.keys())  # List all branches in the dataset\n",
    "\n",
    "# print(\"Jets branch (first 5):\")\n",
    "# print(reco_dataset.data[\"cleanedJet_pt\"][:5])\n",
    "\n",
    "# print(\"Registered objects:\")\n",
    "# print(reco_dataset.objects)\n",
    "\n",
    "print(\"JETS:\")\n",
    "print(len(reco_dataset.objects['jets'][0])) #Dims: [[event], [jet number], [pt, eta, phi, mass, btag]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0ac35-78bf-45a7-ab31-d1d8884f4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Before preprocessing')\n",
    "reco_dataset.plot(selection=True,raw=True,log=True)\n",
    "print ('After preprocessing')\n",
    "reco_dataset.plot(selection=True,raw=False,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290aa9f-1bed-4c0e-b461-73f2c465db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also not needed, just checking \n",
    "reco_loader = DataLoader(\n",
    "    reco_dataset,\n",
    "    batch_size = 32,\n",
    ")\n",
    "batch = next(iter(reco_loader))\n",
    "\n",
    "for obj,mask,sel in zip(batch['data'],batch['mask'],reco_loader.dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc702e-66b1-450a-86d0-a28192f99d98",
   "metadata": {},
   "source": [
    "# Combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a63d7-eafc-43ef-8147-d0887d4bceec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Intersection Branch: {reco_dataset.intersection_branch}')\n",
    "print (f'Hard Datset keys: {hard_dataset.metadata.keys()}')\n",
    "print (f'Reco Datset keys: {reco_dataset.metadata.keys()}')\n",
    "\n",
    "combined_dataset = CombinedDataset(\n",
    "    hard_dataset=hard_dataset,\n",
    "    reco_dataset=reco_dataset,\n",
    ")\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acc7c1-02e8-4c7f-8354-69090de0a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_loader = DataLoader(\n",
    "    combined_dataset,\n",
    "    batch_size = 256,\n",
    ")\n",
    "batch = next(iter(combined_loader))\n",
    "\n",
    "print ('Reco')\n",
    "for obj,mask,sel in zip(batch['reco']['data'],batch['reco']['mask'],combined_loader.dataset.reco_dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)\n",
    "\n",
    "print ('Hard')\n",
    "for obj,mask,sel in zip(batch['hard']['data'],batch['hard']['mask'],combined_loader.dataset.hard_dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d4124-4353-4233-a521-81f2b94f9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation\n",
    "# Not randomly for reproducilibility, but just based on number\n",
    "\n",
    "train_frac = 0.8\n",
    "indices = torch.arange(len(combined_dataset))\n",
    "sep = int(train_frac*len(combined_dataset))\n",
    "train_indices = indices[:sep]\n",
    "valid_indices = indices[sep:]\n",
    "\n",
    "combined_dataset_train = torch.utils.data.Subset(combined_dataset,train_indices)\n",
    "combined_dataset_valid = torch.utils.data.Subset(combined_dataset,valid_indices)\n",
    "print (f'Dataset : training {len(combined_dataset_train)} / validation {len(combined_dataset_valid)}')\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "combined_loader_train = DataLoader(\n",
    "    combined_dataset_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    ")\n",
    "combined_loader_valid = DataLoader(\n",
    "    combined_dataset_valid,\n",
    "    batch_size = 10000,\n",
    "    shuffle = False,\n",
    ")\n",
    "print (f'Batching {len(combined_loader_train)} / Validation {len(combined_loader_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e440a8-85ba-4044-8f44-e094e6a58347",
   "metadata": {},
   "source": [
    "# TransferCFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_dataset.hard_dataset.input_features) # Determine the available features that you can select in ` flow_input_features`\n",
    "# Note the length of this (2 elements) must match the length of  `flow_input_features`\n",
    "# But you dont have to select all the features in each element\n",
    "print(combined_dataset.reco_dataset.input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbfe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (obj, mask) in enumerate(zip(batch['reco']['data'], batch['reco']['mask'])):\n",
    "    print(f\"Reco Object {i}: Shape = {obj.shape}, Mask Shape = {mask.shape}\")\n",
    "for i, (obj, mask) in enumerate(zip(batch['hard']['data'], batch['hard']['mask'])):\n",
    "    print(f\"Hard Object {i}: Shape = {obj.shape}, Mask Shape = {mask.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4980367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CFM model\n",
    "model = TransferCFM(\n",
    "    embed_dims=[32, 64],\n",
    "    embed_act=nn.GELU,\n",
    "    dropout=0.0,\n",
    "\n",
    "    n_hard_particles_per_type=combined_dataset.hard_dataset.number_particles_per_type,\n",
    "    hard_particle_type_names=combined_dataset.hard_dataset.selection,\n",
    "    hard_input_features_per_type=combined_dataset.hard_dataset.input_features, # These are all the features available, speciefied in dataclass\n",
    "\n",
    "    n_reco_particles_per_type=combined_dataset.reco_dataset.number_particles_per_type,\n",
    "    reco_particle_type_names= combined_dataset.reco_dataset.selection,\n",
    "    reco_input_features_per_type=combined_dataset.reco_dataset.input_features,\n",
    "\n",
    "    # Only pick a subset in bridging:\n",
    "    flow_input_features = [\n",
    "        [\"pt\", \"eta\", \"phi\"],  # Features for reco type 0 (e.g., jets)\n",
    "        [\"pt\", \"phi\"],         # Features for reco type 1 (e.g., MET)\n",
    "        # Add more reco types as needed\n",
    "    ],\n",
    "\n",
    "\n",
    "    hard_mask_attn=None,\n",
    "    reco_mask_attn=reco_dataset.attention_mask,\n",
    "    transformer_args={\n",
    "        \"nhead\": 8,\n",
    "        \"num_encoder_layers\": 8,\n",
    "        \"num_decoder_layers\": 8,\n",
    "        \"dim_feedforward\": 128,\n",
    "        \"activation\": \"gelu\",\n",
    "    },\n",
    "    sigma=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "# Quick test on one batch\n",
    "batch = next(iter(combined_loader_train))\n",
    "for i, (obj, mask) in enumerate(zip(batch[\"hard\"][\"data\"], batch[\"hard\"][\"mask\"])):\n",
    "    print(f\"hard_data[{i}] shape = {obj.shape}, mask shape = {mask.shape}\")\n",
    "for i, (obj, mask) in enumerate(zip(batch[\"reco\"][\"data\"], batch[\"reco\"][\"mask\"])):\n",
    "    print(f\"reco_data[{i}] shape = {obj.shape}, mask shape = {mask.shape}\")\n",
    "loss = model(batch)\n",
    "print(\"Initial CFM loss:\", loss.item())\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(batch['hard']['data']), len(batch['hard']['data'])) # len = 2 is for 2 particles\n",
    "print(type(batch['reco']['data']), len(batch['reco']['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b854dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks to make plots within comet\n",
    "bias = BiasCallback(\n",
    "    dataset = combined_dataset_valid,               # dataset on which to evaluate bias\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing, # preprocessing pipeline to draw raw variables\n",
    "    N_sample = 100,                                 # number of samples to draw\n",
    "    frequency = 1,                                 # plotting frequency (epochs)\n",
    "    raw = True,\n",
    "    bins = 101,                                      # 1D/2D plot number of bins\n",
    "    points = 20,                                    # Number of points for the quantile\n",
    "    log_scale = True,                               # log\n",
    "    batch_size = 1000,                              # Batch size to evaluate the dataset (internally makes a loaded)\n",
    "    N_batch = 1,                                   # Stop after N batches (makes it faster)\n",
    "    suffix = 'ttH',                                 # name for plots\n",
    "    label_names = {                                 # makes nicer labels\n",
    "        'pt' : 'p_T',\n",
    "        'eta' : '\\eta',\n",
    "        'phi' : '\\phi',\n",
    "    },\n",
    ")\n",
    "\n",
    "sampling = SamplingCallback(\n",
    "    dataset = combined_dataset_valid,           # dataset to check sampling\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing, # preprocessing pipeline\n",
    "    idx_to_monitor = [0,2],               # idx of events in dataset to make plots with\n",
    "    N_sample = 1000,                         # number of samples to draw\n",
    "    frequency = 1,                             # plotting frequency (epochs)\n",
    "    bins = 51,                                  # 1D/2D plot number of bins\n",
    "    log_scale = True,                           # log\n",
    "    label_names = {                             # makes nicer labels\n",
    "        'pt' : 'p_T',\n",
    "        'eta' : '\\eta',\n",
    "        'phi' : '\\phi',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d42c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Optimizer + scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    threshold=0.001,\n",
    "    threshold_mode='rel',\n",
    "    cooldown=0,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "model.set_scheduler_config({\n",
    "    'scheduler': scheduler,\n",
    "    'interval': 'epoch',\n",
    "    'frequency': 1,\n",
    "    'monitor': 'val_loss',  # <-- or 'val/loss_tot' if you prefer\n",
    "    'strict': True,\n",
    "    'name': 'scheduler',\n",
    "})\n",
    "\n",
    "# 5) Logger + Trainer\n",
    "logger = pl_loggers.CometLogger(\n",
    "    save_dir='../comet_logs',\n",
    "    project_name='mem-flow-ttH',\n",
    "    experiment_name='combined',\n",
    "    offline=False,\n",
    ")\n",
    "\n",
    "epochs = 11\n",
    "steps_per_epoch_train = math.ceil(len(combined_dataset_train)/combined_loader_train.batch_size)\n",
    "\n",
    "##### Callbacks #####\n",
    "callbacks = [\n",
    "    L.pytorch.callbacks.LearningRateMonitor(logging_interval = 'epoch'),\n",
    "    L.pytorch.callbacks.ModelSummary(max_depth=2),\n",
    "    sampling,\n",
    "    bias,\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    min_epochs=5,\n",
    "    max_epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    devices='auto',\n",
    "    accelerator='auto',\n",
    "    logger=logger,\n",
    "    log_every_n_steps=steps_per_epoch_train // 100,\n",
    ")\n",
    "\n",
    "# 6) Fit\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=combined_loader_train,\n",
    "    val_dataloaders=combined_loader_valid,\n",
    ")\n",
    "\n",
    "logger.experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf5d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, manually evaluate the model\n",
    "batch = next(iter(combined_loader_valid))\n",
    "\n",
    "# Move batch to the model's device manually\n",
    "batch = CFMSamplingCallback.move_batch_to_device(batch, model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = model.to(model.device)\n",
    "\n",
    "    # Extract the necessary inputs for sampling\n",
    "    hard_data = batch[\"hard\"][\"data\"]\n",
    "    hard_mask_exist = batch[\"hard\"][\"mask\"]\n",
    "    reco_data = batch[\"reco\"][\"data\"]\n",
    "    reco_mask_exist = batch[\"reco\"][\"mask\"]\n",
    "\n",
    "    # Generate samples from the trained model\n",
    "    gen_data_list = model.sample(hard_data, hard_mask_exist, reco_data, reco_mask_exist, steps=10)\n",
    "\n",
    "# Extract real and generated data for comparison\n",
    "real_data = batch[\"reco\"][\"data\"][0]  # Jets data\n",
    "gen_data  = gen_data_list[0]         # Generated jets data\n",
    "\n",
    "# Ensure correct shape if necessary\n",
    "if real_data.dim() == 4:  # If shape is (N_sample, B, P, F)\n",
    "    real_data = real_data[0]  # Select the first sample\n",
    "if gen_data.dim() == 4:\n",
    "    gen_data = gen_data[0]  # Select the first sample\n",
    "\n",
    "# Compare distributions\n",
    "compare_distributions(real_data, gen_data, feat_idx=0, feat_name=\"pt\")\n",
    "compare_distributions(real_data, gen_data, feat_idx=1, feat_name=\"eta\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
