{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import dask\n",
    "import vector\n",
    "import particle\n",
    "import hepunits\n",
    "\n",
    "import zuko\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from memflow.dataset.data import RootData,ParquetData\n",
    "from dataset2 import AcceptanceDataset\n",
    "from memflow.ttH.classifier.classifier_models import *\n",
    "\n",
    "from ttH_dataclasses_acceptance import ttHHardDataset, ttHRecoDataset\n",
    "\n",
    "from memflow.ttH.classifier.classifier_callbacks import *\n",
    "from memflow.ttH.models.callbacks import ModelCheckpoint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 100})\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "print (f\"Running on GPU : {torch.cuda.is_available()}\")\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "print (f\"Accelerator : {accelerator}\")\n",
    "torch.set_float32_matmul_precision('medium')  \n",
    "if accelerator =='gpu':\n",
    "    torch.cuda.empty_cache()\n",
    "    print (torch.cuda.memory_summary(device=None, abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hard = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    ")\n",
    "data_reco = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    ")\n",
    "print(data_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_dataset = ttHHardDataset(\n",
    "    data = data_hard,\n",
    "    selection = [\n",
    "        # 'higgs',\n",
    "        # 'tops',\n",
    "        'bottoms',\n",
    "        # 'Ws',\n",
    "        # 'Zs',\n",
    "        'quarks',\n",
    "        'neutrinos',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "print(hard_dataset)\n",
    "\n",
    "reco_dataset = ttHRecoDataset(\n",
    "    data = data_reco,\n",
    "    selection = [\n",
    "        'jets',\n",
    "        'met',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "print(reco_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AcceptanceDataset(\n",
    "    hard_dataset = hard_dataset,\n",
    "    reco_dataset = reco_dataset,\n",
    ")\n",
    "\n",
    "train_frac = 0.8 # from 0.7\n",
    "indices = torch.arange(len(dataset))\n",
    "sep = int(train_frac*len(dataset))\n",
    "train_indices = indices[:sep]\n",
    "valid_indices = indices[sep:]\n",
    "\n",
    "dataset_train = torch.utils.data.Subset(dataset,train_indices)\n",
    "dataset_valid = torch.utils.data.Subset(dataset,valid_indices)\n",
    "\n",
    "print (f'Dataset : training {len(dataset_train)} / validation {len(dataset_valid)}')\n",
    "\n",
    "batch_size = 1024 # from 1024\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    ")\n",
    "loader_valid = DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size = 10000,\n",
    "    shuffle = False,\n",
    ")\n",
    "print (f'Batching {len(loader_train)} / Validation {len(loader_valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before Preprocessing\")\n",
    "dataset.hard_dataset.plot(selection=True, raw=True,log=False)\n",
    "print(\"After prerocessing\")\n",
    "dataset.hard_dataset.plot(selection=True, raw=False,log=False,fields_to_plot=['pt','eta','phi','mass','pdgid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ttH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a model that can classify between selected (1) and not selected (0)\n",
    "# We can either use a fully connected network\n",
    "# backbone : DNN\n",
    "# head : DNN\n",
    "\n",
    "# backbone = BaseMLP(\n",
    "#     dim_in = dataset.flatten_dim,\n",
    "#     dim_out = None,\n",
    "#     neurons = [64]*7, # changed from [64]*3\n",
    "#     hidden_activation = nn.GELU,\n",
    "#     batch_norm = True,\n",
    "# )\n",
    "# head = BaseMLP(\n",
    "#     dim_in = 64, # changed from 64 to match the last layer of the backbone\n",
    "#     dim_out = 1,\n",
    "#     output_activation = nn.Sigmoid,\n",
    "#     batch_norm = True,\n",
    "# )\n",
    "\n",
    "# Or we can use\n",
    "# backbone : transformer (+ mean pooling)\n",
    "# head : DNN\n",
    "\n",
    "backbone = BaseTransformerEncoder(\n",
    "    n_particles_per_type = dataset.hard_dataset.number_particles_per_type,\n",
    "    particle_type_names = dataset.hard_dataset.selection,\n",
    "    input_features_per_type = dataset.hard_dataset.input_features,\n",
    "    embed_dims = [64,256],\n",
    "    activation = nn.GELU,\n",
    "    num_layers = 6, # From 4\n",
    "    nhead = 8, # From 4\n",
    "    dim_feedforward = 512, # From 256\n",
    "    layer_norm = True,\n",
    "    dropout = 0.2, # From 0.1\n",
    ")\n",
    "head = BaseMLP(\n",
    "    dim_in = 256,\n",
    "    neurons = [128, 64, 64, 32], # changed from [128, 64, 32]\n",
    "    dim_out = 1,\n",
    "    output_activation = nn.Sigmoid,\n",
    "    batch_norm = True,\n",
    ")\n",
    "\n",
    "# Combine the backbone and head into classifier\n",
    "#weights = torch.tensor([2.0]) # new line, used because of class imbalance between unselected and selected events, makes the mistakes on minority class (selected) more expensive as its 2.0 compared to 1.0\n",
    "model = Classifier(\n",
    "    backbone = backbone,\n",
    "    head = head,\n",
    "    loss_function = nn.BCELoss(reduce='none')#nn.BCELoss(weight=weights, reduction='none'), # Was loss_function = nn.BCELoss(reduce='none')\n",
    ")\n",
    "print (model)\n",
    "\n",
    "batch = next(iter(loader_train))\n",
    "\n",
    "out = model(batch)\n",
    "print (out.shape)\n",
    "\n",
    "loss = model.shared_eval(batch,0,'test')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptance = AcceptanceCallback(\n",
    "    # Boilerplate arguments #\n",
    "    dataset = dataset_valid,\n",
    "    selection = dataset.hard_dataset.selection,\n",
    "    features_per_type = dataset.hard_dataset.input_features,\n",
    "    preprocessing = dataset.hard_dataset.preprocessing,\n",
    "    # Plotting arguments #\n",
    "    frequency = 20,     # frequency of plotting as callback\n",
    "    raw = True,        # undo preprocessing (see raw values)\n",
    "    bins = 50,         # number of bins for histograms\n",
    "    batch_size=10000,  # number of events per batch for model evaluation\n",
    "    # N_batch = 1,     # cutdown number of batches used (to make it faster, optional)\n",
    "    min_selected_events_per_bin = { # rebinning option to make plots nicer (optional)\n",
    "        'pt'   : 10,\n",
    "        'eta'  : 10,\n",
    "        'phi'  : None,\n",
    "        'mass' : 10,\n",
    "        'pdgid': None,\n",
    "    },\n",
    "    label_names = { # rename option to make plot labels nicer (optional)\n",
    "        'pt'   : r'$p_T$',\n",
    "        'eta'  : r'$\\eta$',\n",
    "        'phi'  : r'$\\phi$',\n",
    "        'mass' : r'$M$',\n",
    "        'pdgid': r'PDG ID',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters #####\n",
    "epochs = 200\n",
    "steps_per_epoch_train = math.ceil(len(dataset_train)/loader_train.batch_size)\n",
    "\n",
    "print (f'Training   : Batch size = {loader_train.batch_size} => {steps_per_epoch_train} steps per epoch')\n",
    "##### Optimizer #####\n",
    "optimizer = optim.RAdam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "##### Scheduler #####\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer = optimizer,\n",
    "    mode = 'min', \n",
    "    factor = 0.5, \n",
    "    patience = 10, \n",
    "    threshold = 0., \n",
    "    threshold_mode = 'rel',  \n",
    "    cooldown = 0, \n",
    "    min_lr = 1e-7,\n",
    ")\n",
    "model.set_scheduler_config(\n",
    "    {\n",
    "        'scheduler' : scheduler,\n",
    "        'interval' : 'step' if isinstance(scheduler,optim.lr_scheduler.OneCycleLR) else 'epoch',\n",
    "        'frequency' : 1,\n",
    "        'monitor' : 'val/loss_tot',\n",
    "        'strict' : True, \n",
    "        'name' : 'scheduler',\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "##### Callbacks #####\n",
    "callbacks = [\n",
    "    L.pytorch.callbacks.LearningRateMonitor(logging_interval='epoch'),\n",
    "    L.pytorch.callbacks.ModelSummary(max_depth=2),\n",
    "    acceptance,\n",
    "    ModelCheckpoint(save_every_n_epochs=5, save_dir=\"trained_model_checkpoints/acceptance_checkpoints\")\n",
    "] \n",
    "\n",
    "##### Logger #####\n",
    "logger = pl_loggers.CometLogger(\n",
    "    save_dir = '../comet_logs',\n",
    "    project_name = 'mem-flow-ttH',\n",
    "    experiment_name = 'Acceptance',\n",
    "    offline = False,\n",
    ") \n",
    "logger.log_graph(model)\n",
    "# logger.log_hyperparams()\n",
    "# logger.experiment.log_code(folder='../src/')\n",
    "logger.experiment.log_notebook(filename='acceptance_process_DL.ipynb',overwrite=True)\n",
    "\n",
    "##### Trainer #####\n",
    "trainer = L.Trainer(    \n",
    "    min_epochs = 5,\n",
    "    max_epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    devices = 'auto',\n",
    "    accelerator = accelerator,\n",
    "    logger = logger,\n",
    "    log_every_n_steps = steps_per_epoch_train,\n",
    ")\n",
    "##### Fit #####\n",
    "trainer.fit(\n",
    "    model = model, \n",
    "    train_dataloaders = loader_train,\n",
    "    val_dataloaders = loader_valid,\n",
    "    ckpt_path=\"trained_model_checkpoints/acceptance_checkpoints/model_epoch_95.ckpt\" # Use to resume training from a checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptance_model = Classifier.load_from_checkpoint(checkpoint_path=\"trained_model_checkpoints/acceptance_checkpoints/model_epoch_95.ckpt\")\n",
    "acceptance_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptance.N_batch = np.inf # for final plots, want to make sure we use as much stats as possible\n",
    "figs = acceptance.make_plots(model=acceptance_model.cuda(),show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
