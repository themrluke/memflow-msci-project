{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef46420-2270-4607-adef-c26c3872b320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:15: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 12.0.1. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU workers for dataloading: 16\n",
      "Running on GPU : True\n",
      "Accelerator : cuda\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import dask\n",
    "import os\n",
    "import multiprocessing\n",
    "import vector\n",
    "import particle\n",
    "import hepunits\n",
    "\n",
    "# Deep learning imports\n",
    "import zuko # Flow based models\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import pytorch_lightning as L # PyTorch Lightning for training\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from memflow.dataset.data import RootData, ParquetData\n",
    "from memflow.dataset.dataset import HardDataset, RecoDataset, CombinedDataset\n",
    "\n",
    "from memflow.ttH.ttH_dataclasses import ttHHardDataset, ttHRecoDataset\n",
    "\n",
    "from transfer_flow.tools import *\n",
    "from transfer_flow.custom_flows import *\n",
    "from transfer_flow.transfer_flow_model import TransferFlow\n",
    "\n",
    "from transfer_flow.transfer_flow_callbacks import *\n",
    "from models.callbacks import ModelCheckpoint\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 100})\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "num_workers = min(16, multiprocessing.cpu_count())  # Use up to 16 CPU cores\n",
    "print(f'Number of CPU workers for dataloading: {num_workers}')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Change \"<n>\" to the index of the GPU you want to use on node\n",
    "\n",
    "print (f\"Running on GPU : {torch.cuda.is_available()}\")\n",
    "accelerator = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (f\"Accelerator : {accelerator}\")\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "if accelerator =='cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print (torch.cuda.memory_summary(device=None, abbreviated=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96384951-a140-49df-a732-7e0d870533eb",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d3adc1-595b-47ab-96cc-5fb37be86313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object\n",
      "Loaded branches:\n",
      "   ... file: 1903554\n",
      "   ... sample: 1903554\n",
      "   ... tree: 1903554\n",
      "Branch in files not loaded:\n",
      "   ... Generator_scalePDF\n",
      "   ... Generator_weight\n",
      "   ... Generator_x1\n",
      "   ... Generator_x2\n",
      "   ... Generator_xpdf1\n",
      "   ... Generator_xpdf2\n",
      "   ... W_minus_from_antitop_eta\n",
      "   ... W_minus_from_antitop_genPartIdxMother\n",
      "   ... W_minus_from_antitop_idx\n",
      "   ... W_minus_from_antitop_mass\n",
      "   ... W_minus_from_antitop_pdgId\n",
      "   ... W_minus_from_antitop_phi\n",
      "   ... W_minus_from_antitop_pt\n",
      "   ... W_minus_from_antitop_status\n",
      "   ... W_minus_from_antitop_statusFlags\n",
      "   ... W_plus_from_top_eta\n",
      "   ... W_plus_from_top_genPartIdxMother\n",
      "   ... W_plus_from_top_idx\n",
      "   ... W_plus_from_top_mass\n",
      "   ... W_plus_from_top_pdgId\n",
      "   ... W_plus_from_top_phi\n",
      "   ... W_plus_from_top_pt\n",
      "   ... W_plus_from_top_status\n",
      "   ... W_plus_from_top_statusFlags\n",
      "   ... Z_from_higgs_eta\n",
      "   ... Z_from_higgs_genPartIdxMother\n",
      "   ... Z_from_higgs_idx\n",
      "   ... Z_from_higgs_mass\n",
      "   ... Z_from_higgs_pdgId\n",
      "   ... Z_from_higgs_phi\n",
      "   ... Z_from_higgs_pt\n",
      "   ... Z_from_higgs_status\n",
      "   ... Z_from_higgs_statusFlags\n",
      "   ... antibottom_eta\n",
      "   ... antibottom_genPartIdxMother\n",
      "   ... antibottom_idx\n",
      "   ... antibottom_mass\n",
      "   ... antibottom_pdgId\n",
      "   ... antibottom_phi\n",
      "   ... antibottom_pt\n",
      "   ... antibottom_status\n",
      "   ... antibottom_statusFlags\n",
      "   ... antineutrino_from_W_minus_eta\n",
      "   ... antineutrino_from_W_minus_genPartIdxMother\n",
      "   ... antineutrino_from_W_minus_idx\n",
      "   ... antineutrino_from_W_minus_mass\n",
      "   ... antineutrino_from_W_minus_pdgId\n",
      "   ... antineutrino_from_W_minus_phi\n",
      "   ... antineutrino_from_W_minus_pt\n",
      "   ... antineutrino_from_W_minus_status\n",
      "   ... antineutrino_from_W_minus_statusFlags\n",
      "   ... antiquark_from_W_minus_eta\n",
      "   ... antiquark_from_W_minus_genPartIdxMother\n",
      "   ... antiquark_from_W_minus_idx\n",
      "   ... antiquark_from_W_minus_mass\n",
      "   ... antiquark_from_W_minus_pdgId\n",
      "   ... antiquark_from_W_minus_phi\n",
      "   ... antiquark_from_W_minus_pt\n",
      "   ... antiquark_from_W_minus_status\n",
      "   ... antiquark_from_W_minus_statusFlags\n",
      "   ... antiquark_from_W_plus_eta\n",
      "   ... antiquark_from_W_plus_genPartIdxMother\n",
      "   ... antiquark_from_W_plus_idx\n",
      "   ... antiquark_from_W_plus_mass\n",
      "   ... antiquark_from_W_plus_pdgId\n",
      "   ... antiquark_from_W_plus_phi\n",
      "   ... antiquark_from_W_plus_pt\n",
      "   ... antiquark_from_W_plus_status\n",
      "   ... antiquark_from_W_plus_statusFlags\n",
      "   ... antitop_eta\n",
      "   ... antitop_genPartIdxMother\n",
      "   ... antitop_idx\n",
      "   ... antitop_mass\n",
      "   ... antitop_pdgId\n",
      "   ... antitop_phi\n",
      "   ... antitop_pt\n",
      "   ... antitop_status\n",
      "   ... antitop_statusFlags\n",
      "   ... bottom_eta\n",
      "   ... bottom_genPartIdxMother\n",
      "   ... bottom_idx\n",
      "   ... bottom_mass\n",
      "   ... bottom_pdgId\n",
      "   ... bottom_phi\n",
      "   ... bottom_pt\n",
      "   ... bottom_status\n",
      "   ... bottom_statusFlags\n",
      "   ... event\n",
      "   ... higgs_eta\n",
      "   ... higgs_genPartIdxMother\n",
      "   ... higgs_idx\n",
      "   ... higgs_mass\n",
      "   ... higgs_pdgId\n",
      "   ... higgs_phi\n",
      "   ... higgs_pt\n",
      "   ... higgs_status\n",
      "   ... higgs_statusFlags\n",
      "   ... lep_minus_from_W_minus_eta\n",
      "   ... lep_minus_from_W_minus_genPartIdxMother\n",
      "   ... lep_minus_from_W_minus_idx\n",
      "   ... lep_minus_from_W_minus_mass\n",
      "   ... lep_minus_from_W_minus_pdgId\n",
      "   ... lep_minus_from_W_minus_phi\n",
      "   ... lep_minus_from_W_minus_pt\n",
      "   ... lep_minus_from_W_minus_status\n",
      "   ... lep_minus_from_W_minus_statusFlags\n",
      "   ... lep_plus_from_W_plus_eta\n",
      "   ... lep_plus_from_W_plus_genPartIdxMother\n",
      "   ... lep_plus_from_W_plus_idx\n",
      "   ... lep_plus_from_W_plus_mass\n",
      "   ... lep_plus_from_W_plus_pdgId\n",
      "   ... lep_plus_from_W_plus_phi\n",
      "   ... lep_plus_from_W_plus_pt\n",
      "   ... lep_plus_from_W_plus_status\n",
      "   ... lep_plus_from_W_plus_statusFlags\n",
      "   ... neutrino_from_W_plus_eta\n",
      "   ... neutrino_from_W_plus_genPartIdxMother\n",
      "   ... neutrino_from_W_plus_idx\n",
      "   ... neutrino_from_W_plus_mass\n",
      "   ... neutrino_from_W_plus_pdgId\n",
      "   ... neutrino_from_W_plus_phi\n",
      "   ... neutrino_from_W_plus_pt\n",
      "   ... neutrino_from_W_plus_status\n",
      "   ... neutrino_from_W_plus_statusFlags\n",
      "   ... neutrinos_from_Z_eta\n",
      "   ... neutrinos_from_Z_genPartIdxMother\n",
      "   ... neutrinos_from_Z_idx\n",
      "   ... neutrinos_from_Z_mass\n",
      "   ... neutrinos_from_Z_pdgId\n",
      "   ... neutrinos_from_Z_phi\n",
      "   ... neutrinos_from_Z_pt\n",
      "   ... neutrinos_from_Z_status\n",
      "   ... neutrinos_from_Z_statusFlags\n",
      "   ... quark_from_W_minus_eta\n",
      "   ... quark_from_W_minus_genPartIdxMother\n",
      "   ... quark_from_W_minus_idx\n",
      "   ... quark_from_W_minus_mass\n",
      "   ... quark_from_W_minus_pdgId\n",
      "   ... quark_from_W_minus_phi\n",
      "   ... quark_from_W_minus_pt\n",
      "   ... quark_from_W_minus_status\n",
      "   ... quark_from_W_minus_statusFlags\n",
      "   ... quark_from_W_plus_eta\n",
      "   ... quark_from_W_plus_genPartIdxMother\n",
      "   ... quark_from_W_plus_idx\n",
      "   ... quark_from_W_plus_mass\n",
      "   ... quark_from_W_plus_pdgId\n",
      "   ... quark_from_W_plus_phi\n",
      "   ... quark_from_W_plus_pt\n",
      "   ... quark_from_W_plus_status\n",
      "   ... quark_from_W_plus_statusFlags\n",
      "   ... top_eta\n",
      "   ... top_genPartIdxMother\n",
      "   ... top_idx\n",
      "   ... top_mass\n",
      "   ... top_pdgId\n",
      "   ... top_phi\n",
      "   ... top_pt\n",
      "   ... top_status\n",
      "   ... top_statusFlags\n"
     ]
    }
   ],
   "source": [
    "data_hard = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    # N = int(1e5),\n",
    ")\n",
    "\n",
    "print (data_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a55593c-2eca-43d8-8797-f811372444d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading objects from /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard\n",
      "Saving preprocessing to /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard\n",
      "Will overwrite what is in output directory /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard/preprocessing\n",
      "Preprocessing saved in /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard/preprocessing\n",
      "Type of neutrinos object: <class 'tuple'>\n",
      "Length of tuple: 3\n",
      "Element 0 has type <class 'torch.Tensor'> and shape/length: torch.Size([756642, 4, 5])\n",
      "Element 1 has type <class 'torch.Tensor'> and shape/length: torch.Size([756642, 4])\n",
      "Element 2 has type <class 'torch.Tensor'> and shape/length: torch.Size([756642, 4])\n"
     ]
    }
   ],
   "source": [
    "hard_dataset = ttHHardDataset(\n",
    "    data = data_hard,\n",
    "    selection = [\n",
    "        # 'higgs',\n",
    "        # 'tops',\n",
    "        'bottoms',\n",
    "        # 'Ws',\n",
    "        # 'Zs',\n",
    "        'quarks',\n",
    "        'neutrinos',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "\n",
    "obj = hard_dataset.objects['neutrinos']\n",
    "\n",
    "# Check its type\n",
    "print(\"Type of neutrinos object:\", type(obj))\n",
    "\n",
    "# If itâ€™s a tuple, print its length\n",
    "if isinstance(obj, tuple):\n",
    "    print(\"Length of tuple:\", len(obj))\n",
    "    for i, element in enumerate(obj):\n",
    "        print(f\"Element {i} has type {type(element)} and shape/length:\", end=\" \")\n",
    "        # If element is a torch.Tensor or numpy array:\n",
    "        if hasattr(element, 'shape'):\n",
    "            print(element.shape)\n",
    "        # If it's a list of fields:\n",
    "        else:\n",
    "            print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4ad57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottoms torch.Size([32, 2, 5]) torch.Size([32, 2])\n",
      "quarks torch.Size([32, 4, 5]) torch.Size([32, 4])\n",
      "neutrinos torch.Size([32, 4, 5]) torch.Size([32, 4])\n"
     ]
    }
   ],
   "source": [
    "# This is not strictly necessary, but just to make sure loading works as expected\n",
    "# We will use later a combined dataset (hard+reco) below\n",
    "hard_loader = DataLoader(\n",
    "    hard_dataset,\n",
    "    batch_size = 32,\n",
    "    num_workers = num_workers, # Parallel loading\n",
    "    pin_memory = True, # Faster transfer to GPU\n",
    ")\n",
    "batch = next(iter(hard_loader))\n",
    "\n",
    "for obj,mask,sel in zip(batch['data'],batch['mask'],hard_loader.dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d76f624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object\n",
      "Loaded branches:\n",
      "   ... file: 231528\n",
      "   ... sample: 231528\n",
      "   ... tree: 231528\n",
      "Branch in files not loaded:\n",
      "   ... Generator_scalePDF\n",
      "   ... Generator_weight\n",
      "   ... Generator_x1\n",
      "   ... Generator_x2\n",
      "   ... Generator_xpdf1\n",
      "   ... Generator_xpdf2\n",
      "   ... InputMet_phi\n",
      "   ... InputMet_pt\n",
      "   ... cleanedJet_btagDeepFlavB\n",
      "   ... cleanedJet_eta\n",
      "   ... cleanedJet_mass\n",
      "   ... cleanedJet_phi\n",
      "   ... cleanedJet_pt\n",
      "   ... event\n",
      "   ... ncleanedBJet\n",
      "   ... ncleanedJet\n",
      "   ... region\n",
      "   ... weight_nominal\n",
      "   ... xs_weight\n"
     ]
    }
   ],
   "source": [
    "data_reco = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    #N = data_hard.N,\n",
    ")\n",
    "\n",
    "print(data_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9184a638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading objects from /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco\n",
      "Saving preprocessing to /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco\n",
      "Will overwrite what is in output directory /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco/preprocessing\n",
      "Preprocessing saved in /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco/preprocessing\n",
      "Reco dataset with 114647 events\n",
      "Containing the following tensors\n",
      "jets  : data ([114647, 6, 5]), mask ([114647, 6])\n",
      "        Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%, 100.00%, 62.85%]\n",
      "        Mask attn     : [True, True, True, True, True, True]\n",
      "        Weights       : 114647.00, 114647.00, 114647.00, 114647.00, 114647.00, 114647.00\n",
      "        Features      : ['pt', 'eta', 'phi', 'mass', 'btag']\n",
      "        Selected for batches : True\n",
      "met   : data ([114647, 1, 4]), mask ([114647, 1])\n",
      "        Mask exist    : [100.00%]\n",
      "        Mask attn     : [True]\n",
      "        Weights       : 114647.00\n",
      "        Features      : ['pt', 'eta', 'phi', 'mass']\n",
      "        Selected for batches : True\n",
      "Preprocessing steps\n",
      "Step applied to ['jets']\n",
      "\tpt  : <class 'memflow.dataset.preprocessing.lowercutshift'>\n",
      "  - jets : ('pt',)\n",
      "Step applied to ['met']\n",
      "\tpt  : <class 'memflow.dataset.preprocessing.lowercutshift'>\n",
      "  - met : ('pt',)\n",
      "Step applied to ['jets', 'met']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "  - jets : ('pt', 'mass')\n",
      "  - met  : ('pt', 'mass')\n",
      "Step applied to ['jets', 'met']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\teta   : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "  - jets : ('pt', 'eta', 'mass')\n",
      "  - met  : ('pt', 'eta', 'mass')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reco_dataset = ttHRecoDataset(\n",
    "    data = data_reco,\n",
    "    selection = [\n",
    "        'jets',\n",
    "        'met',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "print(reco_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48800b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jets torch.Size([32, 6, 5]) torch.Size([32, 6])\n",
      "met torch.Size([32, 1, 4]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Also not needed, just checking \n",
    "reco_loader = DataLoader(\n",
    "    reco_dataset,\n",
    "    batch_size = 32,\n",
    "    num_workers = num_workers,\n",
    "    pin_memory = True,\n",
    ")\n",
    "batch = next(iter(reco_loader))\n",
    "\n",
    "for obj,mask,sel in zip(batch['data'],batch['mask'],reco_loader.dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf9d9be-ca75-4815-84c4-cb85eb70cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection Branch: event\n",
      "Hard Datset keys: dict_keys(['file', 'tree', 'sample', 'intersection'])\n",
      "Reco Datset keys: dict_keys(['file', 'tree', 'sample', 'intersection'])\n",
      "Intersection branches : `event` for hard dataset and `event` for reco dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking into file metadata\n",
      "Will pair these files together :\n",
      "   - /cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet <-> /cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet\n",
      "For entry 0 : from 756642 events, 91819 selected\n",
      "For entry 1 : from 114647 events, 91819 selected\n",
      "Combined dataset (extracting 91819 events of the following) :\n",
      "Parton dataset with 756642 events\n",
      " Initial states pdgids : [21, 21]\n",
      " Final states pdgids   : [25, 6, -6]\n",
      " Final states masses   : [125.2, 172.57, 172.57]\n",
      "Containing the following tensors\n",
      "bottoms    : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "Zs         : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "tops       : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "neutrinos  : data ([756642, 4, 5]), mask ([756642, 4])\n",
      "             Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%]\n",
      "             Mask attn     : [True, True, True, True]\n",
      "             Weights       : 756642.00, 756642.00, 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "boost      : data ([756642, 1, 4]), mask ([756642, 1])\n",
      "             Mask exist    : [100.00%]\n",
      "             Mask attn     : [True]\n",
      "             Weights       : 756642.00\n",
      "             Features      : ['x', 'y', 'z', 't']\n",
      "             Selected for batches : False\n",
      "Ws         : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "quarks     : data ([756642, 4, 5]), mask ([756642, 4])\n",
      "             Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%]\n",
      "             Mask attn     : [True, True, True, True]\n",
      "             Weights       : 756642.00, 756642.00, 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "higgs      : data ([756642, 1, 5]), mask ([756642, 1])\n",
      "             Mask exist    : [100.00%]\n",
      "             Mask attn     : [True]\n",
      "             Weights       : 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "Preprocessing steps\n",
      "Step applied to ['higgs', 'tops', 'bottoms', 'Ws', 'quarks', 'Zs', 'neutrinos']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "  - higgs     : ('pt', 'mass')\n",
      "  - tops      : ('pt', 'mass')\n",
      "  - bottoms   : ('pt', 'mass')\n",
      "  - Ws        : ('pt', 'mass')\n",
      "  - quarks    : ('pt', 'mass')\n",
      "  - Zs        : ('pt', 'mass')\n",
      "  - neutrinos : ('pt', 'mass')\n",
      "Step applied to ['higgs', 'tops', 'bottoms', 'Ws', 'quarks', 'Zs', 'neutrinos']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\teta   : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "  - higgs     : ('pt', 'eta', 'mass')\n",
      "  - tops      : ('pt', 'eta', 'mass')\n",
      "  - bottoms   : ('pt', 'eta', 'mass')\n",
      "  - Ws        : ('pt', 'eta', 'mass')\n",
      "  - quarks    : ('pt', 'eta', 'mass')\n",
      "  - Zs        : ('pt', 'eta', 'mass')\n",
      "  - neutrinos : ('pt', 'eta', 'mass')\n",
      "\n",
      "Reco dataset with 114647 events\n",
      "Containing the following tensors\n",
      "jets  : data ([114647, 6, 5]), mask ([114647, 6])\n",
      "        Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%, 100.00%, 62.85%]\n",
      "        Mask attn     : [True, True, True, True, True, True]\n",
      "        Weights       : 114647.00, 114647.00, 114647.00, 114647.00, 114647.00, 114647.00\n",
      "        Features      : ['pt', 'eta', 'phi', 'mass', 'btag']\n",
      "        Selected for batches : True\n",
      "met   : data ([114647, 1, 4]), mask ([114647, 1])\n",
      "        Mask exist    : [100.00%]\n",
      "        Mask attn     : [True]\n",
      "        Weights       : 114647.00\n",
      "        Features      : ['pt', 'eta', 'phi', 'mass']\n",
      "        Selected for batches : True\n",
      "Preprocessing steps\n",
      "Step applied to ['jets']\n",
      "\tpt  : <class 'memflow.dataset.preprocessing.lowercutshift'>\n",
      "  - jets : ('pt',)\n",
      "Step applied to ['met']\n",
      "\tpt  : <class 'memflow.dataset.preprocessing.lowercutshift'>\n",
      "  - met : ('pt',)\n",
      "Step applied to ['jets', 'met']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "  - jets : ('pt', 'mass')\n",
      "  - met  : ('pt', 'mass')\n",
      "Step applied to ['jets', 'met']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\teta   : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "  - jets : ('pt', 'eta', 'mass')\n",
      "  - met  : ('pt', 'eta', 'mass')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Intersection Branch: {reco_dataset.intersection_branch}')\n",
    "print (f'Hard Datset keys: {hard_dataset.metadata.keys()}')\n",
    "print (f'Reco Datset keys: {reco_dataset.metadata.keys()}')\n",
    "\n",
    "combined_dataset = CombinedDataset(\n",
    "    hard_dataset=hard_dataset,\n",
    "    reco_dataset=reco_dataset,\n",
    ")\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60a752-36e5-4083-80ae-53cfd5de342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : training 73455 / validation 18364\n",
      "Batching 72 / Validation 4\n"
     ]
    }
   ],
   "source": [
    "# Split train and validation #\n",
    "train_frac = 0.8\n",
    "indices = torch.arange(len(combined_dataset))\n",
    "sep = int(train_frac*len(combined_dataset))\n",
    "train_indices = indices[:sep]\n",
    "valid_indices = indices[sep:]\n",
    "\n",
    "dataset_train = torch.utils.data.Subset(combined_dataset,train_indices)\n",
    "dataset_valid = torch.utils.data.Subset(combined_dataset,valid_indices)\n",
    "print (f'Dataset : training {len(dataset_train)} / validation {len(ataset_valid)}')\n",
    "\n",
    "# make data loader #\n",
    "batch_size = 1024\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset_train ,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    ")\n",
    "loader_valid = DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size = 5000,\n",
    "    shuffle = False,\n",
    ")\n",
    "print (f'Batching {len(loader_train)} / Validation {len(loader_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89d560-cbb1-45c2-814d-cceb822efedd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ee7ff6-ab8f-4409-abe4-2c135e6caa74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'encoder_embeddings' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder_embeddings'])`.\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'decoder_embeddings' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder_embeddings'])`.\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'transformer' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transformer'])`.\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'flow' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['flow'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1024, 2, 5]), torch.Size([1024, 4, 5]), torch.Size([1024, 4, 5])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/zuko/transforms.py:494: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/BucketizationUtils.h:32.)\n",
      "  return torch.searchsorted(seq, value).squeeze(dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs [torch.Size([1024, 6]), torch.Size([1024, 1])]\n",
      "masks     [torch.Size([1024, 6]), torch.Size([1024, 1])]\n",
      "weights   [torch.Size([1024, 6]), torch.Size([1024, 1])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/core/module.py:441: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot log probs tensor(4.8277, grad_fn=<MeanBackward0>)\n",
      "samples\n",
      "\t torch.Size([3, 1024, 6, 5])\n",
      "\t torch.Size([3, 1024, 1, 4])\n",
      "TransferFlow(\n",
      "  (encoder_embeddings): MultiEmbeddings(\n",
      "    (embeddings): ModuleList(\n",
      "      (0-2): 3 x MLP(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=32, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_embeddings): MultiEmbeddings(\n",
      "    (embeddings): ModuleList(\n",
      "      (0): MLP(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=32, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): MLP(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (transformer): Transformer(\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): TransformerDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-7): 8 x TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flow): KinematicFlow(\n",
      "    (flows): ModuleDict(\n",
      "      (pt): NSF(\n",
      "        (transform): LazyComposedTransform(\n",
      "          (0-4): 5 x ElementWiseTransform(\n",
      "            (base): MonotonicRQSTransform(bins=16)\n",
      "            (hyper): MLP(\n",
      "              (0): Linear(in_features=66, out_features=256, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): ReLU()\n",
      "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (5): ReLU()\n",
      "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (base): UnconditionalDistribution(DiagNormal(loc: tensor([0.]), scale: tensor([1.])))\n",
      "      )\n",
      "      (eta): UniformNSF(\n",
      "        (transform): LazyComposedTransform(\n",
      "          (0-4): 5 x ElementWiseTransform(\n",
      "            (base): MonotonicRQSTransform(bins=16)\n",
      "            (hyper): MLP(\n",
      "              (0): Linear(in_features=66, out_features=256, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): ReLU()\n",
      "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (5): ReLU()\n",
      "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (base): UnconditionalDistribution(BoxUniform(low: tensor([-1.]), high: tensor([1.])))\n",
      "      )\n",
      "      (phi): UniformNCSF(\n",
      "        (transform): LazyComposedTransform(\n",
      "          (0-4): 5 x ElementWiseTransform(\n",
      "            (base): ComposedTransform(\n",
      "              (0): CircularShiftTransform(bound=3.141592653589793)\n",
      "              (1): MonotonicRQSTransform(bins=16)\n",
      "            )\n",
      "            (hyper): MLP(\n",
      "              (0): Linear(in_features=66, out_features=256, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): ReLU()\n",
      "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (5): ReLU()\n",
      "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (base): UnconditionalDistribution(BoxUniform(low: tensor([-3.1416]), high: tensor([3.1416])))\n",
      "      )\n",
      "      (mass): NSF(\n",
      "        (transform): LazyComposedTransform(\n",
      "          (0-4): 5 x ElementWiseTransform(\n",
      "            (base): MonotonicRQSTransform(bins=16)\n",
      "            (hyper): MLP(\n",
      "              (0): Linear(in_features=67, out_features=256, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (3): ReLU()\n",
      "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (5): ReLU()\n",
      "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (base): UnconditionalDistribution(DiagNormal(loc: tensor([0.]), scale: tensor([1.])))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = TransferFlow(\n",
    "    encoder_embeddings = MultiEmbeddings(\n",
    "        features_per_type = combined_dataset.hard_dataset.input_features,\n",
    "        embed_dims = [32,64],\n",
    "        hidden_activation = nn.GELU,\n",
    "    ),\n",
    "    decoder_embeddings = MultiEmbeddings(\n",
    "        features_per_type = combined_dataset.reco_dataset.input_features,\n",
    "        embed_dims = [32,64],\n",
    "        hidden_activation = nn.GELU,\n",
    "    ),\n",
    "    transformer = Transformer(\n",
    "        d_model = 64,\n",
    "        encoder_layers = 6,\n",
    "        decoder_layers = 8,\n",
    "        nhead = 8,\n",
    "        dim_feedforward = 256,\n",
    "        activation = nn.GELU,\n",
    "        encoder_mask_attn = None,\n",
    "        decoder_mask_attn = combined_dataset.reco_dataset.attention_mask,\n",
    "        use_null_token = True,\n",
    "        dropout = 0.,\n",
    "    ),\n",
    "    flow = KinematicFlow(\n",
    "        d_model = 64,\n",
    "        flow_mode = 'global',\n",
    "        flow_features = [\n",
    "            ['pt','eta','phi','mass'], # jets\n",
    "            ['pt','phi'],              # met\n",
    "        ],\n",
    "        flow_classes = { # classes for each feature\n",
    "            'pt'  : zuko.flows.NSF,\n",
    "            'eta' : UniformNSF,\n",
    "            'phi' : UniformNCSF,\n",
    "            'mass': zuko.flows.NSF,\n",
    "        },\n",
    "        flow_common_args = { # common args for all flows\n",
    "        'bins' : 16,\n",
    "        'transforms' : 5,\n",
    "        'randperm' : True,\n",
    "        'passes' : None,\n",
    "        'hidden_features' : [256] * 3,   \n",
    "        },\n",
    "        flow_specific_args = { # specific args for each class above\n",
    "            'eta' : {'bound' : 1.},\n",
    "            'phi' : {'bound' : math.pi},\n",
    "        },\n",
    "    ),\n",
    "    hard_names = combined_dataset.hard_dataset.selection,\n",
    "    reco_names = combined_dataset.reco_dataset.selection,\n",
    ")\n",
    "\n",
    "model = model.cpu()\n",
    "\n",
    "batch = next(iter(loader_train))\n",
    "print ([ data.shape for data in batch['hard']['data']])\n",
    "log_probs, masks, weights = model(batch)\n",
    "print ('log_probs',[log_prob.shape for log_prob in log_probs])\n",
    "print ('masks    ',[mask.shape for mask in masks])\n",
    "print ('weights  ',[weight.shape for weight in weights])\n",
    "\n",
    "log_probs_tot = model.shared_eval(batch,0,'test')\n",
    "print ('tot log probs',log_probs_tot)\n",
    "\n",
    "samples = model.sample(\n",
    "    batch['hard']['data'],\n",
    "    batch['hard']['mask'],\n",
    "    batch['reco']['data'],\n",
    "    batch['reco']['mask'],\n",
    "    N = 3,\n",
    ")\n",
    "print ('samples')\n",
    "for sample in samples:\n",
    "    print ('\\t',sample.shape)\n",
    "\n",
    "print (model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb6df6-0ece-42fb-aada-e3b498e4279f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9470681d-25a1-419c-945b-bbcfe02a7fd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sampling \u001b[38;5;241m=\u001b[39m SamplingCallback(\n\u001b[0;32m----> 2\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_valid\u001b[49m,\n\u001b[1;32m      3\u001b[0m     preprocessing \u001b[38;5;241m=\u001b[39m combined_dataset\u001b[38;5;241m.\u001b[39mreco_dataset\u001b[38;5;241m.\u001b[39mpreprocessing,\n\u001b[1;32m      4\u001b[0m     idx_to_monitor \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m      5\u001b[0m     N_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e3\u001b[39m), \n\u001b[1;32m      6\u001b[0m     frequency \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m      7\u001b[0m     bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      8\u001b[0m     hexbin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     kde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     log_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     label_names \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$p_T$\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124meta$\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphi\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mphi$\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmass\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$M$\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     },\n\u001b[1;32m     17\u001b[0m     feature_rng \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m : (\u001b[38;5;241m0.\u001b[39m,\u001b[38;5;241m500.\u001b[39m),\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m : (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5.\u001b[39m,\u001b[38;5;241m5.\u001b[39m),\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphi\u001b[39m\u001b[38;5;124m'\u001b[39m : (\u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39mpi,math\u001b[38;5;241m.\u001b[39mpi),\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmass\u001b[39m\u001b[38;5;124m'\u001b[39m : (\u001b[38;5;241m0.\u001b[39m,\u001b[38;5;241m100.\u001b[39m),\n\u001b[1;32m     22\u001b[0m     }\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m bias \u001b[38;5;241m=\u001b[39m BiasCallback(\n\u001b[1;32m     25\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset_valid,\n\u001b[1;32m     26\u001b[0m     preprocessing \u001b[38;5;241m=\u001b[39m combined_dataset\u001b[38;5;241m.\u001b[39mreco_dataset\u001b[38;5;241m.\u001b[39mpreprocessing,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     },\n\u001b[1;32m     40\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_valid' is not defined"
     ]
    }
   ],
   "source": [
    "sampling = SamplingCallback(\n",
    "    dataset = dataset_valid,\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing,\n",
    "    idx_to_monitor = [0,1,2],\n",
    "    N_sample = int(1e3), \n",
    "    frequency = 500,\n",
    "    bins = 50,\n",
    "    hexbin = True,\n",
    "    kde = False,\n",
    "    log_scale = True,\n",
    "    label_names = {\n",
    "        'pt' : '$p_T$',\n",
    "        'eta' : '$\\eta$',\n",
    "        'phi' : '$\\phi$',\n",
    "        'mass' : '$M$',\n",
    "    },\n",
    "    feature_rng = {\n",
    "        'pt' : (0.,500.),\n",
    "        'eta' : (-5.,5.),\n",
    "        'phi' : (-math.pi,math.pi),\n",
    "        'mass' : (0.,100.),\n",
    "    }\n",
    ")\n",
    "bias = BiasCallback(\n",
    "    dataset = dataset_valid,\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing,\n",
    "    N_sample = 30,\n",
    "    frequency = 500,\n",
    "    bins = 51,\n",
    "    points = 30,\n",
    "    log_scale = True,\n",
    "    batch_size = 1024,\n",
    "    N_batch = math.inf,\n",
    "    label_names = {\n",
    "        'pt' : '$p_T$',\n",
    "        'eta' : '$\\eta$',\n",
    "        'phi' : '$\\phi$',\n",
    "        'mass' : '$M$',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d399eb-7046-4d9b-b070-8e07452e7f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   : Batch size = 1024 => 81 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/themrluke/mem-flow-hinv/6d4cd14bd93c471aa763d0bbfbd638e6\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name                          | Type            | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | encoder_embeddings            | MultiEmbeddings | 6.9 K  | train\n",
      "1 | encoder_embeddings.embeddings | ModuleList      | 6.9 K  | train\n",
      "2 | decoder_embeddings            | MultiEmbeddings | 4.6 K  | train\n",
      "3 | decoder_embeddings.embeddings | ModuleList      | 4.6 K  | train\n",
      "4 | transformer                   | Transformer     | 834 K  | train\n",
      "5 | transformer.transformer       | Transformer     | 834 K  | train\n",
      "6 | flow                          | KinematicFlow   | 3.2 M  | train\n",
      "7 | flow.flows                    | ModuleDict      | 3.2 M  | train\n",
      "--------------------------------------------------------------------------\n",
      "4.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 M     Total params\n",
      "16.253    Total estimated model params size (MB)\n",
      "421       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m String value length exceeds 1000 characters and will be truncated. Provided value: 'KinematicFlow(\n",
      "  (flows): ModuleDict(\n",
      "    (pt): NSF(\n",
      "      (transform): LazyComposedTransform(\n",
      "        (0-4): 5 x ElementWiseTransform(\n",
      "          (base): MonotonicRQSTransform(bins=16)\n",
      "          (hyper): MLP(\n",
      "            (0): Linear(in_features=66, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): ReLU()\n",
      "            (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (5): ReLU()\n",
      "            (6): Linear(in_features=256, out_features=47, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (base): UnconditionalDistribution(DiagNormal(loc: tensor([0.]), scale: tensor([1.])))\n",
      "    )\n",
      "    (eta): UniformNSF(\n",
      "      (transform): LazyComposedTransform(\n",
      "        (0-4): 5 x ElementWiseTransform(\n",
      "          (base): MonotonicRQSTransform(bins=16)\n",
      "          (hyper): MLP(\n",
      "            (0): Linear(in_features=66, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): ReLU()\n",
      "            (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (5): ReLU()\n",
      "            (6): Linear(in_features=256, out_features=47, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (base): UnconditionalDistribution(BoxUniform(low: tensor([-1.]), high: tensor([1.])))\n",
      "    )\n",
      "    (phi): UniformNCSF(\n",
      "      (transform): LazyComposedTransform(\n",
      "        (0-4): 5 x ElementWiseTransform(\n",
      "          (base): ComposedTransform(\n",
      "            (0): CircularShiftTransform(bound=3.141592653589793)\n",
      "            (1): MonotonicRQSTransform(bins=16)\n",
      "          )\n",
      "          (hyper): MLP(\n",
      "            (0): Linear(in_features=66, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): ReLU()\n",
      "            (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (5): ReLU()\n",
      "            (6): Linear(in_features=256, out_features=47, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (base): UnconditionalDistribution(BoxUniform(low: tensor([-3.1416]), high: tensor([3.1416])))\n",
      "    )\n",
      "    (mass): NSF(\n",
      "      (transform): LazyComposedTransform(\n",
      "        (0-4): 5 x ElementWiseTransform(\n",
      "          (base): MonotonicRQSTransform(bins=16)\n",
      "          (hyper): MLP(\n",
      "            (0): Linear(in_features=67, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): ReLU()\n",
      "            (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (5): ReLU()\n",
      "            (6): Linear(in_features=256, out_features=47, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (base): UnconditionalDistribution(DiagNormal(loc: tensor([0.]), scale: tensor([1.])))\n",
      "    )\n",
      "  )\n",
      ")'\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m String value length exceeds 1000 characters and will be truncated. Provided value: 'Transformer(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.0, inplace=False)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-7): 8 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.0, inplace=False)\n",
      "          (dropout2): Dropout(p=0.0, inplace=False)\n",
      "          (dropout3): Dropout(p=0.0, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e41d691918e41188fd3b5f5e32d46a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/transformer.py:380: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/zuko/transforms.py:494: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/BucketizationUtils.h:32.)\n",
      "  return torch.searchsorted(seq, value).squeeze(dim=-1)\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff411292d3943ccaf2ffedadcc5105b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f23af8c5e24421ac7758fe83af93c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc9f387c06046b4bf3cbce3d73a14af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef20ba9c651549329776036397e1f395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e234395d824bb583f63f85881e945b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28603a99f2bd4e0bbee22db20282400e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e657f02ae146aea86f8b88a404e3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dc70f2f5214427b18c445c3252247e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a10a0eee664b3c9232beedb43e5e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c727d0c8154bc5b350106d76260790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428ac44fad0249dc96174a127a89d78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 10: TransferFlow_checkpoints/model_epoch_10.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87268b2136674c9484a0f43deb91b302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc900527130435ea6fd74fedb948a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcec46b226d473ea785cdf4439d3e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Parameters #####\n",
    "epochs = 501\n",
    "steps_per_epoch_train = math.ceil(len(dataset_train)/loader_train.batch_size)\n",
    "\n",
    "print (f'Training   : Batch size = {loader_train.batch_size} => {steps_per_epoch_train} steps per epoch')\n",
    "##### Optimizer #####\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "##### Scheduler #####\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer = optimizer,\n",
    "    mode='min', \n",
    "    factor=0.1, \n",
    "    patience=10, \n",
    "    threshold=0.001, \n",
    "    threshold_mode='rel', \n",
    "    cooldown=0, \n",
    "    min_lr=1e-7\n",
    ")\n",
    "model.set_scheduler_config(\n",
    "    {\n",
    "        'scheduler' : scheduler,\n",
    "        'interval' : 'step' if isinstance(scheduler,optim.lr_scheduler.OneCycleLR) else 'epoch',\n",
    "        'frequency' : 1,\n",
    "        'monitor' : 'val/loss_tot',\n",
    "        'strict' : True,\n",
    "        'name' : 'scheduler',\n",
    "    }\n",
    ")\n",
    "\n",
    "##### Callbacks #####\n",
    "callbacks = [\n",
    "    L.pytorch.callbacks.LearningRateMonitor(logging_interval = 'epoch'),\n",
    "    L.pytorch.callbacks.ModelSummary(max_depth=2),\n",
    "    sampling,\n",
    "    bias,\n",
    "    ModelCheckpoint(save_every_n_epochs=10, save_dir=\"TransferFlow_checkpoints\"),\n",
    "]\n",
    "\n",
    "##### Logger #####\n",
    "logger = pl_loggers.CometLogger(\n",
    "    save_dir = '../comet_logs',\n",
    "    project_name = 'mem-flow-Hinv',\n",
    "    experiment_name = 'transfer-flow',\n",
    "    offline = False,\n",
    ") \n",
    "logger.log_graph(model)\n",
    "# logger.log_hyperparams()\n",
    "# logger.experiment.log_code(folder='../src/')\n",
    "# logger.experiment.log_notebook(filename=globals()['__session__'],overwrite=True)\n",
    "\n",
    "##### Trainer #####\n",
    "trainer = L.Trainer(    \n",
    "    min_epochs = 5,\n",
    "    max_epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    devices = 'auto',\n",
    "    accelerator = accelerator,\n",
    "    logger = logger,\n",
    ")\n",
    "##### Fit #####\n",
    "trainer.fit(\n",
    "    model = model, \n",
    "    train_dataloaders = loader_train,\n",
    "    val_dataloaders = loader_valid,\n",
    "    #ckpt_path=\"TransferFlow_checkpoints/model_epoch_230.ckpt\" # Use to resume training from a checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc99fd-4cb8-4255-ae12-473bc8a56d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figs = sampling.make_plots(model.cuda(),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27255367-7bb4-4049-ad12-b6f0321804c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figs = bias.make_plots(model.cuda(),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e0f47-055e-42f9-b83b-0ed8c245b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dphi_j1j2(jets,met):\n",
    "    return jets[:,0].deltaphi(jets[:,1])\n",
    "        \n",
    "def dR_j1j2(jets,met):\n",
    "    return jets[:,0].deltaR(jets[:,1])\n",
    "\n",
    "def HT(jets,met):\n",
    "    return ak.sum(jets.pt,axis=1)\n",
    "\n",
    "def dR_met_j1j2(jets,met):\n",
    "    j1j2 = jets[:,0] + jets[:,1]\n",
    "    return met[:,0].deltaR(j1j2)\n",
    "\n",
    "def min_mass_jj(jets,met):\n",
    "    # Make all possible jet pairs for each event #\n",
    "    dijets = ak.combinations(jets,n=2,replacement=False,axis=1)\n",
    "    # Split into pairs of jets #\n",
    "    j1, j2 = ak.unzip(dijets)\n",
    "    # Calculate minimum invariant mass for all pairs #\n",
    "    return ak.min((j1+j2).mass,axis=1)\n",
    "\n",
    "high_level = HighLevelVariableCallback(\n",
    "    dataset = dataset_valid,\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing,\n",
    "    N_sample = 30,\n",
    "    frequency = 100,\n",
    "    bins = 51,\n",
    "    log_scale = True,\n",
    "    batch_size = 1000,\n",
    "    N_batch = math.inf,\n",
    "    var_functions = {\n",
    "        'dphi_j1j2'   : dphi_j1j2,\n",
    "        'dR_j1j2'     : dR_j1j2,\n",
    "        'HT'          : HT,\n",
    "        'dR_met_j1j2' : dR_met_j1j2,\n",
    "        'min_mass_jj' : min_mass_jj,\n",
    "    },\n",
    "    label_names = {\n",
    "        'dphi_j1j2'      : r'$\\Delta \\phi(j_1,j_2)$',\n",
    "        'dR_j1j2'        : r'$\\Delta R(j_1,j_2)$',\n",
    "        'dR_met_j1j2'    : r'$\\Delta R(MET,jj)$',\n",
    "        'HT'             : r'$H_T$',\n",
    "        'min_mass_jj'    : r'$min_{j1,j2 \\in \\text{jets}} (m_{j_1j_2})$',\n",
    "    },\n",
    ")\n",
    "samples = high_level.make_plots(model.cuda(),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0304b-d890-475b-93b0-2b6593971598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
