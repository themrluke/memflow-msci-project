{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f754e5-5310-4717-8700-8dc85ecf889f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a7bc2a-42e5-48ff-8ddd-a01ed240d9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:15: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 12.0.1. Please consider upgrading.\n",
      "  warnings.warn(\n",
      "2025-02-26 12:16:49.449646: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-26 12:16:49.449721: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-26 12:16:49.449730: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 12:16:49.456768: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU workers for dataloading: 16\n",
      "Running on GPU : True\n",
      "Accelerator : cuda\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import dask\n",
    "from tqdm import tqdm\n",
    "\n",
    "import vector\n",
    "import particle\n",
    "import hepunits\n",
    "\n",
    "import comet_ml\n",
    "import zuko\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import SequentialLR, LambdaLR, CosineAnnealingLR\n",
    "\n",
    "import multiprocessing\n",
    "import uuid\n",
    "\n",
    "from memflow.dataset.data import ParquetData\n",
    "from memflow.dataset.dataset import CombinedDataset\n",
    "from memflow.ttH.ttH_dataclasses import ttHHardDataset, ttHRecoDataset\n",
    "\n",
    "from memflow.ttH.models.ParallelTransfusion import StandardCFM, OptimalTransportCFM, TargetBridgingCFM, SchrodingerBridgeCFM, VariancePreservingCFM\n",
    "from models.utils import *\n",
    "from models.callbacks import CFMSamplingCallback, SamplingCallback, BiasCallback\n",
    "\n",
    "from transfer_flow.transfer_flow_model import *\n",
    "from transfer_flow.custom_flows import *\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "num_workers = min(16, multiprocessing.cpu_count())  # Use up to 16 CPU cores\n",
    "print(f'Number of CPU workers for dataloading: {num_workers}')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Change \"<n>\" to the index of the GPU you want to use on node\n",
    "\n",
    "print (f\"Running on GPU : {torch.cuda.is_available()}\")\n",
    "accelerator = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (f\"Accelerator : {accelerator}\")\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "if accelerator =='cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print (torch.cuda.memory_summary(device=None, abbreviated=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c50d80-9342-47c6-9efd-87c39a91bc5b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a68956-c2a9-4fbd-b11a-9025f8339cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object\n",
      "Loaded branches:\n",
      "   ... file: 1903554\n",
      "   ... sample: 1903554\n",
      "   ... tree: 1903554\n",
      "Branch in files not loaded:\n",
      "   ... Generator_scalePDF\n",
      "   ... Generator_weight\n",
      "   ... Generator_x1\n",
      "   ... Generator_x2\n",
      "   ... Generator_xpdf1\n",
      "   ... Generator_xpdf2\n",
      "   ... W_minus_from_antitop_eta\n",
      "   ... W_minus_from_antitop_genPartIdxMother\n",
      "   ... W_minus_from_antitop_idx\n",
      "   ... W_minus_from_antitop_mass\n",
      "   ... W_minus_from_antitop_pdgId\n",
      "   ... W_minus_from_antitop_phi\n",
      "   ... W_minus_from_antitop_pt\n",
      "   ... W_minus_from_antitop_status\n",
      "   ... W_minus_from_antitop_statusFlags\n",
      "   ... W_plus_from_top_eta\n",
      "   ... W_plus_from_top_genPartIdxMother\n",
      "   ... W_plus_from_top_idx\n",
      "   ... W_plus_from_top_mass\n",
      "   ... W_plus_from_top_pdgId\n",
      "   ... W_plus_from_top_phi\n",
      "   ... W_plus_from_top_pt\n",
      "   ... W_plus_from_top_status\n",
      "   ... W_plus_from_top_statusFlags\n",
      "   ... Z_from_higgs_eta\n",
      "   ... Z_from_higgs_genPartIdxMother\n",
      "   ... Z_from_higgs_idx\n",
      "   ... Z_from_higgs_mass\n",
      "   ... Z_from_higgs_pdgId\n",
      "   ... Z_from_higgs_phi\n",
      "   ... Z_from_higgs_pt\n",
      "   ... Z_from_higgs_status\n",
      "   ... Z_from_higgs_statusFlags\n",
      "   ... antibottom_eta\n",
      "   ... antibottom_genPartIdxMother\n",
      "   ... antibottom_idx\n",
      "   ... antibottom_mass\n",
      "   ... antibottom_pdgId\n",
      "   ... antibottom_phi\n",
      "   ... antibottom_pt\n",
      "   ... antibottom_status\n",
      "   ... antibottom_statusFlags\n",
      "   ... antineutrino_from_W_minus_eta\n",
      "   ... antineutrino_from_W_minus_genPartIdxMother\n",
      "   ... antineutrino_from_W_minus_idx\n",
      "   ... antineutrino_from_W_minus_mass\n",
      "   ... antineutrino_from_W_minus_pdgId\n",
      "   ... antineutrino_from_W_minus_phi\n",
      "   ... antineutrino_from_W_minus_pt\n",
      "   ... antineutrino_from_W_minus_status\n",
      "   ... antineutrino_from_W_minus_statusFlags\n",
      "   ... antiquark_from_W_minus_eta\n",
      "   ... antiquark_from_W_minus_genPartIdxMother\n",
      "   ... antiquark_from_W_minus_idx\n",
      "   ... antiquark_from_W_minus_mass\n",
      "   ... antiquark_from_W_minus_pdgId\n",
      "   ... antiquark_from_W_minus_phi\n",
      "   ... antiquark_from_W_minus_pt\n",
      "   ... antiquark_from_W_minus_status\n",
      "   ... antiquark_from_W_minus_statusFlags\n",
      "   ... antiquark_from_W_plus_eta\n",
      "   ... antiquark_from_W_plus_genPartIdxMother\n",
      "   ... antiquark_from_W_plus_idx\n",
      "   ... antiquark_from_W_plus_mass\n",
      "   ... antiquark_from_W_plus_pdgId\n",
      "   ... antiquark_from_W_plus_phi\n",
      "   ... antiquark_from_W_plus_pt\n",
      "   ... antiquark_from_W_plus_status\n",
      "   ... antiquark_from_W_plus_statusFlags\n",
      "   ... antitop_eta\n",
      "   ... antitop_genPartIdxMother\n",
      "   ... antitop_idx\n",
      "   ... antitop_mass\n",
      "   ... antitop_pdgId\n",
      "   ... antitop_phi\n",
      "   ... antitop_pt\n",
      "   ... antitop_status\n",
      "   ... antitop_statusFlags\n",
      "   ... bottom_eta\n",
      "   ... bottom_genPartIdxMother\n",
      "   ... bottom_idx\n",
      "   ... bottom_mass\n",
      "   ... bottom_pdgId\n",
      "   ... bottom_phi\n",
      "   ... bottom_pt\n",
      "   ... bottom_status\n",
      "   ... bottom_statusFlags\n",
      "   ... event\n",
      "   ... higgs_eta\n",
      "   ... higgs_genPartIdxMother\n",
      "   ... higgs_idx\n",
      "   ... higgs_mass\n",
      "   ... higgs_pdgId\n",
      "   ... higgs_phi\n",
      "   ... higgs_pt\n",
      "   ... higgs_status\n",
      "   ... higgs_statusFlags\n",
      "   ... lep_minus_from_W_minus_eta\n",
      "   ... lep_minus_from_W_minus_genPartIdxMother\n",
      "   ... lep_minus_from_W_minus_idx\n",
      "   ... lep_minus_from_W_minus_mass\n",
      "   ... lep_minus_from_W_minus_pdgId\n",
      "   ... lep_minus_from_W_minus_phi\n",
      "   ... lep_minus_from_W_minus_pt\n",
      "   ... lep_minus_from_W_minus_status\n",
      "   ... lep_minus_from_W_minus_statusFlags\n",
      "   ... lep_plus_from_W_plus_eta\n",
      "   ... lep_plus_from_W_plus_genPartIdxMother\n",
      "   ... lep_plus_from_W_plus_idx\n",
      "   ... lep_plus_from_W_plus_mass\n",
      "   ... lep_plus_from_W_plus_pdgId\n",
      "   ... lep_plus_from_W_plus_phi\n",
      "   ... lep_plus_from_W_plus_pt\n",
      "   ... lep_plus_from_W_plus_status\n",
      "   ... lep_plus_from_W_plus_statusFlags\n",
      "   ... neutrino_from_W_plus_eta\n",
      "   ... neutrino_from_W_plus_genPartIdxMother\n",
      "   ... neutrino_from_W_plus_idx\n",
      "   ... neutrino_from_W_plus_mass\n",
      "   ... neutrino_from_W_plus_pdgId\n",
      "   ... neutrino_from_W_plus_phi\n",
      "   ... neutrino_from_W_plus_pt\n",
      "   ... neutrino_from_W_plus_status\n",
      "   ... neutrino_from_W_plus_statusFlags\n",
      "   ... neutrinos_from_Z_eta\n",
      "   ... neutrinos_from_Z_genPartIdxMother\n",
      "   ... neutrinos_from_Z_idx\n",
      "   ... neutrinos_from_Z_mass\n",
      "   ... neutrinos_from_Z_pdgId\n",
      "   ... neutrinos_from_Z_phi\n",
      "   ... neutrinos_from_Z_pt\n",
      "   ... neutrinos_from_Z_status\n",
      "   ... neutrinos_from_Z_statusFlags\n",
      "   ... quark_from_W_minus_eta\n",
      "   ... quark_from_W_minus_genPartIdxMother\n",
      "   ... quark_from_W_minus_idx\n",
      "   ... quark_from_W_minus_mass\n",
      "   ... quark_from_W_minus_pdgId\n",
      "   ... quark_from_W_minus_phi\n",
      "   ... quark_from_W_minus_pt\n",
      "   ... quark_from_W_minus_status\n",
      "   ... quark_from_W_minus_statusFlags\n",
      "   ... quark_from_W_plus_eta\n",
      "   ... quark_from_W_plus_genPartIdxMother\n",
      "   ... quark_from_W_plus_idx\n",
      "   ... quark_from_W_plus_mass\n",
      "   ... quark_from_W_plus_pdgId\n",
      "   ... quark_from_W_plus_phi\n",
      "   ... quark_from_W_plus_pt\n",
      "   ... quark_from_W_plus_status\n",
      "   ... quark_from_W_plus_statusFlags\n",
      "   ... top_eta\n",
      "   ... top_genPartIdxMother\n",
      "   ... top_idx\n",
      "   ... top_mass\n",
      "   ... top_pdgId\n",
      "   ... top_phi\n",
      "   ... top_pt\n",
      "   ... top_status\n",
      "   ... top_statusFlags\n"
     ]
    }
   ],
   "source": [
    "data_hard = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "        #'all_jets_fullRun2_ttHbb_forTraining_allyears_spanetprov_part1_validation.parquet',\n",
    "        #'all_jets_fullRun2_ttHTobb_forTraining_2016_PreVFP_v3.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    # N = int(1e5),\n",
    ")\n",
    "\n",
    "print (data_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35dfc61-246f-444b-930a-e5cdd77fd3c9",
   "metadata": {},
   "source": [
    "# Hard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd35299-f40b-4864-9132-67a95e03acd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading objects from /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard\n",
      "Saving preprocessing to /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard\n",
      "Will overwrite what is in output directory /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard/preprocessing\n",
      "Preprocessing saved in /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard/preprocessing\n",
      "Parton dataset with 756642 events\n",
      " Initial states pdgids : [21, 21]\n",
      " Final states pdgids   : [25, 6, -6]\n",
      " Final states masses   : [125.2, 172.57, 172.57]\n",
      "Containing the following tensors\n",
      "bottoms    : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "Zs         : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "tops       : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "neutrinos  : data ([756642, 4, 5]), mask ([756642, 4])\n",
      "             Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%]\n",
      "             Mask attn     : [True, True, True, True]\n",
      "             Weights       : 756642.00, 756642.00, 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "boost      : data ([756642, 1, 4]), mask ([756642, 1])\n",
      "             Mask exist    : [100.00%]\n",
      "             Mask attn     : [True]\n",
      "             Weights       : 756642.00\n",
      "             Features      : ['x', 'y', 'z', 't']\n",
      "             Selected for batches : False\n",
      "Ws         : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "quarks     : data ([756642, 4, 5]), mask ([756642, 4])\n",
      "             Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%]\n",
      "             Mask attn     : [True, True, True, True]\n",
      "             Weights       : 756642.00, 756642.00, 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "higgs      : data ([756642, 1, 5]), mask ([756642, 1])\n",
      "             Mask exist    : [100.00%]\n",
      "             Mask attn     : [True]\n",
      "             Weights       : 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "Preprocessing steps\n",
      "Step applied to ['higgs', 'tops', 'bottoms', 'Ws', 'quarks', 'Zs', 'neutrinos']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "  - higgs     : ('pt', 'mass')\n",
      "  - tops      : ('pt', 'mass')\n",
      "  - bottoms   : ('pt', 'mass')\n",
      "  - Ws        : ('pt', 'mass')\n",
      "  - quarks    : ('pt', 'mass')\n",
      "  - Zs        : ('pt', 'mass')\n",
      "  - neutrinos : ('pt', 'mass')\n",
      "Step applied to ['higgs', 'tops', 'bottoms', 'Ws', 'quarks', 'Zs', 'neutrinos']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\teta   : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "  - higgs     : ('pt', 'eta', 'mass')\n",
      "  - tops      : ('pt', 'eta', 'mass')\n",
      "  - bottoms   : ('pt', 'eta', 'mass')\n",
      "  - Ws        : ('pt', 'eta', 'mass')\n",
      "  - quarks    : ('pt', 'eta', 'mass')\n",
      "  - Zs        : ('pt', 'eta', 'mass')\n",
      "  - neutrinos : ('pt', 'eta', 'mass')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hard_dataset = ttHHardDataset(\n",
    "    data = data_hard,\n",
    "    selection = [\n",
    "        # 'higgs',\n",
    "        # 'tops',\n",
    "        'bottoms',\n",
    "        # 'Ws',\n",
    "        # 'Zs',\n",
    "        'quarks',\n",
    "        'neutrinos',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "\n",
    "print(hard_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c82aff-8a32-4f83-b199-0c7725639eda",
   "metadata": {},
   "source": [
    "# Reco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc523344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object\n",
      "Loaded branches:\n",
      "   ... file: 231528\n",
      "   ... sample: 231528\n",
      "   ... tree: 231528\n",
      "Branch in files not loaded:\n",
      "   ... Generator_scalePDF\n",
      "   ... Generator_weight\n",
      "   ... Generator_x1\n",
      "   ... Generator_x2\n",
      "   ... Generator_xpdf1\n",
      "   ... Generator_xpdf2\n",
      "   ... InputMet_phi\n",
      "   ... InputMet_pt\n",
      "   ... cleanedJet_btagDeepFlavB\n",
      "   ... cleanedJet_eta\n",
      "   ... cleanedJet_mass\n",
      "   ... cleanedJet_phi\n",
      "   ... cleanedJet_pt\n",
      "   ... event\n",
      "   ... ncleanedBJet\n",
      "   ... ncleanedJet\n",
      "   ... region\n",
      "   ... weight_nominal\n",
      "   ... xs_weight\n"
     ]
    }
   ],
   "source": [
    "data_reco = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    #N = data_hard.N,\n",
    ")\n",
    "\n",
    "print(data_reco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ece5f1",
   "metadata": {},
   "source": [
    "Have a look at athe minimum values for Jet and MET pT in the raw dataset. This can give an indication as to what the cutoff in the SR is and hence what to set the `'pt':lowercutshift()` to in the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e781dae8-05ad-462e-b91a-bc544337a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading objects from /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco\n",
      "Saving preprocessing to /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco\n",
      "Will overwrite what is in output directory /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco/preprocessing\n",
      "Preprocessing saved in /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco/preprocessing\n",
      "Reco dataset with 114647 events\n",
      "Containing the following tensors\n",
      "jets  : data ([114647, 6, 5]), mask ([114647, 6])\n",
      "        Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%, 100.00%, 62.85%]\n",
      "        Mask attn     : [True, True, True, True, True, True]\n",
      "        Weights       : 114647.00, 114647.00, 114647.00, 114647.00, 114647.00, 114647.00\n",
      "        Features      : ['pt', 'eta', 'phi', 'mass', 'btag']\n",
      "        Selected for batches : True\n",
      "met   : data ([114647, 1, 4]), mask ([114647, 1])\n",
      "        Mask exist    : [100.00%]\n",
      "        Mask attn     : [True]\n",
      "        Weights       : 114647.00\n",
      "        Features      : ['pt', 'eta', 'phi', 'mass']\n",
      "        Selected for batches : True\n",
      "Preprocessing steps\n",
      "Step applied to ['jets']\n",
      "\tpt  : <class 'memflow.dataset.preprocessing.lowercutshift'>\n",
      "  - jets : ('pt',)\n",
      "Step applied to ['met']\n",
      "\tpt  : <class 'memflow.dataset.preprocessing.lowercutshift'>\n",
      "  - met : ('pt',)\n",
      "Step applied to ['jets', 'met']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "  - jets : ('pt', 'mass')\n",
      "  - met  : ('pt', 'mass')\n",
      "Step applied to ['jets', 'met']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\teta   : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "  - jets : ('pt', 'eta', 'mass')\n",
      "  - met  : ('pt', 'eta', 'mass')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reco_dataset = ttHRecoDataset(\n",
    "    data = data_reco,\n",
    "    selection = [\n",
    "        'jets',\n",
    "        'met',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "print(reco_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc702e-66b1-450a-86d0-a28192f99d98",
   "metadata": {},
   "source": [
    "# Combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12a63d7-eafc-43ef-8147-d0887d4bceec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection branches : `event` for hard dataset and `event` for reco dataset\n",
      "Looking into file metadata\n",
      "Will pair these files together :\n",
      "   - /cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet <-> /cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet\n",
      "For entry 0 : from 756642 events, 91819 selected\n",
      "For entry 1 : from 114647 events, 91819 selected\n",
      "Combined dataset (extracting 91819 events of the following) :\n",
      "Parton dataset with 756642 events\n",
      " Initial states pdgids : [21, 21]\n",
      " Final states pdgids   : [25, 6, -6]\n",
      " Final states masses   : [125.2, 172.57, 172.57]\n",
      "Containing the following tensors\n",
      "bottoms    : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "Zs         : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "tops       : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "neutrinos  : data ([756642, 4, 5]), mask ([756642, 4])\n",
      "             Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%]\n",
      "             Mask attn     : [True, True, True, True]\n",
      "             Weights       : 756642.00, 756642.00, 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "boost      : data ([756642, 1, 4]), mask ([756642, 1])\n",
      "             Mask exist    : [100.00%]\n",
      "             Mask attn     : [True]\n",
      "             Weights       : 756642.00\n",
      "             Features      : ['x', 'y', 'z', 't']\n",
      "             Selected for batches : False\n",
      "Ws         : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "quarks     : data ([756642, 4, 5]), mask ([756642, 4])\n",
      "             Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%]\n",
      "             Mask attn     : [True, True, True, True]\n",
      "             Weights       : 756642.00, 756642.00, 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "higgs      : data ([756642, 1, 5]), mask ([756642, 1])\n",
      "             Mask exist    : [100.00%]\n",
      "             Mask attn     : [True]\n",
      "             Weights       : 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "Preprocessing steps\n",
      "Step applied to ['higgs', 'tops', 'bottoms', 'Ws', 'quarks', 'Zs', 'neutrinos']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "  - higgs     : ('pt', 'mass')\n",
      "  - tops      : ('pt', 'mass')\n",
      "  - bottoms   : ('pt', 'mass')\n",
      "  - Ws        : ('pt', 'mass')\n",
      "  - quarks    : ('pt', 'mass')\n",
      "  - Zs        : ('pt', 'mass')\n",
      "  - neutrinos : ('pt', 'mass')\n",
      "Step applied to ['higgs', 'tops', 'bottoms', 'Ws', 'quarks', 'Zs', 'neutrinos']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\teta   : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "  - higgs     : ('pt', 'eta', 'mass')\n",
      "  - tops      : ('pt', 'eta', 'mass')\n",
      "  - bottoms   : ('pt', 'eta', 'mass')\n",
      "  - Ws        : ('pt', 'eta', 'mass')\n",
      "  - quarks    : ('pt', 'eta', 'mass')\n",
      "  - Zs        : ('pt', 'eta', 'mass')\n",
      "  - neutrinos : ('pt', 'eta', 'mass')\n",
      "\n",
      "Reco dataset with 114647 events\n",
      "Containing the following tensors\n",
      "jets  : data ([114647, 6, 5]), mask ([114647, 6])\n",
      "        Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%, 100.00%, 62.85%]\n",
      "        Mask attn     : [True, True, True, True, True, True]\n",
      "        Weights       : 114647.00, 114647.00, 114647.00, 114647.00, 114647.00, 114647.00\n",
      "        Features      : ['pt', 'eta', 'phi', 'mass', 'btag']\n",
      "        Selected for batches : True\n",
      "met   : data ([114647, 1, 4]), mask ([114647, 1])\n",
      "        Mask exist    : [100.00%]\n",
      "        Mask attn     : [True]\n",
      "        Weights       : 114647.00\n",
      "        Features      : ['pt', 'eta', 'phi', 'mass']\n",
      "        Selected for batches : True\n",
      "Preprocessing steps\n",
      "Step applied to ['jets']\n",
      "\tpt  : <class 'memflow.dataset.preprocessing.lowercutshift'>\n",
      "  - jets : ('pt',)\n",
      "Step applied to ['met']\n",
      "\tpt  : <class 'memflow.dataset.preprocessing.lowercutshift'>\n",
      "  - met : ('pt',)\n",
      "Step applied to ['jets', 'met']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "  - jets : ('pt', 'mass')\n",
      "  - met  : ('pt', 'mass')\n",
      "Step applied to ['jets', 'met']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\teta   : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "  - jets : ('pt', 'eta', 'mass')\n",
      "  - met  : ('pt', 'eta', 'mass')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_dataset = CombinedDataset(\n",
    "    hard_dataset=hard_dataset,\n",
    "    reco_dataset=reco_dataset,\n",
    ")\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76acc7c1-02e8-4c7f-8354-69090de0a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_loader = DataLoader(\n",
    "    combined_dataset,\n",
    "    batch_size = 10000,\n",
    "    shuffle = False,\n",
    "    num_workers = num_workers,\n",
    "    pin_memory = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0848b68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hard attention mask provided; will use existence mask only.\n",
      "Transformer args: will override `d_model` to 64\n",
      "('pt', 'eta', 'phi', 'mass')\n",
      "('pt', 'phi')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardCFM(\n",
       "  (hard_embeddings): ModuleList(\n",
       "    (0-2): 3 x Sequential(\n",
       "      (0): CircularEmbedding(\n",
       "        (linear): Linear(in_features=6, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (reco_embeddings): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): CircularEmbedding(\n",
       "        (linear): Linear(in_features=6, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CircularEmbedding(\n",
       "        (linear): Linear(in_features=5, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (state_embeddings): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (time_embedding): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (velocity_net): Sequential(\n",
       "    (0): Linear(in_features=70, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): SiLU()\n",
       "    (9): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (10): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): SiLU()\n",
       "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): SiLU()\n",
       "    (15): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (16): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): SiLU()\n",
       "    (18): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (19): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): SiLU()\n",
       "    (21): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (22): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): SiLU()\n",
       "    (24): Linear(in_features=512, out_features=5, bias=True)\n",
       "    (25): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StandardCFM.load_from_checkpoint(checkpoint_path=\"parallel_transfusion_checkpoints/model_epoch_500.ckpt\")\n",
    "model.to(accelerator)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e440a8-85ba-4044-8f44-e094e6a58347",
   "metadata": {},
   "source": [
    "# TransferCFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf5d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(inference_loader))\n",
    "batch = CFMSamplingCallback.move_batch_to_device(batch, model.device)\n",
    "\n",
    "# Print the structure of the batch\n",
    "print(\"Batch structure:\")\n",
    "for key in batch:\n",
    "    print(f\"{key}:\")\n",
    "    for sub_key, val in batch[key].items():\n",
    "        if isinstance(val, list):\n",
    "            print(f\"  {sub_key}: {[v.shape for v in val]}\")\n",
    "        elif isinstance(val, torch.Tensor):\n",
    "            print(f\"  {sub_key}: {val.shape}\")\n",
    "\n",
    "traj_samples_file = \"traj_samples.pt\"\n",
    "all_traj_file = \"all_traj.pt\"\n",
    "if os.path.exists(os.path.join(\"saved_samples\", traj_samples_file)) and os.path.exists(os.path.join(\"saved_samples\", all_traj_file)):\n",
    "    traj_samples = load_samples(traj_samples_file)\n",
    "    all_traj = load_samples(all_traj_file)\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        model = model.to(model.device)\n",
    "\n",
    "        # Extract the necessary inputs for sampling\n",
    "        hard_data = batch[\"hard\"][\"data\"]\n",
    "        hard_mask = batch[\"hard\"][\"mask\"]\n",
    "        reco_data = batch[\"reco\"][\"data\"]\n",
    "        reco_mask = batch[\"reco\"][\"mask\"]\n",
    "\n",
    "        print(f\"Hard data batch size: {hard_data[0].shape[0]}\")\n",
    "        print(f\"Reco data batch size: {reco_data[0].shape[0]}\")\n",
    "\n",
    "        traj_samples, all_traj = model.sample(\n",
    "            hard_data,\n",
    "            hard_mask,\n",
    "            reco_data,\n",
    "            reco_mask,\n",
    "            N_sample=1000,\n",
    "            steps=20,\n",
    "            store_trajectories=True\n",
    "        )\n",
    "    save_samples(traj_samples, traj_samples_file)\n",
    "    save_samples(all_traj, all_traj_file)\n",
    "    # Took 12 mins for 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e07bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_traj is the shape [N_sample, steps+1, B, sum_reco, 2], used for 2D trajectory plots\n",
    "# Feature indexes:\n",
    "#   Jets: \"pt\"=0, \"eta\"=1 \"phi\"=2\n",
    "#   MET: \"pt\"=0, \"phi\"=1\n",
    "plot_trajectories_2d(\n",
    "    all_traj,\n",
    "    model,\n",
    "    type_idx = 1, # \"Jets\"=0, \"met\"=1\n",
    "    feat_idx_x = 1,\n",
    "    feat_idx_y = 0,\n",
    "    num_events = 100,\n",
    "    mode = \"single_event\",\n",
    "    event_idx = 1,\n",
    "    object_idx = 0,\n",
    "    #preprocessing = combined_dataset.reco_dataset.preprocessing,\n",
    "    batch=batch,\n",
    ")\n",
    "plot_trajectories_2d(\n",
    "    all_traj,\n",
    "    model,\n",
    "    type_idx = 0, # \"Jets\"=0, \"met\"=1\n",
    "    feat_idx_x = 1,\n",
    "    feat_idx_y = 0,\n",
    "    num_events = 100,\n",
    "    mode = \"single_event\",\n",
    "    event_idx = 325,\n",
    "    object_idx = 0,\n",
    "    #preprocessing = combined_dataset.reco_dataset.preprocessing,\n",
    "    batch=batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories_grid(\n",
    "    all_traj,\n",
    "    model,\n",
    "    custom_timesteps=[4, 10, 20],\n",
    "    type_idx=0,\n",
    "    feat_idx_x=1,\n",
    "    feat_idx_y=0,\n",
    "    max_points=500,\n",
    "    event_idx=57,\n",
    "    object_idx=0,\n",
    "    batch=batch,\n",
    "    grid_size=14\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14eb29",
   "metadata": {},
   "source": [
    "# Main Distributions Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874239f",
   "metadata": {},
   "source": [
    "## Parallel Transfusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_samples_file = \"PT_samples_full.pt\"\n",
    "\n",
    "if os.path.exists(os.path.join(\"saved_samples\", PT_samples_file)):\n",
    "    PT_samples = load_samples(PT_samples_file)\n",
    "else:\n",
    "    train_frac = 0.8\n",
    "    indices = torch.arange(len(combined_dataset))\n",
    "    sep = int(train_frac*len(combined_dataset))\n",
    "    valid_indices = indices[sep:]\n",
    "    combined_dataset_valid = torch.utils.data.Subset(combined_dataset,valid_indices)\n",
    "    print (f'Dataset : validation {len(combined_dataset_valid)}')\n",
    "\n",
    "    # Use a DataLoader with batch_size=10 for testing (change as needed)\n",
    "    full_loader = DataLoader(\n",
    "        combined_dataset_valid,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Lists to accumulate outputs from each mini-batch\n",
    "    PT_samples_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Use enumerate to get the batch index along with the batch\n",
    "        for batch_idx, batch in enumerate(tqdm(full_loader, desc=\"Processing full dataset\")):\n",
    "\n",
    "            # Move the batch to the appropriate device\n",
    "            batch = CFMSamplingCallback.move_batch_to_device(batch, model.device)\n",
    "\n",
    "            # Extract inputs for the sampling function\n",
    "            hard_data = batch[\"hard\"][\"data\"]\n",
    "            hard_mask = batch[\"hard\"][\"mask\"]\n",
    "            reco_data = batch[\"reco\"][\"data\"]\n",
    "            reco_mask = batch[\"reco\"][\"mask\"]\n",
    "\n",
    "            # Run model sampling on the current batch\n",
    "            PT_samples = model.sample(\n",
    "                hard_data,\n",
    "                hard_mask,\n",
    "                reco_data,\n",
    "                reco_mask,\n",
    "                N_sample=100,      # number of samples per event\n",
    "                steps=20,\n",
    "            )\n",
    "\n",
    "            # Append outputs from this batch (each is a list of tensors)\n",
    "            PT_samples_list.append(PT_samples)\n",
    "\n",
    "    # Determine the number of elements per sample (assuming it's consistent across batches)\n",
    "    num_items = len(PT_samples_list[0])\n",
    "\n",
    "    # Concatenate corresponding elements across batches along the batch dimension (dim=2)\n",
    "    PT_samples = [\n",
    "        torch.cat([batch_output[i] for batch_output in PT_samples_list], dim=1)\n",
    "        for i in range(num_items)\n",
    "    ]\n",
    "\n",
    "    # Debugging prints\n",
    "    print(f\"Number of elements per sample: {num_items}\")\n",
    "    for i, sample in enumerate(PT_samples):\n",
    "        print(f\"Sample {i}: {sample.shape}\")\n",
    "\n",
    "    # Save the concatenated results\n",
    "    save_samples(PT_samples, PT_samples_file)\n",
    "\n",
    "# 88mins 23s for 100 samples 18364 events "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c678ed",
   "metadata": {},
   "source": [
    "## Transfermer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2867a1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'encoder_embeddings' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder_embeddings'])`.\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'decoder_embeddings' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder_embeddings'])`.\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'transformer' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transformer'])`.\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'flow' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['flow'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransferFlow(\n",
       "  (encoder_embeddings): MultiEmbeddings(\n",
       "    (embeddings): ModuleList(\n",
       "      (0-2): 3 x MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_embeddings): MultiEmbeddings(\n",
       "    (embeddings): ModuleList(\n",
       "      (0): MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (transformer): Transformer(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (activation): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-7): 8 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.0, inplace=False)\n",
       "            (activation): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (flow): KinematicFlow(\n",
       "    (flows): ModuleDict(\n",
       "      (pt): NSF(\n",
       "        (transform): LazyComposedTransform(\n",
       "          (0-4): 5 x ElementWiseTransform(\n",
       "            (base): MonotonicRQSTransform(bins=16)\n",
       "            (hyper): MLP(\n",
       "              (0): Linear(in_features=66, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (5): ReLU()\n",
       "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (base): UnconditionalDistribution(DiagNormal(loc: tensor([0.], device='cuda:0'), scale: tensor([1.], device='cuda:0')))\n",
       "      )\n",
       "      (eta): UniformNSF(\n",
       "        (transform): LazyComposedTransform(\n",
       "          (0-4): 5 x ElementWiseTransform(\n",
       "            (base): MonotonicRQSTransform(bins=16)\n",
       "            (hyper): MLP(\n",
       "              (0): Linear(in_features=66, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (5): ReLU()\n",
       "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (base): UnconditionalDistribution(BoxUniform(low: tensor([-1.], device='cuda:0'), high: tensor([1.], device='cuda:0')))\n",
       "      )\n",
       "      (phi): UniformNCSF(\n",
       "        (transform): LazyComposedTransform(\n",
       "          (0-4): 5 x ElementWiseTransform(\n",
       "            (base): ComposedTransform(\n",
       "              (0): CircularShiftTransform(bound=3.141592653589793)\n",
       "              (1): MonotonicRQSTransform(bins=16)\n",
       "            )\n",
       "            (hyper): MLP(\n",
       "              (0): Linear(in_features=66, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (5): ReLU()\n",
       "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (base): UnconditionalDistribution(BoxUniform(low: tensor([-3.1416], device='cuda:0'), high: tensor([3.1416], device='cuda:0')))\n",
       "      )\n",
       "      (mass): NSF(\n",
       "        (transform): LazyComposedTransform(\n",
       "          (0-4): 5 x ElementWiseTransform(\n",
       "            (base): MonotonicRQSTransform(bins=16)\n",
       "            (hyper): MLP(\n",
       "              (0): Linear(in_features=67, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (5): ReLU()\n",
       "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (base): UnconditionalDistribution(DiagNormal(loc: tensor([0.], device='cuda:0'), scale: tensor([1.], device='cuda:0')))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfermer_model = TransferFlow.load_from_checkpoint(\n",
    "    checkpoint_path=\"TransferFlow_checkpoints/model_epoch_500.ckpt\",\n",
    "    encoder_embeddings=MultiEmbeddings(\n",
    "        features_per_type=combined_dataset.hard_dataset.input_features,\n",
    "        embed_dims=[32, 64],\n",
    "        hidden_activation=nn.GELU,\n",
    "    ),\n",
    "    decoder_embeddings=MultiEmbeddings(\n",
    "        features_per_type=combined_dataset.reco_dataset.input_features,\n",
    "        embed_dims=[32, 64],\n",
    "        hidden_activation=nn.GELU,\n",
    "    ),\n",
    "    transformer=Transformer(\n",
    "        d_model=64,\n",
    "        encoder_layers=6,\n",
    "        decoder_layers=8,\n",
    "        nhead=8,\n",
    "        dim_feedforward=256,\n",
    "        activation=nn.GELU,\n",
    "        encoder_mask_attn=None,\n",
    "        decoder_mask_attn=combined_dataset.reco_dataset.attention_mask,\n",
    "        use_null_token=True,\n",
    "        dropout=0.0,\n",
    "    ),\n",
    "    flow=KinematicFlow(\n",
    "        d_model=64,\n",
    "        flow_mode='global',\n",
    "        flow_features=[\n",
    "            ['pt', 'eta', 'phi', 'mass'],  # jets\n",
    "            ['pt', 'phi'],  # met\n",
    "        ],\n",
    "        flow_classes={\n",
    "            'pt': zuko.flows.NSF,\n",
    "            'eta': UniformNSF,\n",
    "            'phi': UniformNCSF,\n",
    "            'mass': zuko.flows.NSF,\n",
    "        },\n",
    "        flow_common_args={\n",
    "            'bins': 16,\n",
    "            'transforms': 5,\n",
    "            'randperm': True,\n",
    "            'passes': None,\n",
    "            'hidden_features': [256] * 3,\n",
    "        },\n",
    "        flow_specific_args={\n",
    "            'eta': {'bound': 1.0},\n",
    "            'phi': {'bound': math.pi},\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "transfermer_model.to(accelerator)\n",
    "transfermer_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda39ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfermer_samples_file = \"Transfermer_samples_full.pt\"\n",
    "\n",
    "if os.path.exists(os.path.join(\"saved_samples\", transfermer_samples_file)):\n",
    "    transfermer_samples = load_samples(transfermer_samples_file)\n",
    "else:\n",
    "    # Split dataset into training and validation (using 80% training)\n",
    "    train_frac = 0.8\n",
    "    indices = torch.arange(len(combined_dataset))\n",
    "    sep = int(train_frac * len(combined_dataset))\n",
    "    valid_indices = indices[sep:]\n",
    "    combined_dataset_valid = torch.utils.data.Subset(combined_dataset, valid_indices)\n",
    "    print(f'Dataset : validation {len(combined_dataset_valid)}')\n",
    "\n",
    "    # Set up a DataLoader for the validation set (batch_size can be adjusted)\n",
    "    full_loader = DataLoader(\n",
    "        combined_dataset_valid,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # List to store samples from each mini-batch\n",
    "    transfermer_samples_list = []\n",
    "\n",
    "    # Loop over the validation batches without tracking gradients\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(full_loader, desc=\"Processing full dataset for Transfer Flow model\")):\n",
    "            # Transfer batch to the appropriate device using the model's method\n",
    "            batch = transfermer_model.transfer_batch_to_device(batch, transfermer_model.device, batch_idx)\n",
    "\n",
    "            # Sample using the Transfer Flow model; note we do not use a 'steps' argument here.\n",
    "            batch_samples = transfermer_model.sample(\n",
    "                batch['hard']['data'],\n",
    "                batch['hard']['mask'],\n",
    "                batch['reco']['data'],\n",
    "                batch['reco']['mask'],\n",
    "                N=100  # number of samples per event\n",
    "            )\n",
    "\n",
    "            # Permute each tensor so that the output shape is [batch, N_sample, particles, features]\n",
    "            batch_samples = [sample.cpu() for sample in batch_samples]\n",
    "\n",
    "            # Feature indices to keep\n",
    "            jets_indices = [0, 1, 2, 3]  # Keep ['pt', 'eta', 'phi', 'mass'], remove index 4 ('btag')\n",
    "            met_indices = [0, 2]         # Keep ['pt', 'phi'], remove indices 1 ('eta') and 3 ('mass')\n",
    "\n",
    "            # Apply feature selection\n",
    "            batch_samples[0] = batch_samples[0][..., jets_indices]  # Filter jet features\n",
    "            batch_samples[1] = batch_samples[1][..., met_indices]   # Filter MET features\n",
    "\n",
    "            transfermer_samples_list.append(batch_samples)\n",
    "\n",
    "    # Concatenate corresponding tensors from all batches along the batch dimension\n",
    "    num_items = len(transfermer_samples_list[0])\n",
    "    transfermer_samples = [\n",
    "        torch.cat([batch_output[i] for batch_output in transfermer_samples_list], dim=1)\n",
    "        for i in range(num_items)\n",
    "    ]\n",
    "\n",
    "    # Debugging prints\n",
    "    print(f\"Number of elements per sample: {num_items}\")\n",
    "    for i, sample in enumerate(transfermer_samples):\n",
    "        print(f\"Sample {i}: {sample.shape}\")\n",
    "\n",
    "    # Save the concatenated samples for later plotting/comparison\n",
    "    save_samples(transfermer_samples, transfermer_samples_file)\n",
    "\n",
    "# 10mins 25s for 100 samples 18364 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c09abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_loader = DataLoader(\n",
    "    combined_dataset_valid,\n",
    "    batch_size = 20000,\n",
    "    shuffle = False,\n",
    "    num_workers = 0,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "full_batch = next(iter(full_loader))\n",
    "full_batch = CFMSamplingCallback.move_batch_to_device(full_batch, model.device)\n",
    "print(f'Number of batches: {len(full_loader)}')\n",
    "print(f\"batch_size: {len(full_batch['hard']['data'][0])}\")\n",
    "# Get the reco mask for the same index.\n",
    "real_data_full = full_batch[\"reco\"][\"data\"]\n",
    "real_mask_full = full_batch[\"reco\"][\"mask\"]\n",
    "\n",
    "# # ========== MET ==========\n",
    "# compare_distributions(model, real_data_full, samples_full,\n",
    "#                                  ptype_idx=1,\n",
    "#                                  feat_idx=0,\n",
    "#                                  feat_name=r\"$p_T$ [GeV]\",\n",
    "#                                  preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "#                                  real_mask=real_mask_full,\n",
    "#                                  log_scale=True,\n",
    "#                                  nbins=100)\n",
    "# compare_distributions(model, real_data_full, samples_full,\n",
    "#                                  ptype_idx=1,\n",
    "#                                  feat_idx=1,\n",
    "#                                  feat_name=r\"$\\phi$ [rad]\",\n",
    "#                                  preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "#                                  real_mask=real_mask_full,\n",
    "#                                  nbins=100)\n",
    "\n",
    "# ========= JETS =========\n",
    "# compare_distributions(model, real_data_full, samples_full,\n",
    "#                                  ptype_idx=0,\n",
    "#                                  feat_idx=0,\n",
    "#                                  feat_name=r\"$p_T$ [GeV]\",\n",
    "#                                  preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "#                                  real_mask=real_mask_full,\n",
    "#                                  log_scale=True,\n",
    "#                                  nbins=100)\n",
    "# compare_distributions(model, real_data_full, samples_full,\n",
    "#                                  ptype_idx=0,\n",
    "#                                  feat_idx=1,\n",
    "#                                  feat_name=r\"$\\eta$\",\n",
    "#                                  preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "#                                  real_mask=real_mask_full,\n",
    "#                                  log_scale=True,\n",
    "#                                  nbins=100)\n",
    "# compare_distributions(model, real_data_full, samples_full,\n",
    "#                                  ptype_idx=0,\n",
    "#                                  feat_idx=2,\n",
    "#                                  feat_name=r\"$\\phi$ [rad]\",\n",
    "#                                  preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "#                                  real_mask=real_mask_full,\n",
    "#                                  nbins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jets\n",
    "compare_distributions_multiple(model, real_data_full,\n",
    "                                 transfermer_samples,\n",
    "                                 PT_samples,\n",
    "                                 ptype_idx=0,\n",
    "                                 feat_idx=0,\n",
    "                                 feat_name=r\"$p_T$ [GeV]\",\n",
    "                                 preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "                                 real_mask=real_mask_full,\n",
    "                                 log_scale=True,\n",
    "                                 nbins=1000)\n",
    "compare_distributions_multiple(model, real_data_full,\n",
    "                                 transfermer_samples,\n",
    "                                 PT_samples,\n",
    "                                 ptype_idx=0,\n",
    "                                 feat_idx=1,\n",
    "                                 feat_name=r\"$\\eta$\",\n",
    "                                 preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "                                 real_mask=real_mask_full,\n",
    "                                 log_scale=True,\n",
    "                                 nbins=50)\n",
    "compare_distributions_multiple(model, real_data_full,\n",
    "                                 transfermer_samples,\n",
    "                                 PT_samples,\n",
    "                                 ptype_idx=0,\n",
    "                                 feat_idx=2,\n",
    "                                 feat_name=r\"$\\phi$ [rad]\",\n",
    "                                 preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "                                 real_mask=real_mask_full,\n",
    "                                 nbins=50)\n",
    "compare_distributions_multiple(model, real_data_full,\n",
    "                                 transfermer_samples,\n",
    "                                 PT_samples,\n",
    "                                 ptype_idx=0,\n",
    "                                 feat_idx=3,\n",
    "                                 feat_name=r\"Mass [GeV]\",\n",
    "                                 preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "                                 real_mask=real_mask_full,\n",
    "                                 log_scale=True,\n",
    "                                 nbins=400)\n",
    "# MET\n",
    "# compare_distributions_multiple(model, real_data_full,\n",
    "#                                  transfermer_samples,\n",
    "#                                  PT_samples,\n",
    "#                                  ptype_idx=1,\n",
    "#                                  feat_idx=0,\n",
    "#                                  feat_name=r\"$p_T$ [GeV]\",\n",
    "#                                  preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "#                                  real_mask=real_mask_full,\n",
    "#                                  log_scale=True,\n",
    "#                                  nbins=500)\n",
    "# compare_distributions_multiple(model, real_data_full,\n",
    "#                                  transfermer_samples,\n",
    "#                                  PT_samples,\n",
    "#                                  ptype_idx=1,\n",
    "#                                  feat_idx=1,\n",
    "#                                  feat_name=r\"$\\phi$ [rad]\",\n",
    "#                                  preprocessing=combined_dataset.reco_dataset.preprocessing,\n",
    "#                                  real_mask=real_mask_full,\n",
    "#                                  nbins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a654eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks to make plots within comet\n",
    "bias = BiasCallback(\n",
    "    dataset = combined_dataset,               # dataset on which to evaluate bias\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing, # preprocessing pipeline to draw raw variables\n",
    "    N_sample = 100,                                 # number of samples to draw\n",
    "    steps = 20,                                     # Number of bridging steps\n",
    "    store_trajectories = False,                     # To save trajectories plots\n",
    "    frequency = 50,                                 # plotting frequency (epochs)\n",
    "    raw = True,\n",
    "    bins = 101,                                      # 1D/2D plot number of bins\n",
    "    points = 20,                                    # Number of points for the quantile\n",
    "    log_scale = True,                               # log\n",
    "    batch_size = 1000,                              # Batch size to evaluate the dataset (internally makes a loaded)\n",
    "    N_batch = 1,                                   # Stop after N batches (makes it faster)\n",
    "    suffix = 'ttH',                                 # name for plots\n",
    "    label_names = {                             # makes nicer labels\n",
    "        'pt' : 'p_T',\n",
    "        'eta' : '\\eta',\n",
    "        'phi' : '\\phi',\n",
    "    },\n",
    ")\n",
    "\n",
    "sampling = SamplingCallback(\n",
    "    dataset = combined_dataset,           # dataset to check sampling\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing, # preprocessing pipeline\n",
    "    idx_to_monitor = [1,2,3,4,5,6,7,8,9,10],               # idx of events in dataset to make plots with\n",
    "    N_sample = 10000,                          # number of samples to draw\n",
    "    steps = 20,                                     # Number of bridging steps\n",
    "    store_trajectories = False,                     # To save trajectories plots\n",
    "    frequency = 50,                             # plotting frequency (epochs)\n",
    "    bins = 31,                                  # 1D/2D plot number of bins\n",
    "    log_scale = True,                           # log\n",
    "    label_names = {                             # makes nicer labels\n",
    "        'pt' : r'$p_T$ [GeV]',\n",
    "        'eta' : r'$\\eta$',\n",
    "        'phi' : r'$\\phi$ [rad]',\n",
    "    },\n",
    "    pt_range = 350,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a49e6",
   "metadata": {},
   "source": [
    "# Sampling 2D CFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a7a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples loaded from saved_samples/samples.pt\n"
     ]
    }
   ],
   "source": [
    "samples_file = \"samples.pt\"\n",
    "if os.path.exists(os.path.join(\"saved_samples\", samples_file)):\n",
    "    samples = load_samples(samples_file)\n",
    "else:\n",
    "    device = model.device\n",
    "    sampling.set_idx(sampling.idx_to_monitor)\n",
    "    hard_data = [d.to(device) for d in sampling.batch['hard']['data']]\n",
    "    hard_mask = [m.to(device) for m in sampling.batch['hard']['mask']]\n",
    "    reco_data = [d.to(device) for d in sampling.batch['reco']['data']]\n",
    "    reco_mask = [m.to(device) for m in sampling.batch['reco']['mask']]\n",
    "\n",
    "    print(f\"Hard data batch size: {hard_data[0].shape[0]}\")\n",
    "    print(f\"Reco data batch size: {reco_data[0].shape[0]}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = model.to(model.device)\n",
    "        samples = model.sample(\n",
    "                            hard_data, hard_mask,\n",
    "                            reco_data, reco_mask,\n",
    "                            sampling.N_sample,\n",
    "                            sampling.steps,\n",
    "                            sampling.store_trajectories\n",
    "                        )\n",
    "    # Debugging prints\n",
    "    num_items = len(samples)\n",
    "    print(f\"Number of elements per sample: {num_items}\")\n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Sample {i}: {sample.shape}\")\n",
    "    save_samples(samples, samples_file)\n",
    "\n",
    "# 42mins 14s for 10 events, 10000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54eb8ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Figures/event_0_obj_jets_0.png\n",
      "Saved: Figures/event_0_obj_jets_1.png\n",
      "Saved: Figures/event_0_obj_jets_2.png\n",
      "Saved: Figures/event_0_obj_jets_3.png\n",
      "Saved: Figures/event_0_obj_jets_4.png\n",
      "Saved: Figures/event_0_obj_jets_5.png\n",
      "Saved: Figures/event_0_obj_met_0.png\n",
      "Saved: Figures/event_1_obj_jets_0.png\n",
      "Saved: Figures/event_1_obj_jets_1.png\n",
      "Saved: Figures/event_1_obj_jets_2.png\n",
      "Saved: Figures/event_1_obj_jets_3.png\n",
      "Saved: Figures/event_1_obj_jets_4.png\n",
      "Saved: Figures/event_1_obj_met_0.png\n",
      "Saved: Figures/event_2_obj_jets_0.png\n",
      "Saved: Figures/event_2_obj_jets_1.png\n",
      "Saved: Figures/event_2_obj_jets_2.png\n",
      "Saved: Figures/event_2_obj_jets_3.png\n",
      "Saved: Figures/event_2_obj_jets_4.png\n",
      "Saved: Figures/event_2_obj_met_0.png\n",
      "Saved: Figures/event_3_obj_jets_0.png\n",
      "Saved: Figures/event_3_obj_jets_1.png\n",
      "Saved: Figures/event_3_obj_jets_2.png\n",
      "Saved: Figures/event_3_obj_jets_3.png\n",
      "Saved: Figures/event_3_obj_jets_4.png\n",
      "Saved: Figures/event_3_obj_met_0.png\n",
      "Saved: Figures/event_4_obj_jets_0.png\n",
      "Saved: Figures/event_4_obj_jets_1.png\n",
      "Saved: Figures/event_4_obj_jets_2.png\n",
      "Saved: Figures/event_4_obj_jets_3.png\n",
      "Saved: Figures/event_4_obj_jets_4.png\n",
      "Saved: Figures/event_4_obj_jets_5.png\n",
      "Saved: Figures/event_4_obj_met_0.png\n",
      "Saved: Figures/event_5_obj_jets_0.png\n",
      "Saved: Figures/event_5_obj_jets_1.png\n",
      "Saved: Figures/event_5_obj_jets_2.png\n",
      "Saved: Figures/event_5_obj_jets_3.png\n",
      "Saved: Figures/event_5_obj_jets_4.png\n",
      "Saved: Figures/event_5_obj_jets_5.png\n",
      "Saved: Figures/event_5_obj_met_0.png\n",
      "Saved: Figures/event_6_obj_jets_0.png\n",
      "Saved: Figures/event_6_obj_jets_1.png\n",
      "Saved: Figures/event_6_obj_jets_2.png\n",
      "Saved: Figures/event_6_obj_jets_3.png\n",
      "Saved: Figures/event_6_obj_jets_4.png\n",
      "Saved: Figures/event_6_obj_jets_5.png\n",
      "Saved: Figures/event_6_obj_met_0.png\n",
      "Saved: Figures/event_7_obj_jets_0.png\n",
      "Saved: Figures/event_7_obj_jets_1.png\n",
      "Saved: Figures/event_7_obj_jets_2.png\n",
      "Saved: Figures/event_7_obj_jets_3.png\n",
      "Saved: Figures/event_7_obj_jets_4.png\n",
      "Saved: Figures/event_7_obj_jets_5.png\n",
      "Saved: Figures/event_7_obj_met_0.png\n",
      "Saved: Figures/event_8_obj_jets_0.png\n",
      "Saved: Figures/event_8_obj_jets_1.png\n",
      "Saved: Figures/event_8_obj_jets_2.png\n",
      "Saved: Figures/event_8_obj_jets_3.png\n",
      "Saved: Figures/event_8_obj_jets_4.png\n",
      "Saved: Figures/event_8_obj_jets_5.png\n",
      "Saved: Figures/event_8_obj_met_0.png\n",
      "Saved: Figures/event_9_obj_jets_0.png\n",
      "Saved: Figures/event_9_obj_jets_1.png\n",
      "Saved: Figures/event_9_obj_jets_2.png\n",
      "Saved: Figures/event_9_obj_jets_3.png\n",
      "Saved: Figures/event_9_obj_jets_4.png\n",
      "Saved: Figures/event_9_obj_met_0.png\n"
     ]
    }
   ],
   "source": [
    "figures = sampling.make_sampling_plots(model,show=True,external_samples=samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46a59d",
   "metadata": {},
   "source": [
    "# Sampling 2D Transfermer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bd5e106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples loaded from saved_samples/Transfermer_samples.pt\n"
     ]
    }
   ],
   "source": [
    "transfermer_samples_file = \"Transfermer_samples.pt\"\n",
    "\n",
    "if os.path.exists(os.path.join(\"saved_samples\", transfermer_samples_file)):\n",
    "    transfermer_samples = load_samples(transfermer_samples_file)\n",
    "else:\n",
    "    device = transfermer_model.device\n",
    "    sampling.set_idx(sampling.idx_to_monitor)\n",
    "\n",
    "    # Move input data to the correct device\n",
    "    hard_data = [d.to(device) for d in sampling.batch['hard']['data']]\n",
    "    hard_mask = [m.to(device) for m in sampling.batch['hard']['mask']]\n",
    "    reco_data = [d.to(device) for d in sampling.batch['reco']['data']]\n",
    "    reco_mask = [m.to(device) for m in sampling.batch['reco']['mask']]\n",
    "\n",
    "    print(f\"Hard data batch size: {hard_data[0].shape[0]}\")\n",
    "    print(f\"Reco data batch size: {reco_data[0].shape[0]}\")\n",
    "\n",
    "    # Number of samples per event\n",
    "    total_samples = sampling.N_sample  \n",
    "    batch_size = 10  # Number of samples per batch to avoid memory issues\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size  # Ensure full coverage\n",
    "\n",
    "    # Storage for results\n",
    "    accumulated_samples = [None, None]  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Sampling batches\", unit=\"batch\"):\n",
    "            # Determine the number of samples to generate in this batch\n",
    "            current_N = min(batch_size, total_samples - batch_idx * batch_size)\n",
    "\n",
    "            # Generate samples\n",
    "            batch_samples = transfermer_model.sample(\n",
    "                hard_data,\n",
    "                hard_mask,\n",
    "                reco_data,\n",
    "                reco_mask,\n",
    "                N=current_N\n",
    "            )\n",
    "\n",
    "            # Feature selection\n",
    "            jets_indices = [0, 1, 2, 3]  # ['pt', 'eta', 'phi', 'mass']\n",
    "            met_indices = [0, 2]         # ['pt', 'phi']\n",
    "\n",
    "            batch_samples[0] = batch_samples[0][..., jets_indices]  # Filter jet features\n",
    "            batch_samples[1] = batch_samples[1][..., met_indices]   # Filter MET features\n",
    "\n",
    "            # Accumulate results\n",
    "            if accumulated_samples[0] is None:\n",
    "                accumulated_samples[0] = batch_samples[0].cpu()\n",
    "                accumulated_samples[1] = batch_samples[1].cpu()\n",
    "            else:\n",
    "                accumulated_samples[0] = torch.cat((accumulated_samples[0], batch_samples[0].cpu()), dim=0)\n",
    "                accumulated_samples[1] = torch.cat((accumulated_samples[1], batch_samples[1].cpu()), dim=0)\n",
    "\n",
    "    # Convert final results to the expected format\n",
    "    transfermer_samples = accumulated_samples\n",
    "\n",
    "    # Debugging prints\n",
    "    num_items = len(transfermer_samples)\n",
    "    print(f\"Number of elements per sample: {num_items}\")\n",
    "    for i, sample in enumerate(transfermer_samples):\n",
    "        print(f\"Sample {i}: {sample.shape}\")\n",
    "\n",
    "    # Save the concatenated samples\n",
    "    save_samples(transfermer_samples, transfermer_samples_file)\n",
    "\n",
    "# 3mins 27s, 10 events, 10000 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e85cf0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Figures_transfermer/event_0_obj_jets_0.png\n",
      "Saved: Figures_transfermer/event_0_obj_jets_1.png\n",
      "Saved: Figures_transfermer/event_0_obj_jets_2.png\n",
      "Saved: Figures_transfermer/event_0_obj_jets_3.png\n",
      "Saved: Figures_transfermer/event_0_obj_jets_4.png\n",
      "Saved: Figures_transfermer/event_0_obj_jets_5.png\n",
      "Saved: Figures_transfermer/event_0_obj_met_0.png\n",
      "Saved: Figures_transfermer/event_1_obj_jets_0.png\n",
      "Saved: Figures_transfermer/event_1_obj_jets_1.png\n",
      "Saved: Figures_transfermer/event_1_obj_jets_2.png\n",
      "Saved: Figures_transfermer/event_1_obj_jets_3.png\n",
      "Saved: Figures_transfermer/event_1_obj_jets_4.png\n",
      "Saved: Figures_transfermer/event_1_obj_met_0.png\n",
      "Saved: Figures_transfermer/event_2_obj_jets_0.png\n",
      "Saved: Figures_transfermer/event_2_obj_jets_1.png\n",
      "Saved: Figures_transfermer/event_2_obj_jets_2.png\n",
      "Saved: Figures_transfermer/event_2_obj_jets_3.png\n",
      "Saved: Figures_transfermer/event_2_obj_jets_4.png\n",
      "Saved: Figures_transfermer/event_2_obj_met_0.png\n",
      "Saved: Figures_transfermer/event_3_obj_jets_0.png\n",
      "Saved: Figures_transfermer/event_3_obj_jets_1.png\n",
      "Saved: Figures_transfermer/event_3_obj_jets_2.png\n",
      "Saved: Figures_transfermer/event_3_obj_jets_3.png\n",
      "Saved: Figures_transfermer/event_3_obj_jets_4.png\n",
      "Saved: Figures_transfermer/event_3_obj_met_0.png\n",
      "Saved: Figures_transfermer/event_4_obj_jets_0.png\n",
      "Saved: Figures_transfermer/event_4_obj_jets_1.png\n",
      "Saved: Figures_transfermer/event_4_obj_jets_2.png\n",
      "Saved: Figures_transfermer/event_4_obj_jets_3.png\n",
      "Saved: Figures_transfermer/event_4_obj_jets_4.png\n",
      "Saved: Figures_transfermer/event_4_obj_jets_5.png\n",
      "Saved: Figures_transfermer/event_4_obj_met_0.png\n",
      "Saved: Figures_transfermer/event_5_obj_jets_0.png\n",
      "Saved: Figures_transfermer/event_5_obj_jets_1.png\n",
      "Saved: Figures_transfermer/event_5_obj_jets_2.png\n",
      "Saved: Figures_transfermer/event_5_obj_jets_3.png\n",
      "Saved: Figures_transfermer/event_5_obj_jets_4.png\n",
      "Saved: Figures_transfermer/event_5_obj_jets_5.png\n",
      "Saved: Figures_transfermer/event_5_obj_met_0.png\n",
      "Saved: Figures_transfermer/event_6_obj_jets_0.png\n",
      "Saved: Figures_transfermer/event_6_obj_jets_1.png\n",
      "Saved: Figures_transfermer/event_6_obj_jets_2.png\n",
      "Saved: Figures_transfermer/event_6_obj_jets_3.png\n",
      "Saved: Figures_transfermer/event_6_obj_jets_4.png\n",
      "Saved: Figures_transfermer/event_6_obj_jets_5.png\n",
      "Saved: Figures_transfermer/event_6_obj_met_0.png\n",
      "Saved: Figures_transfermer/event_7_obj_jets_0.png\n",
      "Saved: Figures_transfermer/event_7_obj_jets_1.png\n",
      "Saved: Figures_transfermer/event_7_obj_jets_2.png\n",
      "Saved: Figures_transfermer/event_7_obj_jets_3.png\n",
      "Saved: Figures_transfermer/event_7_obj_jets_4.png\n",
      "Saved: Figures_transfermer/event_7_obj_jets_5.png\n",
      "Saved: Figures_transfermer/event_7_obj_met_0.png\n",
      "Saved: Figures_transfermer/event_8_obj_jets_0.png\n",
      "Saved: Figures_transfermer/event_8_obj_jets_1.png\n",
      "Saved: Figures_transfermer/event_8_obj_jets_2.png\n",
      "Saved: Figures_transfermer/event_8_obj_jets_3.png\n",
      "Saved: Figures_transfermer/event_8_obj_jets_4.png\n",
      "Saved: Figures_transfermer/event_8_obj_jets_5.png\n",
      "Saved: Figures_transfermer/event_8_obj_met_0.png\n",
      "Saved: Figures_transfermer/event_9_obj_jets_0.png\n",
      "Saved: Figures_transfermer/event_9_obj_jets_1.png\n",
      "Saved: Figures_transfermer/event_9_obj_jets_2.png\n",
      "Saved: Figures_transfermer/event_9_obj_jets_3.png\n",
      "Saved: Figures_transfermer/event_9_obj_jets_4.png\n",
      "Saved: Figures_transfermer/event_9_obj_met_0.png\n"
     ]
    }
   ],
   "source": [
    "figures = sampling.make_sampling_plots(model,show=True,external_samples=transfermer_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46abba3",
   "metadata": {},
   "source": [
    "# Bias Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=model.device\n",
    "\n",
    "bias_samples_file = \"bias_samples.pt\"\n",
    "if os.path.exists(os.path.join(\"saved_samples\", bias_samples_file)):\n",
    "    bias_samples = load_samples(bias_samples_file)\n",
    "else:\n",
    "    for batch_idx, batch in tqdm(enumerate(bias.loader),desc='Predict',disable=False,leave=True,total=min(bias.N_batch,len(bias.loader)),position=0):\n",
    "        if batch_idx >= bias.N_batch:\n",
    "            break\n",
    "\n",
    "        # Get parts #\n",
    "        hard_data = [data.to(device) for data in batch['hard']['data']]\n",
    "        hard_mask_exist = [mask.to(device) for mask in batch['hard']['mask']]\n",
    "        reco_data = [data.to(device) for data in batch['reco']['data']]\n",
    "        reco_mask_exist = [mask.to(device) for mask in batch['reco']['mask']]\n",
    "\n",
    "        print(f\"Hard data batch size: {hard_data[0].shape[0]}\")\n",
    "        print(f\"Reco data batch size: {reco_data[0].shape[0]}\")\n",
    "\n",
    "        # Sample #\n",
    "        with torch.no_grad():\n",
    "            bias_samples = model.sample(\n",
    "                hard_data, hard_mask_exist,\n",
    "                reco_data, reco_mask_exist,\n",
    "                bias.N_sample,\n",
    "                bias.steps,\n",
    "                bias.store_trajectories,\n",
    "            )\n",
    "        save_samples(bias_samples, bias_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = bias.make_bias_plots(model,show=True,external_samples=bias_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
