{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f754e5-5310-4717-8700-8dc85ecf889f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7bc2a-42e5-48ff-8ddd-a01ed240d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import dask\n",
    "\n",
    "import vector\n",
    "import particle\n",
    "import hepunits\n",
    "\n",
    "import comet_ml\n",
    "import zuko\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import SequentialLR, LambdaLR, CosineAnnealingLR\n",
    "\n",
    "import multiprocessing\n",
    "import uuid\n",
    "\n",
    "from memflow.dataset.data import ParquetData\n",
    "from memflow.dataset.dataset import CombinedDataset\n",
    "from memflow.ttH.ttH_dataclasses import ttHHardDataset, ttHRecoDataset\n",
    "from memflow.ttH.models.transfer_flow_callbacks import SamplingCallback, BiasCallback\n",
    "\n",
    "from memflow.ttH.models.TransferCFM import StandardCFM as TransferCFM\n",
    "from memflow.ttH.models.Transfusion import StandardCFM as Transfusion\n",
    "from memflow.ttH.models.ParallelTransfusion import StandardCFM as ParallelTransfusion\n",
    "from memflow.ttH.models.TransferCFM_original import StandardCFM as OriginalCFM\n",
    "from models.callbacks import ModelCheckpoint\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "num_workers = min(16, multiprocessing.cpu_count())  # Use up to 16 CPU cores\n",
    "print(f'Number of CPU workers for dataloading: {num_workers}')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"  # Change \"<n>\" to the index of the GPU you want to use on node\n",
    "\n",
    "print (f\"Running on GPU : {torch.cuda.is_available()}\")\n",
    "accelerator = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (f\"Accelerator : {accelerator}\")\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "if accelerator =='cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print (torch.cuda.memory_summary(device=None, abbreviated=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c50d80-9342-47c6-9efd-87c39a91bc5b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a68956-c2a9-4fbd-b11a-9025f8339cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_hard = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    # N = int(1e5),\n",
    ")\n",
    "\n",
    "print (data_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35dfc61-246f-444b-930a-e5cdd77fd3c9",
   "metadata": {},
   "source": [
    "# Hard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd35299-f40b-4864-9132-67a95e03acd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hard_dataset = ttHHardDataset(\n",
    "    data = data_hard,\n",
    "    selection = [\n",
    "        # 'higgs',\n",
    "        # 'tops',\n",
    "        'bottoms',\n",
    "        # 'Ws',\n",
    "        # 'Zs',\n",
    "        'quarks',\n",
    "        'neutrinos',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "\n",
    "obj = hard_dataset.objects['neutrinos']\n",
    "\n",
    "# Check its type\n",
    "print(\"Type of neutrinos object:\", type(obj))\n",
    "\n",
    "# If itâ€™s a tuple, print its length\n",
    "if isinstance(obj, tuple):\n",
    "    print(\"Length of tuple:\", len(obj))\n",
    "    for i, element in enumerate(obj):\n",
    "        print(f\"Element {i} has type {type(element)} and shape/length:\", end=\" \")\n",
    "        # If element is a torch.Tensor or numpy array:\n",
    "        if hasattr(element, 'shape'):\n",
    "            print(element.shape)\n",
    "        # If it's a list of fields:\n",
    "        else:\n",
    "            print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19d008-7658-4b9e-b3c1-463ec3401a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Before preprocessing')\n",
    "hard_dataset.plot(selection=True,raw=True)\n",
    "print ('After preprocessing')\n",
    "hard_dataset.plot(selection=True,raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47776673-da9c-4c0d-b92d-39a73349df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not strictly necessary, but just to make sure loading works as expected\n",
    "# We will use later a combined dataset (hard+reco) below\n",
    "hard_loader = DataLoader(\n",
    "    hard_dataset,\n",
    "    batch_size = 32,\n",
    "    num_workers = num_workers, # Parallel loading\n",
    "    pin_memory = True, # Faster transfer to GPU\n",
    ")\n",
    "batch = next(iter(hard_loader))\n",
    "\n",
    "for obj,mask,sel in zip(batch['data'],batch['mask'],hard_loader.dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c82aff-8a32-4f83-b199-0c7725639eda",
   "metadata": {},
   "source": [
    "# Reco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc523344",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reco = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    #N = data_hard.N,\n",
    ")\n",
    "\n",
    "print(data_reco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ece5f1",
   "metadata": {},
   "source": [
    "Have a look at athe minimum values for Jet and MET pT in the raw dataset. This can give an indication as to what the cutoff in the SR is and hence what to set the `'pt':lowercutshift()` to in the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MET pt from raw dataset\n",
    "raw_met_pt = data_reco['InputMet_pt']\n",
    "# Print min and max MET pt\n",
    "print(\"Raw Minimum MET pt:\", ak.min(raw_met_pt))\n",
    "print(\"Raw Maximum MET pt:\", ak.max(raw_met_pt))\n",
    "\n",
    "# Extract Jet pt from raw dataset\n",
    "raw_jet_pt = data_reco['cleanedJet_pt']\n",
    "# Print min and max MET pt\n",
    "print(\"Raw Minimum MET pt:\", ak.min(raw_jet_pt))\n",
    "print(\"Raw Maximum MET pt:\", ak.max(raw_jet_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781dae8-05ad-462e-b91a-bc544337a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_dataset = ttHRecoDataset(\n",
    "    data = data_reco,\n",
    "    selection = [\n",
    "        'jets',\n",
    "        'met',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "print(reco_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0ac35-78bf-45a7-ab31-d1d8884f4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Before preprocessing')\n",
    "reco_dataset.plot(selection=True,raw=True,log=True)\n",
    "print ('After preprocessing')\n",
    "reco_dataset.plot(selection=True,raw=False,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290aa9f-1bed-4c0e-b461-73f2c465db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also not needed, just checking \n",
    "reco_loader = DataLoader(\n",
    "    reco_dataset,\n",
    "    batch_size = 32,\n",
    "    num_workers = num_workers,\n",
    "    pin_memory = True,\n",
    ")\n",
    "batch = next(iter(reco_loader))\n",
    "\n",
    "for obj,mask,sel in zip(batch['data'],batch['mask'],reco_loader.dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc702e-66b1-450a-86d0-a28192f99d98",
   "metadata": {},
   "source": [
    "# Combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a63d7-eafc-43ef-8147-d0887d4bceec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Intersection Branch: {reco_dataset.intersection_branch}')\n",
    "print (f'Hard Datset keys: {hard_dataset.metadata.keys()}')\n",
    "print (f'Reco Datset keys: {reco_dataset.metadata.keys()}')\n",
    "\n",
    "combined_dataset = CombinedDataset(\n",
    "    hard_dataset=hard_dataset,\n",
    "    reco_dataset=reco_dataset,\n",
    ")\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acc7c1-02e8-4c7f-8354-69090de0a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_loader = DataLoader(\n",
    "    combined_dataset,\n",
    "    batch_size = 256,\n",
    ")\n",
    "batch = next(iter(combined_loader))\n",
    "\n",
    "print ('Reco')\n",
    "for obj,mask,sel in zip(batch['reco']['data'],batch['reco']['mask'],combined_loader.dataset.reco_dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)\n",
    "\n",
    "print ('Hard')\n",
    "for obj,mask,sel in zip(batch['hard']['data'],batch['hard']['mask'],combined_loader.dataset.hard_dataset.selection):\n",
    "    print (sel,obj.shape,mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d4124-4353-4233-a521-81f2b94f9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation\n",
    "# Not randomly for reproducilibility, but just based on number\n",
    "\n",
    "train_frac = 0.8\n",
    "indices = torch.arange(len(combined_dataset))\n",
    "sep = int(train_frac*len(combined_dataset))\n",
    "train_indices = indices[:sep]\n",
    "valid_indices = indices[sep:]\n",
    "\n",
    "combined_dataset_train = torch.utils.data.Subset(combined_dataset,train_indices)\n",
    "combined_dataset_valid = torch.utils.data.Subset(combined_dataset,valid_indices)\n",
    "print (f'Dataset : training {len(combined_dataset_train)} / validation {len(combined_dataset_valid)}')\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "combined_loader_train = DataLoader(\n",
    "    combined_dataset_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers,\n",
    "    pin_memory = True,\n",
    ")\n",
    "combined_loader_valid = DataLoader(\n",
    "    combined_dataset_valid,\n",
    "    batch_size = 10000,\n",
    "    shuffle = False,\n",
    "    num_workers = num_workers,\n",
    "    pin_memory = True,\n",
    ")\n",
    "print (f'Batching {len(combined_loader_train)} / Validation {len(combined_loader_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e440a8-85ba-4044-8f44-e094e6a58347",
   "metadata": {},
   "source": [
    "# TransferCFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbfe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (obj, mask) in enumerate(zip(batch['reco']['data'], batch['reco']['mask'])):\n",
    "    print(f\"Reco Object {i}: Shape = {obj.shape}, Mask Shape = {mask.shape}\")\n",
    "for i, (obj, mask) in enumerate(zip(batch['hard']['data'], batch['hard']['mask'])):\n",
    "    print(f\"Hard Object {i}: Shape = {obj.shape}, Mask Shape = {mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d61c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_dataset.hard_dataset.input_features)\n",
    "print(combined_dataset.reco_dataset.input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4980367",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ParallelTransfusion(\n",
    "    embed_dims=[32, 64], # was [32, 64]\n",
    "    embed_act=nn.GELU,\n",
    "    dropout=0.0,\n",
    "\n",
    "    n_hard_particles_per_type=combined_dataset.hard_dataset.number_particles_per_type,\n",
    "    hard_particle_type_names=combined_dataset.hard_dataset.selection,\n",
    "    hard_input_features_per_type=combined_dataset.hard_dataset.input_features, # These are all the features available, speciefied in dataclass\n",
    "\n",
    "    n_reco_particles_per_type=combined_dataset.reco_dataset.number_particles_per_type,\n",
    "    reco_particle_type_names= combined_dataset.reco_dataset.selection,\n",
    "    reco_input_features_per_type=combined_dataset.reco_dataset.input_features,\n",
    "\n",
    "    # Only pick a subset in bridging:\n",
    "    flow_input_features = [\n",
    "        [\"pt\", \"eta\", \"phi\", \"mass\"],  # Features for reco type 0 (e.g., jets)\n",
    "        [\"pt\", \"phi\"],         # Features for reco type 1 (e.g., MET)\n",
    "        # Add more reco types as needed\n",
    "    ],\n",
    "\n",
    "    hard_mask_attn=None,\n",
    "    reco_mask_attn=reco_dataset.attention_mask,\n",
    "    transformer_args={\n",
    "        \"nhead\": 8, # was 8\n",
    "        \"num_encoder_layers\": 6,\n",
    "        \"num_decoder_layers\": 8, # was 8\n",
    "        \"dim_feedforward\": 256, # was 256\n",
    "        \"activation\": \"relu\",\n",
    "    },\n",
    "    cfm_args={\n",
    "        \"dim_hidden\": 512, # was 512\n",
    "        \"num_layers\": 8, # was 8\n",
    "        \"activation\": nn.SiLU,\n",
    "    },\n",
    "    sigma=0.1,\n",
    "    # # ot_reg=0.1, # For OT CFMs\n",
    "    # ot_method='exact', # For OT & SchrodingBridge CFMs\n",
    "    # # normalize_cost=True, # For sinkhorn ot_method\n",
    ")\n",
    "\n",
    "\n",
    "# Quick test on one batch\n",
    "batch = next(iter(combined_loader_train))\n",
    "for i, (obj, mask) in enumerate(zip(batch[\"hard\"][\"data\"], batch[\"hard\"][\"mask\"])):\n",
    "    print(f\"hard_data[{i}] shape = {obj.shape}, mask shape = {mask.shape}\")\n",
    "for i, (obj, mask) in enumerate(zip(batch[\"reco\"][\"data\"], batch[\"reco\"][\"mask\"])):\n",
    "    print(f\"reco_data[{i}] shape = {obj.shape}, mask shape = {mask.shape}\")\n",
    "\n",
    "loss = model.shared_eval(batch, 1, 'train')\n",
    "print(\"Initial CFM loss:\", loss.item())\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(batch['hard']['data']), len(batch['hard']['data'])) # len = 2 is for 2 particles\n",
    "print(type(batch['reco']['data']), len(batch['reco']['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks to make plots within comet\n",
    "bias = BiasCallback(\n",
    "    dataset = combined_dataset_valid,               # dataset on which to evaluate bias\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing, # preprocessing pipeline to draw raw variables\n",
    "    N_sample = 100,                                 # number of samples to draw\n",
    "    steps = 20,                                     # Number of bridging steps\n",
    "    store_trajectories = False,                     # To save trajectories plots\n",
    "    frequency = 501,                                 # plotting frequency (epochs)\n",
    "    raw = True,\n",
    "    bins = 101,                                      # 1D/2D plot number of bins\n",
    "    points = 20,                                    # Number of points for the quantile\n",
    "    log_scale = True,                               # log\n",
    "    batch_size = 1000,                              # Batch size to evaluate the dataset (internally makes a loaded)\n",
    "    N_batch = 1,                                   # Stop after N batches (makes it faster)\n",
    "    suffix = 'ttH',                                 # name for plots\n",
    "    label_names = {                                 # makes nicer labels\n",
    "        'pt' : 'p_T',\n",
    "        'eta' : '\\eta',\n",
    "        'phi' : '\\phi',\n",
    "    },\n",
    ")\n",
    "\n",
    "sampling = SamplingCallback(\n",
    "    dataset = combined_dataset_valid,           # dataset to check sampling\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing, # preprocessing pipeline\n",
    "    idx_to_monitor = [1,2,3,4,5,6],               # idx of events in dataset to make plots with\n",
    "    N_sample = 1000,                         # number of samples to draw\n",
    "    steps = 20,                                     # Number of bridging steps\n",
    "    store_trajectories = False,                     # To save trajectories plots\n",
    "    frequency = 100,                             # plotting frequency (epochs)\n",
    "    bins = 51,                                  # 1D/2D plot number of bins\n",
    "    log_scale = True,                           # log\n",
    "    label_names = {                             # makes nicer labels\n",
    "        'pt' : 'p_T',\n",
    "        'eta' : '\\eta',\n",
    "        'phi' : '\\phi',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d42c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 501\n",
    "steps_per_epoch_train = math.ceil(len(combined_dataset_train)/combined_loader_train.batch_size)\n",
    "\n",
    "# Optimizer + scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4) # was lr=1e-5\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer=optimizer,\n",
    "#     mode='min',\n",
    "#     factor=0.1,\n",
    "#     patience=10,\n",
    "#     threshold=0.001,\n",
    "#     threshold_mode='rel',\n",
    "#     cooldown=0,\n",
    "#     min_lr=1e-7\n",
    "# )\n",
    "\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer=optimizer,\n",
    "    T_max=epochs,  # Num epochs before lr is reduced to min val\n",
    "    # eta_min=1e-6   # Min value lr can decay to\n",
    "    )\n",
    "\n",
    "model.set_scheduler_config({\n",
    "    'scheduler': scheduler,\n",
    "    'interval': 'epoch',\n",
    "    'frequency': 1,\n",
    "    'monitor': 'val_loss',\n",
    "    'strict': True,\n",
    "    'name': 'scheduler',\n",
    "})\n",
    "\n",
    "# Logger + Trainer\n",
    "logger = pl_loggers.CometLogger(\n",
    "    save_dir='../comet_logs',\n",
    "    project_name='mem-flow-ttH',\n",
    "    experiment_name='test',\n",
    "    offline=False,\n",
    "    # experiment_key=\"4f3d2b1e843d489ea4ebd17ce92d9035\", # Append to existing experiment on Comet\n",
    "    # resume = True\n",
    ")\n",
    "\n",
    "##### Callbacks #####\n",
    "callbacks = [\n",
    "    L.pytorch.callbacks.LearningRateMonitor(logging_interval = 'epoch'),\n",
    "    L.pytorch.callbacks.ModelSummary(max_depth=2),\n",
    "    sampling,\n",
    "    bias,\n",
    "    ModelCheckpoint(save_every_n_epochs=10, save_dir=\"trained_model_checkpoints/test\"),\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    min_epochs=5,\n",
    "    max_epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    devices='auto',\n",
    "    accelerator='auto',\n",
    "    logger=logger,\n",
    "    log_every_n_steps=steps_per_epoch_train // 100,\n",
    ")\n",
    "\n",
    "# 6) Fit\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=combined_loader_train,\n",
    "    val_dataloaders=combined_loader_valid,\n",
    "    # ckpt_path=\"TransferCFM_checkpoints/model_epoch_10.ckpt\" # Use to resume training from a checkpoint\n",
    ")\n",
    "\n",
    "logger.experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
