{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f754e5-5310-4717-8700-8dc85ecf889f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a7bc2a-42e5-48ff-8ddd-a01ed240d9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:15: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 12.0.1. Please consider upgrading.\n",
      "  warnings.warn(\n",
      "2025-03-01 15:30:33.747353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-01 15:30:33.747407: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-01 15:30:33.747416: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-01 15:30:33.752478: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU workers for dataloading: 16\n",
      "Running on GPU : True\n",
      "Accelerator : cuda\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import dask\n",
    "from tqdm import tqdm\n",
    "\n",
    "import vector\n",
    "import particle\n",
    "import hepunits\n",
    "\n",
    "import comet_ml\n",
    "import zuko\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import multiprocessing\n",
    "import uuid\n",
    "\n",
    "from memflow.dataset.data import ParquetData\n",
    "from memflow.dataset.dataset import CombinedDataset\n",
    "from memflow.ttH.ttH_dataclasses import ttHHardDataset, ttHRecoDataset\n",
    "\n",
    "from memflow.ttH.models.TransferCFM import StandardCFM as TransferCFM\n",
    "from memflow.ttH.models.Transfusion import StandardCFM as Transfusion\n",
    "from memflow.ttH.models.ParallelTransfusion import StandardCFM as ParallelTransfusion\n",
    "from memflow.ttH.models.TransferCFM_original import StandardCFM as OriginalCFM\n",
    "\n",
    "from memflow.ttH.distribution_plots import *\n",
    "from models.utils import load_samples, save_samples\n",
    "from models.callbacks import CFMSamplingCallback, SamplingCallback, BiasCallback\n",
    "\n",
    "from transfer_flow.transfer_flow_model import *\n",
    "from transfer_flow.custom_flows import *\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "num_workers = min(16, multiprocessing.cpu_count())  # Use up to 16 CPU cores\n",
    "print(f'Number of CPU workers for dataloading: {num_workers}')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Change \"<n>\" to the index of the GPU you want to use on node\n",
    "\n",
    "print (f\"Running on GPU : {torch.cuda.is_available()}\")\n",
    "accelerator = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (f\"Accelerator : {accelerator}\")\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "if accelerator =='cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print (torch.cuda.memory_summary(device=None, abbreviated=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c50d80-9342-47c6-9efd-87c39a91bc5b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a68956-c2a9-4fbd-b11a-9025f8339cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object\n",
      "Loaded branches:\n",
      "   ... file: 1903554\n",
      "   ... sample: 1903554\n",
      "   ... tree: 1903554\n",
      "Branch in files not loaded:\n",
      "   ... Generator_scalePDF\n",
      "   ... Generator_weight\n",
      "   ... Generator_x1\n",
      "   ... Generator_x2\n",
      "   ... Generator_xpdf1\n",
      "   ... Generator_xpdf2\n",
      "   ... W_minus_from_antitop_eta\n",
      "   ... W_minus_from_antitop_genPartIdxMother\n",
      "   ... W_minus_from_antitop_idx\n",
      "   ... W_minus_from_antitop_mass\n",
      "   ... W_minus_from_antitop_pdgId\n",
      "   ... W_minus_from_antitop_phi\n",
      "   ... W_minus_from_antitop_pt\n",
      "   ... W_minus_from_antitop_status\n",
      "   ... W_minus_from_antitop_statusFlags\n",
      "   ... W_plus_from_top_eta\n",
      "   ... W_plus_from_top_genPartIdxMother\n",
      "   ... W_plus_from_top_idx\n",
      "   ... W_plus_from_top_mass\n",
      "   ... W_plus_from_top_pdgId\n",
      "   ... W_plus_from_top_phi\n",
      "   ... W_plus_from_top_pt\n",
      "   ... W_plus_from_top_status\n",
      "   ... W_plus_from_top_statusFlags\n",
      "   ... Z_from_higgs_eta\n",
      "   ... Z_from_higgs_genPartIdxMother\n",
      "   ... Z_from_higgs_idx\n",
      "   ... Z_from_higgs_mass\n",
      "   ... Z_from_higgs_pdgId\n",
      "   ... Z_from_higgs_phi\n",
      "   ... Z_from_higgs_pt\n",
      "   ... Z_from_higgs_status\n",
      "   ... Z_from_higgs_statusFlags\n",
      "   ... antibottom_eta\n",
      "   ... antibottom_genPartIdxMother\n",
      "   ... antibottom_idx\n",
      "   ... antibottom_mass\n",
      "   ... antibottom_pdgId\n",
      "   ... antibottom_phi\n",
      "   ... antibottom_pt\n",
      "   ... antibottom_status\n",
      "   ... antibottom_statusFlags\n",
      "   ... antineutrino_from_W_minus_eta\n",
      "   ... antineutrino_from_W_minus_genPartIdxMother\n",
      "   ... antineutrino_from_W_minus_idx\n",
      "   ... antineutrino_from_W_minus_mass\n",
      "   ... antineutrino_from_W_minus_pdgId\n",
      "   ... antineutrino_from_W_minus_phi\n",
      "   ... antineutrino_from_W_minus_pt\n",
      "   ... antineutrino_from_W_minus_status\n",
      "   ... antineutrino_from_W_minus_statusFlags\n",
      "   ... antiquark_from_W_minus_eta\n",
      "   ... antiquark_from_W_minus_genPartIdxMother\n",
      "   ... antiquark_from_W_minus_idx\n",
      "   ... antiquark_from_W_minus_mass\n",
      "   ... antiquark_from_W_minus_pdgId\n",
      "   ... antiquark_from_W_minus_phi\n",
      "   ... antiquark_from_W_minus_pt\n",
      "   ... antiquark_from_W_minus_status\n",
      "   ... antiquark_from_W_minus_statusFlags\n",
      "   ... antiquark_from_W_plus_eta\n",
      "   ... antiquark_from_W_plus_genPartIdxMother\n",
      "   ... antiquark_from_W_plus_idx\n",
      "   ... antiquark_from_W_plus_mass\n",
      "   ... antiquark_from_W_plus_pdgId\n",
      "   ... antiquark_from_W_plus_phi\n",
      "   ... antiquark_from_W_plus_pt\n",
      "   ... antiquark_from_W_plus_status\n",
      "   ... antiquark_from_W_plus_statusFlags\n",
      "   ... antitop_eta\n",
      "   ... antitop_genPartIdxMother\n",
      "   ... antitop_idx\n",
      "   ... antitop_mass\n",
      "   ... antitop_pdgId\n",
      "   ... antitop_phi\n",
      "   ... antitop_pt\n",
      "   ... antitop_status\n",
      "   ... antitop_statusFlags\n",
      "   ... bottom_eta\n",
      "   ... bottom_genPartIdxMother\n",
      "   ... bottom_idx\n",
      "   ... bottom_mass\n",
      "   ... bottom_pdgId\n",
      "   ... bottom_phi\n",
      "   ... bottom_pt\n",
      "   ... bottom_status\n",
      "   ... bottom_statusFlags\n",
      "   ... event\n",
      "   ... higgs_eta\n",
      "   ... higgs_genPartIdxMother\n",
      "   ... higgs_idx\n",
      "   ... higgs_mass\n",
      "   ... higgs_pdgId\n",
      "   ... higgs_phi\n",
      "   ... higgs_pt\n",
      "   ... higgs_status\n",
      "   ... higgs_statusFlags\n",
      "   ... lep_minus_from_W_minus_eta\n",
      "   ... lep_minus_from_W_minus_genPartIdxMother\n",
      "   ... lep_minus_from_W_minus_idx\n",
      "   ... lep_minus_from_W_minus_mass\n",
      "   ... lep_minus_from_W_minus_pdgId\n",
      "   ... lep_minus_from_W_minus_phi\n",
      "   ... lep_minus_from_W_minus_pt\n",
      "   ... lep_minus_from_W_minus_status\n",
      "   ... lep_minus_from_W_minus_statusFlags\n",
      "   ... lep_plus_from_W_plus_eta\n",
      "   ... lep_plus_from_W_plus_genPartIdxMother\n",
      "   ... lep_plus_from_W_plus_idx\n",
      "   ... lep_plus_from_W_plus_mass\n",
      "   ... lep_plus_from_W_plus_pdgId\n",
      "   ... lep_plus_from_W_plus_phi\n",
      "   ... lep_plus_from_W_plus_pt\n",
      "   ... lep_plus_from_W_plus_status\n",
      "   ... lep_plus_from_W_plus_statusFlags\n",
      "   ... neutrino_from_W_plus_eta\n",
      "   ... neutrino_from_W_plus_genPartIdxMother\n",
      "   ... neutrino_from_W_plus_idx\n",
      "   ... neutrino_from_W_plus_mass\n",
      "   ... neutrino_from_W_plus_pdgId\n",
      "   ... neutrino_from_W_plus_phi\n",
      "   ... neutrino_from_W_plus_pt\n",
      "   ... neutrino_from_W_plus_status\n",
      "   ... neutrino_from_W_plus_statusFlags\n",
      "   ... neutrinos_from_Z_eta\n",
      "   ... neutrinos_from_Z_genPartIdxMother\n",
      "   ... neutrinos_from_Z_idx\n",
      "   ... neutrinos_from_Z_mass\n",
      "   ... neutrinos_from_Z_pdgId\n",
      "   ... neutrinos_from_Z_phi\n",
      "   ... neutrinos_from_Z_pt\n",
      "   ... neutrinos_from_Z_status\n",
      "   ... neutrinos_from_Z_statusFlags\n",
      "   ... quark_from_W_minus_eta\n",
      "   ... quark_from_W_minus_genPartIdxMother\n",
      "   ... quark_from_W_minus_idx\n",
      "   ... quark_from_W_minus_mass\n",
      "   ... quark_from_W_minus_pdgId\n",
      "   ... quark_from_W_minus_phi\n",
      "   ... quark_from_W_minus_pt\n",
      "   ... quark_from_W_minus_status\n",
      "   ... quark_from_W_minus_statusFlags\n",
      "   ... quark_from_W_plus_eta\n",
      "   ... quark_from_W_plus_genPartIdxMother\n",
      "   ... quark_from_W_plus_idx\n",
      "   ... quark_from_W_plus_mass\n",
      "   ... quark_from_W_plus_pdgId\n",
      "   ... quark_from_W_plus_phi\n",
      "   ... quark_from_W_plus_pt\n",
      "   ... quark_from_W_plus_status\n",
      "   ... quark_from_W_plus_statusFlags\n",
      "   ... top_eta\n",
      "   ... top_genPartIdxMother\n",
      "   ... top_idx\n",
      "   ... top_mass\n",
      "   ... top_pdgId\n",
      "   ... top_phi\n",
      "   ... top_pt\n",
      "   ... top_status\n",
      "   ... top_statusFlags\n"
     ]
    }
   ],
   "source": [
    "data_hard = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "        #'all_jets_fullRun2_ttHbb_forTraining_allyears_spanetprov_part1_validation.parquet',\n",
    "        #'all_jets_fullRun2_ttHTobb_forTraining_2016_PreVFP_v3.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    # N = int(1e5),\n",
    ")\n",
    "\n",
    "print (data_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35dfc61-246f-444b-930a-e5cdd77fd3c9",
   "metadata": {},
   "source": [
    "# Hard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd35299-f40b-4864-9132-67a95e03acd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading objects from /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard\n",
      "Saving preprocessing to /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard\n",
      "Will overwrite what is in output directory /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard/preprocessing\n",
      "Preprocessing saved in /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_hard/preprocessing\n",
      "Parton dataset with 756642 events\n",
      " Initial states pdgids : [21, 21]\n",
      " Final states pdgids   : [25, 6, -6]\n",
      " Final states masses   : [125.2, 172.57, 172.57]\n",
      "Containing the following tensors\n",
      "bottoms    : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "Zs         : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "tops       : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "neutrinos  : data ([756642, 4, 5]), mask ([756642, 4])\n",
      "             Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%]\n",
      "             Mask attn     : [True, True, True, True]\n",
      "             Weights       : 756642.00, 756642.00, 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "boost      : data ([756642, 1, 4]), mask ([756642, 1])\n",
      "             Mask exist    : [100.00%]\n",
      "             Mask attn     : [True]\n",
      "             Weights       : 756642.00\n",
      "             Features      : ['x', 'y', 'z', 't']\n",
      "             Selected for batches : False\n",
      "Ws         : data ([756642, 2, 5]), mask ([756642, 2])\n",
      "             Mask exist    : [100.00%, 100.00%]\n",
      "             Mask attn     : [True, True]\n",
      "             Weights       : 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "quarks     : data ([756642, 4, 5]), mask ([756642, 4])\n",
      "             Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%]\n",
      "             Mask attn     : [True, True, True, True]\n",
      "             Weights       : 756642.00, 756642.00, 756642.00, 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : True\n",
      "higgs      : data ([756642, 1, 5]), mask ([756642, 1])\n",
      "             Mask exist    : [100.00%]\n",
      "             Mask attn     : [True]\n",
      "             Weights       : 756642.00\n",
      "             Features      : ['pt', 'eta', 'phi', 'mass', 'pdgId']\n",
      "             Selected for batches : False\n",
      "Preprocessing steps\n",
      "Step applied to ['higgs', 'tops', 'bottoms', 'Ws', 'quarks', 'Zs', 'neutrinos']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "  - higgs     : ('pt', 'mass')\n",
      "  - tops      : ('pt', 'mass')\n",
      "  - bottoms   : ('pt', 'mass')\n",
      "  - Ws        : ('pt', 'mass')\n",
      "  - quarks    : ('pt', 'mass')\n",
      "  - Zs        : ('pt', 'mass')\n",
      "  - neutrinos : ('pt', 'mass')\n",
      "Step applied to ['higgs', 'tops', 'bottoms', 'Ws', 'quarks', 'Zs', 'neutrinos']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\teta   : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "  - higgs     : ('pt', 'eta', 'mass')\n",
      "  - tops      : ('pt', 'eta', 'mass')\n",
      "  - bottoms   : ('pt', 'eta', 'mass')\n",
      "  - Ws        : ('pt', 'eta', 'mass')\n",
      "  - quarks    : ('pt', 'eta', 'mass')\n",
      "  - Zs        : ('pt', 'eta', 'mass')\n",
      "  - neutrinos : ('pt', 'eta', 'mass')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hard_dataset = ttHHardDataset(\n",
    "    data = data_hard,\n",
    "    selection = [\n",
    "        # 'higgs',\n",
    "        # 'tops',\n",
    "        'bottoms',\n",
    "        # 'Ws',\n",
    "        # 'Zs',\n",
    "        'quarks',\n",
    "        'neutrinos',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "\n",
    "print(hard_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c82aff-8a32-4f83-b199-0c7725639eda",
   "metadata": {},
   "source": [
    "# Reco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc523344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object\n",
      "Loaded branches:\n",
      "   ... file: 231528\n",
      "   ... sample: 231528\n",
      "   ... tree: 231528\n",
      "Branch in files not loaded:\n",
      "   ... Generator_scalePDF\n",
      "   ... Generator_weight\n",
      "   ... Generator_x1\n",
      "   ... Generator_x2\n",
      "   ... Generator_xpdf1\n",
      "   ... Generator_xpdf2\n",
      "   ... InputMet_phi\n",
      "   ... InputMet_pt\n",
      "   ... cleanedJet_btagDeepFlavB\n",
      "   ... cleanedJet_eta\n",
      "   ... cleanedJet_mass\n",
      "   ... cleanedJet_phi\n",
      "   ... cleanedJet_pt\n",
      "   ... event\n",
      "   ... ncleanedBJet\n",
      "   ... ncleanedJet\n",
      "   ... region\n",
      "   ... weight_nominal\n",
      "   ... xs_weight\n"
     ]
    }
   ],
   "source": [
    "data_reco = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True,\n",
    "    #N = data_hard.N,\n",
    ")\n",
    "\n",
    "print(data_reco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ece5f1",
   "metadata": {},
   "source": [
    "Have a look at athe minimum values for Jet and MET pT in the raw dataset. This can give an indication as to what the cutoff in the SR is and hence what to set the `'pt':lowercutshift()` to in the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e781dae8-05ad-462e-b91a-bc544337a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading objects from /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco\n",
      "Saving preprocessing to /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco\n",
      "Will overwrite what is in output directory /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco/preprocessing\n",
      "Preprocessing saved in /cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/ttH_reco/preprocessing\n",
      "Reco dataset with 114647 events\n",
      "Containing the following tensors\n",
      "jets  : data ([114647, 6, 5]), mask ([114647, 6])\n",
      "        Mask exist    : [100.00%, 100.00%, 100.00%, 100.00%, 100.00%, 62.85%]\n",
      "        Mask attn     : [True, True, True, True, True, True]\n",
      "        Weights       : 114647.00, 114647.00, 114647.00, 114647.00, 114647.00, 114647.00\n",
      "        Features      : ['pt', 'eta', 'phi', 'mass', 'btag']\n",
      "        Selected for batches : True\n",
      "met   : data ([114647, 1, 4]), mask ([114647, 1])\n",
      "        Mask exist    : [100.00%]\n",
      "        Mask attn     : [True]\n",
      "        Weights       : 114647.00\n",
      "        Features      : ['pt', 'eta', 'phi', 'mass']\n",
      "        Selected for batches : True\n",
      "Preprocessing steps\n",
      "Step applied to ['jets']\n",
      "\tpt  : <class 'memflow.dataset.preprocessing.lowercutshift'>\n",
      "  - jets : ('pt',)\n",
      "Step applied to ['met']\n",
      "\tpt  : <class 'memflow.dataset.preprocessing.lowercutshift'>\n",
      "  - met : ('pt',)\n",
      "Step applied to ['jets', 'met']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.logmodulus'>\n",
      "  - jets : ('pt', 'mass')\n",
      "  - met  : ('pt', 'mass')\n",
      "Step applied to ['jets', 'met']\n",
      "\tpt    : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\teta   : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "\tmass  : <class 'memflow.dataset.preprocessing.SklearnScaler'>\n",
      "  - jets : ('pt', 'eta', 'mass')\n",
      "  - met  : ('pt', 'eta', 'mass')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reco_dataset = ttHRecoDataset(\n",
    "    data = data_reco,\n",
    "    selection = [\n",
    "        'jets',\n",
    "        'met',\n",
    "    ],\n",
    "    build = False,\n",
    "    fit = True,\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_preprocessing = True,\n",
    "    apply_boost = False,\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "print(reco_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc702e-66b1-450a-86d0-a28192f99d98",
   "metadata": {},
   "source": [
    "# Combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12a63d7-eafc-43ef-8147-d0887d4bceec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection branches : `event` for hard dataset and `event` for reco dataset\n",
      "Looking into file metadata\n",
      "Will pair these files together :\n",
      "   - /cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet <-> /cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet\n",
      "For entry 0 : from 756642 events, 91819 selected\n",
      "For entry 1 : from 114647 events, 91819 selected\n",
      "Dataset : validation 18364\n"
     ]
    }
   ],
   "source": [
    "combined_dataset = CombinedDataset(\n",
    "    hard_dataset=hard_dataset,\n",
    "    reco_dataset=reco_dataset,\n",
    ")\n",
    "\n",
    "train_frac = 0.8\n",
    "indices = torch.arange(len(combined_dataset))\n",
    "sep = int(train_frac*len(combined_dataset))\n",
    "valid_indices = indices[sep:]\n",
    "combined_dataset_valid = torch.utils.data.Subset(combined_dataset,valid_indices)\n",
    "print (f'Dataset : validation {len(combined_dataset_valid)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d48b78ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hard attention mask provided; will use existence mask only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardCFM(\n",
       "  (hard_embeddings): ModuleList(\n",
       "    (0-2): 3 x Sequential(\n",
       "      (0): CircularEmbedding(\n",
       "        (linear): Linear(in_features=6, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (reco_embeddings): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): CircularEmbedding(\n",
       "        (linear): Linear(in_features=6, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CircularEmbedding(\n",
       "        (linear): Linear(in_features=5, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (velocity_net): Sequential(\n",
       "    (0): Linear(in_features=70, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): SiLU()\n",
       "    (9): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (10): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): SiLU()\n",
       "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): SiLU()\n",
       "    (15): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (16): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): SiLU()\n",
       "    (18): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (19): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): SiLU()\n",
       "    (21): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (22): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): SiLU()\n",
       "    (24): Linear(in_features=512, out_features=5, bias=True)\n",
       "    (25): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransferCFM_model = TransferCFM.load_from_checkpoint(checkpoint_path=\"trained_model_checkpoints/TransferCFM_checkpoints/model_epoch_500.ckpt\")\n",
    "TransferCFM_model.to(accelerator)\n",
    "TransferCFM_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb9be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hard attention mask provided; will use existence mask only.\n",
      "Transformer args: will override `d_model` to 64\n",
      "('pt', 'eta', 'phi', 'mass')\n",
      "('pt', 'phi')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardCFM(\n",
       "  (hard_embeddings): ModuleList(\n",
       "    (0-2): 3 x Sequential(\n",
       "      (0): CircularEmbedding(\n",
       "        (linear): Linear(in_features=6, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (reco_embeddings): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): CircularEmbedding(\n",
       "        (linear): Linear(in_features=6, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CircularEmbedding(\n",
       "        (linear): Linear(in_features=5, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (state_embeddings): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (time_embedding): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (velocity_net): Sequential(\n",
       "    (0): Linear(in_features=70, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): SiLU()\n",
       "    (9): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (10): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): SiLU()\n",
       "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): SiLU()\n",
       "    (15): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (16): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): SiLU()\n",
       "    (18): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (19): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): SiLU()\n",
       "    (21): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (22): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): SiLU()\n",
       "    (24): Linear(in_features=512, out_features=5, bias=True)\n",
       "    (25): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ParallelTransfusion_model = ParallelTransfusion.load_from_checkpoint(checkpoint_path=\"trained_model_checkpoints/parallel_transfusion_checkpoints/model_epoch_500.ckpt\")\n",
    "ParallelTransfusion_model.to(accelerator)\n",
    "ParallelTransfusion_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f045f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'encoder_embeddings' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder_embeddings'])`.\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'decoder_embeddings' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder_embeddings'])`.\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'transformer' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transformer'])`.\n",
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'flow' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['flow'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransferFlow(\n",
       "  (encoder_embeddings): MultiEmbeddings(\n",
       "    (embeddings): ModuleList(\n",
       "      (0-2): 3 x MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_embeddings): MultiEmbeddings(\n",
       "    (embeddings): ModuleList(\n",
       "      (0): MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MLP(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (transformer): Transformer(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (activation): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-7): 8 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.0, inplace=False)\n",
       "            (activation): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (flow): KinematicFlow(\n",
       "    (flows): ModuleDict(\n",
       "      (pt): NSF(\n",
       "        (transform): LazyComposedTransform(\n",
       "          (0-4): 5 x ElementWiseTransform(\n",
       "            (base): MonotonicRQSTransform(bins=16)\n",
       "            (hyper): MLP(\n",
       "              (0): Linear(in_features=66, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (5): ReLU()\n",
       "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (base): UnconditionalDistribution(DiagNormal(loc: tensor([0.], device='cuda:0'), scale: tensor([1.], device='cuda:0')))\n",
       "      )\n",
       "      (eta): UniformNSF(\n",
       "        (transform): LazyComposedTransform(\n",
       "          (0-4): 5 x ElementWiseTransform(\n",
       "            (base): MonotonicRQSTransform(bins=16)\n",
       "            (hyper): MLP(\n",
       "              (0): Linear(in_features=66, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (5): ReLU()\n",
       "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (base): UnconditionalDistribution(BoxUniform(low: tensor([-1.], device='cuda:0'), high: tensor([1.], device='cuda:0')))\n",
       "      )\n",
       "      (phi): UniformNCSF(\n",
       "        (transform): LazyComposedTransform(\n",
       "          (0-4): 5 x ElementWiseTransform(\n",
       "            (base): ComposedTransform(\n",
       "              (0): CircularShiftTransform(bound=3.141592653589793)\n",
       "              (1): MonotonicRQSTransform(bins=16)\n",
       "            )\n",
       "            (hyper): MLP(\n",
       "              (0): Linear(in_features=66, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (5): ReLU()\n",
       "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (base): UnconditionalDistribution(BoxUniform(low: tensor([-3.1416], device='cuda:0'), high: tensor([3.1416], device='cuda:0')))\n",
       "      )\n",
       "      (mass): NSF(\n",
       "        (transform): LazyComposedTransform(\n",
       "          (0-4): 5 x ElementWiseTransform(\n",
       "            (base): MonotonicRQSTransform(bins=16)\n",
       "            (hyper): MLP(\n",
       "              (0): Linear(in_features=67, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (5): ReLU()\n",
       "              (6): Linear(in_features=256, out_features=47, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (base): UnconditionalDistribution(DiagNormal(loc: tensor([0.], device='cuda:0'), scale: tensor([1.], device='cuda:0')))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfermer_model = TransferFlow.load_from_checkpoint(\n",
    "    checkpoint_path=\"trained_model_checkpoints/TransferFlow_checkpoints/model_epoch_500.ckpt\",\n",
    "    encoder_embeddings=MultiEmbeddings(\n",
    "        features_per_type=combined_dataset.hard_dataset.input_features,\n",
    "        embed_dims=[32, 64],\n",
    "        hidden_activation=nn.GELU,\n",
    "    ),\n",
    "    decoder_embeddings=MultiEmbeddings(\n",
    "        features_per_type=combined_dataset.reco_dataset.input_features,\n",
    "        embed_dims=[32, 64],\n",
    "        hidden_activation=nn.GELU,\n",
    "    ),\n",
    "    transformer=Transformer(\n",
    "        d_model=64,\n",
    "        encoder_layers=6,\n",
    "        decoder_layers=8,\n",
    "        nhead=8,\n",
    "        dim_feedforward=256,\n",
    "        activation=nn.GELU,\n",
    "        encoder_mask_attn=None,\n",
    "        decoder_mask_attn=combined_dataset.reco_dataset.attention_mask,\n",
    "        use_null_token=True,\n",
    "        dropout=0.0,\n",
    "    ),\n",
    "    flow=KinematicFlow(\n",
    "        d_model=64,\n",
    "        flow_mode='global',\n",
    "        flow_features=[\n",
    "            ['pt', 'eta', 'phi', 'mass'],  # jets\n",
    "            ['pt', 'phi'],  # met\n",
    "        ],\n",
    "        flow_classes={\n",
    "            'pt': zuko.flows.NSF,\n",
    "            'eta': UniformNSF,\n",
    "            'phi': UniformNCSF,\n",
    "            'mass': zuko.flows.NSF,\n",
    "        },\n",
    "        flow_common_args={\n",
    "            'bins': 16,\n",
    "            'transforms': 5,\n",
    "            'randperm': True,\n",
    "            'passes': None,\n",
    "            'hidden_features': [256] * 3,\n",
    "        },\n",
    "        flow_specific_args={\n",
    "            'eta': {'bound': 1.0},\n",
    "            'phi': {'bound': math.pi},\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "transfermer_model.to(accelerator)\n",
    "transfermer_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bccf3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks to make plots within comet\n",
    "bias = BiasCallback(\n",
    "    dataset = combined_dataset_valid,               # dataset on which to evaluate bias\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing, # preprocessing pipeline to draw raw variables\n",
    "    N_sample = 100,                                 # number of samples to draw\n",
    "    steps = 20,                                     # Number of bridging steps\n",
    "    store_trajectories = False,                     # To save trajectories plots\n",
    "    frequency = 50,                                 # plotting frequency (epochs)\n",
    "    raw = True,\n",
    "    bins = 101,                                      # 1D/2D plot number of bins\n",
    "    points = 20,                                    # Number of points for the quantile\n",
    "    log_scale = True,                               # log\n",
    "    batch_size = 1000,                              # Batch size to evaluate the dataset (internally makes a loaded)\n",
    "    #N_batch = 20,                                   # Stop after N batches (makes it faster)\n",
    "    suffix = 'ttH',                                 # name for plots\n",
    "    label_names = {                             # makes nicer labels\n",
    "        'pt' : 'p_T',\n",
    "        'eta' : '\\eta',\n",
    "        'phi' : '\\phi',\n",
    "    },\n",
    ")\n",
    "\n",
    "sampling = SamplingCallback(\n",
    "    dataset = combined_dataset_valid,           # dataset to check sampling\n",
    "    preprocessing = combined_dataset.reco_dataset.preprocessing, # preprocessing pipeline\n",
    "    idx_to_monitor = [1,2,3,4,5,6,7,8,9,10],               # idx of events in dataset to make plots with\n",
    "    N_sample = 10000,                          # number of samples to draw\n",
    "    steps = 20,                                     # Number of bridging steps\n",
    "    store_trajectories = False,                     # To save trajectories plots\n",
    "    frequency = 50,                             # plotting frequency (epochs)\n",
    "    bins = 31,                                  # 1D/2D plot number of bins\n",
    "    log_scale = True,                           # log\n",
    "    label_names = {                             # makes nicer labels\n",
    "        'pt' : r'$p_T$ [GeV]',\n",
    "        'eta' : r'$\\eta$',\n",
    "        'phi' : r'$\\phi$ [rad]',\n",
    "    },\n",
    "    pt_range = 350,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002915a7",
   "metadata": {},
   "source": [
    "# Transfermer per event sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "654ab4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard data batch size: 10\n",
      "Reco data batch size: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling batches:   0%|          | 0/1000 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/transformer.py:380: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
      "Sampling batches: 100%|| 1000/1000 [03:41<00:00,  4.51batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements per sample: 2\n",
      "Sample 0: torch.Size([10000, 10, 6, 4])\n",
      "Sample 1: torch.Size([10000, 10, 1, 2])\n",
      "Samples saved to saved_samples/Transfermer_samples2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transfermer_samples_file = \"Transfermer_samples2.pt\"\n",
    "\n",
    "if os.path.exists(os.path.join(\"saved_samples\", transfermer_samples_file)):\n",
    "    transfermer_samples = load_samples(transfermer_samples_file)\n",
    "else:\n",
    "    device = transfermer_model.device\n",
    "    sampling.set_idx(sampling.idx_to_monitor)\n",
    "\n",
    "    # Move input data to the correct device\n",
    "    hard_data = [d.to(device) for d in sampling.batch['hard']['data']]\n",
    "    hard_mask = [m.to(device) for m in sampling.batch['hard']['mask']]\n",
    "    reco_data = [d.to(device) for d in sampling.batch['reco']['data']]\n",
    "    reco_mask = [m.to(device) for m in sampling.batch['reco']['mask']]\n",
    "\n",
    "    print(f\"Hard data batch size: {hard_data[0].shape[0]}\")\n",
    "    print(f\"Reco data batch size: {reco_data[0].shape[0]}\")\n",
    "\n",
    "    # Number of samples per event\n",
    "    total_samples = sampling.N_sample  \n",
    "    batch_size = 10  # Number of samples per batch to avoid memory issues\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size  # Ensure full coverage\n",
    "\n",
    "    # Storage for results\n",
    "    accumulated_samples = [None, None]  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Sampling batches\", unit=\"batch\"):\n",
    "            # Determine the number of samples to generate in this batch\n",
    "            current_N = min(batch_size, total_samples - batch_idx * batch_size)\n",
    "\n",
    "            # Generate samples\n",
    "            batch_samples = transfermer_model.sample(\n",
    "                hard_data,\n",
    "                hard_mask,\n",
    "                reco_data,\n",
    "                reco_mask,\n",
    "                N=current_N\n",
    "            )\n",
    "\n",
    "            # Feature selection\n",
    "            jets_indices = [0, 1, 2, 3]  # ['pt', 'eta', 'phi', 'mass']\n",
    "            met_indices = [0, 2]         # ['pt', 'phi']\n",
    "\n",
    "            batch_samples[0] = batch_samples[0][..., jets_indices]  # Filter jet features\n",
    "            batch_samples[1] = batch_samples[1][..., met_indices]   # Filter MET features\n",
    "\n",
    "            # Accumulate results\n",
    "            if accumulated_samples[0] is None:\n",
    "                accumulated_samples[0] = batch_samples[0].cpu()\n",
    "                accumulated_samples[1] = batch_samples[1].cpu()\n",
    "            else:\n",
    "                accumulated_samples[0] = torch.cat((accumulated_samples[0], batch_samples[0].cpu()), dim=0)\n",
    "                accumulated_samples[1] = torch.cat((accumulated_samples[1], batch_samples[1].cpu()), dim=0)\n",
    "\n",
    "    # Convert final results to the expected format\n",
    "    transfermer_samples = accumulated_samples\n",
    "\n",
    "    # Debugging prints\n",
    "    num_items = len(transfermer_samples)\n",
    "    print(f\"Number of elements per sample: {num_items}\")\n",
    "    for i, sample in enumerate(transfermer_samples):\n",
    "        print(f\"Sample {i}: {sample.shape}\")\n",
    "\n",
    "    # Save the concatenated samples\n",
    "    save_samples(transfermer_samples, transfermer_samples_file)\n",
    "\n",
    "# 3mins 27s, 10 events, 10000 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b604adc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: sampling_plots/transfermer/event_0_obj_jets_0.png\n",
      "Saved: sampling_plots/transfermer/event_0_obj_jets_1.png\n",
      "Saved: sampling_plots/transfermer/event_0_obj_jets_2.png\n",
      "Saved: sampling_plots/transfermer/event_0_obj_jets_3.png\n",
      "Saved: sampling_plots/transfermer/event_0_obj_jets_4.png\n",
      "Saved: sampling_plots/transfermer/event_0_obj_jets_5.png\n",
      "Saved: sampling_plots/transfermer/event_0_obj_met_0.png\n",
      "Saved: sampling_plots/transfermer/event_1_obj_jets_0.png\n",
      "Saved: sampling_plots/transfermer/event_1_obj_jets_1.png\n",
      "Saved: sampling_plots/transfermer/event_1_obj_jets_2.png\n",
      "Saved: sampling_plots/transfermer/event_1_obj_jets_3.png\n",
      "Saved: sampling_plots/transfermer/event_1_obj_jets_4.png\n",
      "Saved: sampling_plots/transfermer/event_1_obj_jets_5.png\n",
      "Saved: sampling_plots/transfermer/event_1_obj_met_0.png\n",
      "Saved: sampling_plots/transfermer/event_2_obj_jets_0.png\n",
      "Saved: sampling_plots/transfermer/event_2_obj_jets_1.png\n",
      "Saved: sampling_plots/transfermer/event_2_obj_jets_2.png\n",
      "Saved: sampling_plots/transfermer/event_2_obj_jets_3.png\n",
      "Saved: sampling_plots/transfermer/event_2_obj_jets_4.png\n",
      "Saved: sampling_plots/transfermer/event_2_obj_met_0.png\n",
      "Saved: sampling_plots/transfermer/event_3_obj_jets_0.png\n",
      "Saved: sampling_plots/transfermer/event_3_obj_jets_1.png\n",
      "Saved: sampling_plots/transfermer/event_3_obj_jets_2.png\n",
      "Saved: sampling_plots/transfermer/event_3_obj_jets_3.png\n",
      "Saved: sampling_plots/transfermer/event_3_obj_jets_4.png\n",
      "Saved: sampling_plots/transfermer/event_3_obj_jets_5.png\n",
      "Saved: sampling_plots/transfermer/event_3_obj_met_0.png\n",
      "Saved: sampling_plots/transfermer/event_4_obj_jets_0.png\n",
      "Saved: sampling_plots/transfermer/event_4_obj_jets_1.png\n",
      "Saved: sampling_plots/transfermer/event_4_obj_jets_2.png\n",
      "Saved: sampling_plots/transfermer/event_4_obj_jets_3.png\n",
      "Saved: sampling_plots/transfermer/event_4_obj_jets_4.png\n",
      "Saved: sampling_plots/transfermer/event_4_obj_jets_5.png\n",
      "Saved: sampling_plots/transfermer/event_4_obj_met_0.png\n",
      "Saved: sampling_plots/transfermer/event_5_obj_jets_0.png\n",
      "Saved: sampling_plots/transfermer/event_5_obj_jets_1.png\n",
      "Saved: sampling_plots/transfermer/event_5_obj_jets_2.png\n",
      "Saved: sampling_plots/transfermer/event_5_obj_jets_3.png\n",
      "Saved: sampling_plots/transfermer/event_5_obj_jets_4.png\n",
      "Saved: sampling_plots/transfermer/event_5_obj_met_0.png\n",
      "Saved: sampling_plots/transfermer/event_6_obj_jets_0.png\n",
      "Saved: sampling_plots/transfermer/event_6_obj_jets_1.png\n",
      "Saved: sampling_plots/transfermer/event_6_obj_jets_2.png\n",
      "Saved: sampling_plots/transfermer/event_6_obj_jets_3.png\n",
      "Saved: sampling_plots/transfermer/event_6_obj_jets_4.png\n",
      "Saved: sampling_plots/transfermer/event_6_obj_jets_5.png\n",
      "Saved: sampling_plots/transfermer/event_6_obj_met_0.png\n",
      "Saved: sampling_plots/transfermer/event_7_obj_jets_0.png\n",
      "Saved: sampling_plots/transfermer/event_7_obj_jets_1.png\n",
      "Saved: sampling_plots/transfermer/event_7_obj_jets_2.png\n",
      "Saved: sampling_plots/transfermer/event_7_obj_jets_3.png\n",
      "Saved: sampling_plots/transfermer/event_7_obj_jets_4.png\n",
      "Saved: sampling_plots/transfermer/event_7_obj_jets_5.png\n",
      "Saved: sampling_plots/transfermer/event_7_obj_met_0.png\n",
      "Saved: sampling_plots/transfermer/event_8_obj_jets_0.png\n",
      "Saved: sampling_plots/transfermer/event_8_obj_jets_1.png\n",
      "Saved: sampling_plots/transfermer/event_8_obj_jets_2.png\n",
      "Saved: sampling_plots/transfermer/event_8_obj_jets_3.png\n",
      "Saved: sampling_plots/transfermer/event_8_obj_jets_4.png\n",
      "Saved: sampling_plots/transfermer/event_8_obj_jets_5.png\n",
      "Saved: sampling_plots/transfermer/event_8_obj_met_0.png\n",
      "Saved: sampling_plots/transfermer/event_9_obj_jets_0.png\n",
      "Saved: sampling_plots/transfermer/event_9_obj_jets_1.png\n",
      "Saved: sampling_plots/transfermer/event_9_obj_jets_2.png\n",
      "Saved: sampling_plots/transfermer/event_9_obj_jets_3.png\n",
      "Saved: sampling_plots/transfermer/event_9_obj_jets_4.png\n",
      "Saved: sampling_plots/transfermer/event_9_obj_met_0.png\n"
     ]
    }
   ],
   "source": [
    "figures = sampling.make_sampling_plots(ParallelTransfusion_model,external_samples=transfermer_samples, cmap='RdPu', save_dir='sampling_plots/transfermer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20a255",
   "metadata": {},
   "source": [
    "# Parallel Transfusion per event sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26816b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard data batch size: 10\n",
      "Reco data batch size: 10\n",
      "Number of elements per sample: 2\n",
      "Sample 0: torch.Size([10000, 10, 6, 4])\n",
      "Sample 1: torch.Size([10000, 10, 1, 2])\n",
      "Samples saved to saved_samples/PT_samples2.pt\n"
     ]
    }
   ],
   "source": [
    "PT_samples_file = \"PT_samples2.pt\"\n",
    "if os.path.exists(os.path.join(\"saved_samples\", PT_samples_file)):\n",
    "    PT_samples = load_samples(PT_samples_file)\n",
    "else:\n",
    "    device = ParallelTransfusion_model.device\n",
    "    sampling.set_idx(sampling.idx_to_monitor)\n",
    "    hard_data = [d.to(device) for d in sampling.batch['hard']['data']]\n",
    "    hard_mask = [m.to(device) for m in sampling.batch['hard']['mask']]\n",
    "    reco_data = [d.to(device) for d in sampling.batch['reco']['data']]\n",
    "    reco_mask = [m.to(device) for m in sampling.batch['reco']['mask']]\n",
    "\n",
    "    print(f\"Hard data batch size: {hard_data[0].shape[0]}\")\n",
    "    print(f\"Reco data batch size: {reco_data[0].shape[0]}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = ParallelTransfusion_model.to(ParallelTransfusion_model.device)\n",
    "        PT_samples = model.sample(\n",
    "                            hard_data, hard_mask,\n",
    "                            reco_data, reco_mask,\n",
    "                            sampling.N_sample,\n",
    "                            sampling.steps,\n",
    "                            sampling.store_trajectories\n",
    "                        )\n",
    "    # Debugging prints\n",
    "    num_items = len(PT_samples)\n",
    "    print(f\"Number of elements per sample: {num_items}\")\n",
    "    for i, sample in enumerate(PT_samples):\n",
    "        print(f\"Sample {i}: {sample.shape}\")\n",
    "    save_samples(PT_samples, PT_samples_file)\n",
    "\n",
    "# 42mins 14s for 10 events, 10000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e2b4682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: sampling_plots/parallel_transfusion/event_0_obj_jets_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_0_obj_jets_1.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_0_obj_jets_2.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_0_obj_jets_3.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_0_obj_jets_4.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_0_obj_jets_5.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_0_obj_met_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_1_obj_jets_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_1_obj_jets_1.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_1_obj_jets_2.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_1_obj_jets_3.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_1_obj_jets_4.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_1_obj_jets_5.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_1_obj_met_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_2_obj_jets_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_2_obj_jets_1.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_2_obj_jets_2.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_2_obj_jets_3.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_2_obj_jets_4.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_2_obj_met_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_3_obj_jets_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_3_obj_jets_1.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_3_obj_jets_2.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_3_obj_jets_3.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_3_obj_jets_4.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_3_obj_jets_5.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_3_obj_met_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_4_obj_jets_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_4_obj_jets_1.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_4_obj_jets_2.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_4_obj_jets_3.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_4_obj_jets_4.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_4_obj_jets_5.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_4_obj_met_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_5_obj_jets_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_5_obj_jets_1.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_5_obj_jets_2.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_5_obj_jets_3.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_5_obj_jets_4.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_5_obj_met_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_6_obj_jets_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_6_obj_jets_1.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_6_obj_jets_2.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_6_obj_jets_3.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_6_obj_jets_4.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_6_obj_jets_5.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_6_obj_met_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_7_obj_jets_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_7_obj_jets_1.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_7_obj_jets_2.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_7_obj_jets_3.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_7_obj_jets_4.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_7_obj_jets_5.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_7_obj_met_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_8_obj_jets_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_8_obj_jets_1.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_8_obj_jets_2.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_8_obj_jets_3.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_8_obj_jets_4.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_8_obj_jets_5.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_8_obj_met_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_9_obj_jets_0.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_9_obj_jets_1.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_9_obj_jets_2.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_9_obj_jets_3.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_9_obj_jets_4.png\n",
      "Saved: sampling_plots/parallel_transfusion/event_9_obj_met_0.png\n"
     ]
    }
   ],
   "source": [
    "figures = sampling.make_sampling_plots(ParallelTransfusion_model,external_samples=PT_samples, cmap='BuGn', save_dir='sampling_plots/parallel_transfusion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9260f69",
   "metadata": {},
   "source": [
    "# Transfer-CFM per event sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d1837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard data batch size: 10\n",
      "Reco data batch size: 10\n",
      "Number of elements per sample: 2\n",
      "Sample 0: torch.Size([10000, 10, 6, 4])\n",
      "Sample 1: torch.Size([10000, 10, 1, 2])\n",
      "Samples saved to saved_samples/TransferCFM_samples2.pt\n"
     ]
    }
   ],
   "source": [
    "TransferCFM_samples_file = \"TransferCFM_samples2.pt\"\n",
    "if os.path.exists(os.path.join(\"saved_samples\", TransferCFM_samples_file)):\n",
    "    TransferCFM_samples = load_samples(TransferCFM_samples_file)\n",
    "else:\n",
    "    device = ParallelTransfusion_model.device\n",
    "    sampling.set_idx(sampling.idx_to_monitor)\n",
    "    hard_data = [d.to(device) for d in sampling.batch['hard']['data']]\n",
    "    hard_mask = [m.to(device) for m in sampling.batch['hard']['mask']]\n",
    "    reco_data = [d.to(device) for d in sampling.batch['reco']['data']]\n",
    "    reco_mask = [m.to(device) for m in sampling.batch['reco']['mask']]\n",
    "\n",
    "    print(f\"Hard data batch size: {hard_data[0].shape[0]}\")\n",
    "    print(f\"Reco data batch size: {reco_data[0].shape[0]}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = TransferCFM_model.to(TransferCFM_model.device)\n",
    "        TransferCFM_samples = model.sample(\n",
    "                            hard_data, hard_mask,\n",
    "                            reco_data, reco_mask,\n",
    "                            sampling.N_sample,\n",
    "                            sampling.steps,\n",
    "                            sampling.store_trajectories\n",
    "                        )\n",
    "    # Debugging prints\n",
    "    num_items = len(TransferCFM_samples)\n",
    "    print(f\"Number of elements per sample: {num_items}\")\n",
    "    for i, sample in enumerate(TransferCFM_samples):\n",
    "        print(f\"Sample {i}: {sample.shape}\")\n",
    "    save_samples(TransferCFM_samples, TransferCFM_samples_file)\n",
    "\n",
    "# 11mins 25s for 10 events, 10000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7f8513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: sampling_plots/transferCFM/event_0_obj_jets_0.png\n",
      "Saved: sampling_plots/transferCFM/event_0_obj_jets_1.png\n",
      "Saved: sampling_plots/transferCFM/event_0_obj_jets_2.png\n",
      "Saved: sampling_plots/transferCFM/event_0_obj_jets_3.png\n",
      "Saved: sampling_plots/transferCFM/event_0_obj_jets_4.png\n",
      "Saved: sampling_plots/transferCFM/event_0_obj_jets_5.png\n",
      "Saved: sampling_plots/transferCFM/event_0_obj_met_0.png\n",
      "Saved: sampling_plots/transferCFM/event_1_obj_jets_0.png\n",
      "Saved: sampling_plots/transferCFM/event_1_obj_jets_1.png\n",
      "Saved: sampling_plots/transferCFM/event_1_obj_jets_2.png\n",
      "Saved: sampling_plots/transferCFM/event_1_obj_jets_3.png\n",
      "Saved: sampling_plots/transferCFM/event_1_obj_jets_4.png\n",
      "Saved: sampling_plots/transferCFM/event_1_obj_jets_5.png\n",
      "Saved: sampling_plots/transferCFM/event_1_obj_met_0.png\n",
      "Saved: sampling_plots/transferCFM/event_2_obj_jets_0.png\n",
      "Saved: sampling_plots/transferCFM/event_2_obj_jets_1.png\n",
      "Saved: sampling_plots/transferCFM/event_2_obj_jets_2.png\n",
      "Saved: sampling_plots/transferCFM/event_2_obj_jets_3.png\n",
      "Saved: sampling_plots/transferCFM/event_2_obj_jets_4.png\n",
      "Saved: sampling_plots/transferCFM/event_2_obj_met_0.png\n",
      "Saved: sampling_plots/transferCFM/event_3_obj_jets_0.png\n",
      "Saved: sampling_plots/transferCFM/event_3_obj_jets_1.png\n",
      "Saved: sampling_plots/transferCFM/event_3_obj_jets_2.png\n",
      "Saved: sampling_plots/transferCFM/event_3_obj_jets_3.png\n",
      "Saved: sampling_plots/transferCFM/event_3_obj_jets_4.png\n",
      "Saved: sampling_plots/transferCFM/event_3_obj_jets_5.png\n",
      "Saved: sampling_plots/transferCFM/event_3_obj_met_0.png\n",
      "Saved: sampling_plots/transferCFM/event_4_obj_jets_0.png\n",
      "Saved: sampling_plots/transferCFM/event_4_obj_jets_1.png\n",
      "Saved: sampling_plots/transferCFM/event_4_obj_jets_2.png\n",
      "Saved: sampling_plots/transferCFM/event_4_obj_jets_3.png\n",
      "Saved: sampling_plots/transferCFM/event_4_obj_jets_4.png\n",
      "Saved: sampling_plots/transferCFM/event_4_obj_jets_5.png\n",
      "Saved: sampling_plots/transferCFM/event_4_obj_met_0.png\n",
      "Saved: sampling_plots/transferCFM/event_5_obj_jets_0.png\n",
      "Saved: sampling_plots/transferCFM/event_5_obj_jets_1.png\n",
      "Saved: sampling_plots/transferCFM/event_5_obj_jets_2.png\n",
      "Saved: sampling_plots/transferCFM/event_5_obj_jets_3.png\n",
      "Saved: sampling_plots/transferCFM/event_5_obj_jets_4.png\n",
      "Saved: sampling_plots/transferCFM/event_5_obj_met_0.png\n",
      "Saved: sampling_plots/transferCFM/event_6_obj_jets_0.png\n",
      "Saved: sampling_plots/transferCFM/event_6_obj_jets_1.png\n",
      "Saved: sampling_plots/transferCFM/event_6_obj_jets_2.png\n",
      "Saved: sampling_plots/transferCFM/event_6_obj_jets_3.png\n",
      "Saved: sampling_plots/transferCFM/event_6_obj_jets_4.png\n",
      "Saved: sampling_plots/transferCFM/event_6_obj_jets_5.png\n",
      "Saved: sampling_plots/transferCFM/event_6_obj_met_0.png\n",
      "Saved: sampling_plots/transferCFM/event_7_obj_jets_0.png\n",
      "Saved: sampling_plots/transferCFM/event_7_obj_jets_1.png\n",
      "Saved: sampling_plots/transferCFM/event_7_obj_jets_2.png\n",
      "Saved: sampling_plots/transferCFM/event_7_obj_jets_3.png\n",
      "Saved: sampling_plots/transferCFM/event_7_obj_jets_4.png\n",
      "Saved: sampling_plots/transferCFM/event_7_obj_jets_5.png\n",
      "Saved: sampling_plots/transferCFM/event_7_obj_met_0.png\n",
      "Saved: sampling_plots/transferCFM/event_8_obj_jets_0.png\n",
      "Saved: sampling_plots/transferCFM/event_8_obj_jets_1.png\n",
      "Saved: sampling_plots/transferCFM/event_8_obj_jets_2.png\n",
      "Saved: sampling_plots/transferCFM/event_8_obj_jets_3.png\n",
      "Saved: sampling_plots/transferCFM/event_8_obj_jets_4.png\n",
      "Saved: sampling_plots/transferCFM/event_8_obj_jets_5.png\n",
      "Saved: sampling_plots/transferCFM/event_8_obj_met_0.png\n",
      "Saved: sampling_plots/transferCFM/event_9_obj_jets_0.png\n",
      "Saved: sampling_plots/transferCFM/event_9_obj_jets_1.png\n",
      "Saved: sampling_plots/transferCFM/event_9_obj_jets_2.png\n",
      "Saved: sampling_plots/transferCFM/event_9_obj_jets_3.png\n",
      "Saved: sampling_plots/transferCFM/event_9_obj_jets_4.png\n",
      "Saved: sampling_plots/transferCFM/event_9_obj_met_0.png\n"
     ]
    }
   ],
   "source": [
    "figures = sampling.make_sampling_plots(TransferCFM_model,external_samples=TransferCFM_samples, cmap='BuPu', save_dir='sampling_plots/transferCFM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd913e",
   "metadata": {},
   "source": [
    "# Bias Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52943be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict:   0%|          | 0/19 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Sample #\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m     transfermer_bias_samples \u001b[38;5;241m=\u001b[39m \u001b[43mtransfermer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhard_mask_exist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreco_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreco_mask_exist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Feature selection\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     jets_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# ['pt', 'eta', 'phi', 'mass']\u001b[39;00m\n",
      "File \u001b[0;32m/cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/transfer_flow/transfer_flow_model.py:258\u001b[0m, in \u001b[0;36mTransferFlow.sample\u001b[0;34m(self, hard_data, hard_mask_exist, reco_data, reco_mask_exist, N)\u001b[0m\n\u001b[1;32m    256\u001b[0m reco_data_null_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_embeddings(reco_data_null)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Get condition #\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhard_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mm_enc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhard_mask_exist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreco_data_null_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mm_dec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreco_mask_exist_null\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Slice per type #\u001b[39;00m\n\u001b[1;32m    265\u001b[0m conditions \u001b[38;5;241m=\u001b[39m [condition[:,ni:nf,:] \u001b[38;5;28;01mfor\u001b[39;00m ni,nf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(slices[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],slices[\u001b[38;5;241m1\u001b[39m:])]\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/transfer_flow/tools.py:611\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x_enc, m_enc, x_dec, m_dec)\u001b[0m\n\u001b[1;32m    608\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(x_dec\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(x_enc\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Transformer processing #\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                       \u001b[49m\u001b[38;5;66;43;03m# encoder input\u001b[39;49;00m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                       \u001b[49m\u001b[38;5;66;43;03m# decorder input\u001b[39;49;00m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                               \u001b[49m\u001b[38;5;66;43;03m# triangular (causality) mask\u001b[39;49;00m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_mask_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# encoder mask\u001b[39;49;00m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_mask_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# encoder / memory mask\u001b[39;49;00m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder_mask_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# decoder mask\u001b[39;49;00m\n\u001b[1;32m    618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m condition\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/transformer.py:204\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 204\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    207\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    208\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    209\u001b[0m                       tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal, memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/torch/nn/modules/transformer.py:678\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m why_not_sparsity_fast_path:\n\u001b[1;32m    677\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mmerge_masks(src_mask, src_key_padding_mask, src)\n\u001b[0;32m--> 678\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer_encoder_layer_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_relu_or_gelu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = transfermer_model.device\n",
    "\n",
    "transfermer_bias_samples_file = \"Transfermer_bias_samples.pt\"\n",
    "if os.path.exists(os.path.join(\"saved_samples\", transfermer_bias_samples_file)):\n",
    "    transfermer_bias_samples = load_samples(transfermer_bias_samples_file)\n",
    "else:\n",
    "    for batch_idx, batch in tqdm(enumerate(bias.loader),desc='Predict',disable=False,leave=True,total=min(bias.N_batch,len(bias.loader)),position=0):\n",
    "        if batch_idx >= bias.N_batch:\n",
    "            break\n",
    "\n",
    "        # Get parts #\n",
    "        hard_data = [data.to(device) for data in batch['hard']['data']]\n",
    "        hard_mask_exist = [mask.to(device) for mask in batch['hard']['mask']]\n",
    "        reco_data = [data.to(device) for data in batch['reco']['data']]\n",
    "        reco_mask_exist = [mask.to(device) for mask in batch['reco']['mask']]\n",
    "\n",
    "        # Sample #\n",
    "        with torch.no_grad():\n",
    "            transfermer_bias_samples = transfermer_model.sample(\n",
    "                hard_data, hard_mask_exist,\n",
    "                reco_data, reco_mask_exist,\n",
    "                bias.N_sample,\n",
    "            )\n",
    "            # Feature selection\n",
    "            jets_indices = [0, 1, 2, 3]  # ['pt', 'eta', 'phi', 'mass']\n",
    "            met_indices = [0, 2]         # ['pt', 'phi']\n",
    "\n",
    "            transfermer_bias_samples[0] = transfermer_bias_samples[0][..., jets_indices]  # Filter jet features\n",
    "            transfermer_bias_samples[1] = transfermer_bias_samples[1][..., met_indices]   # Filter MET features\n",
    "\n",
    "        save_samples(transfermer_bias_samples, transfermer_bias_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fb2161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of memflow.ttH.models.ParallelTransfusion failed: Traceback (most recent call last):\n",
      "  File \"/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 365, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 319, in update_instances\n",
      "    refs = gc.get_referrers(old)\n",
      "KeyboardInterrupt\n",
      "]\n",
      "Predict: 100%|| 19/19 [00:00<00:00, 37.15it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1900000, 6, 4]' is invalid for input of size 912000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m figs \u001b[38;5;241m=\u001b[39m \u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_bias_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParallelTransfusion_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mexternal_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransfermer_bias_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cephfs/dice/users/sa21722/projects/MEM/memflow/ttH/models/callbacks.py:788\u001b[0m, in \u001b[0;36mBiasCallback.make_bias_plots\u001b[0;34m(self, model, show, disable_tqdm, external_samples)\u001b[0m\n\u001b[1;32m    775\u001b[0m         truth[i], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39minverse(\n\u001b[1;32m    776\u001b[0m             name \u001b[38;5;241m=\u001b[39m name,\n\u001b[1;32m    777\u001b[0m             x \u001b[38;5;241m=\u001b[39m truth[i],\n\u001b[1;32m    778\u001b[0m             mask \u001b[38;5;241m=\u001b[39m mask[i],\n\u001b[1;32m    779\u001b[0m             fields \u001b[38;5;241m=\u001b[39m flow_fields,\n\u001b[1;32m    780\u001b[0m         )\n\u001b[1;32m    781\u001b[0m         \u001b[38;5;66;03m# preprocessing expects :\u001b[39;00m\n\u001b[1;32m    782\u001b[0m         \u001b[38;5;66;03m#   data = [events, particles, features]\u001b[39;00m\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m#   mask = [events, particles]\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# samples dims = [samples, events, particles, features]\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# will merge samples*event and unmerge later\u001b[39;00m\n\u001b[1;32m    786\u001b[0m         samples[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39minverse(\n\u001b[1;32m    787\u001b[0m             name \u001b[38;5;241m=\u001b[39m name,\n\u001b[0;32m--> 788\u001b[0m             x \u001b[38;5;241m=\u001b[39m \u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN_sample\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    789\u001b[0m             mask \u001b[38;5;241m=\u001b[39m mask[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN_sample,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN_sample\u001b[38;5;241m*\u001b[39mmask[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],mask[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    790\u001b[0m             fields \u001b[38;5;241m=\u001b[39m flow_fields,\n\u001b[1;32m    791\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN_sample,samples[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],samples[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],samples[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Make figure plots #\u001b[39;00m\n\u001b[1;32m    794\u001b[0m figs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1900000, 6, 4]' is invalid for input of size 912000"
     ]
    }
   ],
   "source": [
    "figs = bias.make_bias_plots(ParallelTransfusion_model,show=True,external_samples=transfermer_bias_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef537ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            # Generate samples\n",
    "            batch_samples = transfermer_model.sample(\n",
    "                hard_data,\n",
    "                hard_mask,\n",
    "                reco_data,\n",
    "                reco_mask,\n",
    "                N=current_N\n",
    "            )\n",
    "\n",
    "            # Feature selection\n",
    "            jets_indices = [0, 1, 2, 3]  # ['pt', 'eta', 'phi', 'mass']\n",
    "            met_indices = [0, 2]         # ['pt', 'phi']\n",
    "\n",
    "            batch_samples[0] = batch_samples[0][..., jets_indices]  # Filter jet features\n",
    "            batch_samples[1] = batch_samples[1][..., met_indices]   # Filter MET features\n",
    "\n",
    "            # Accumulate results\n",
    "            if accumulated_samples[0] is None:\n",
    "                accumulated_samples[0] = batch_samples[0].cpu()\n",
    "                accumulated_samples[1] = batch_samples[1].cpu()\n",
    "            else:\n",
    "                accumulated_samples[0] = torch.cat((accumulated_samples[0], batch_samples[0].cpu()), dim=0)\n",
    "                accumulated_samples[1] = torch.cat((accumulated_samples[1], batch_samples[1].cpu()), dim=0)\n",
    "\n",
    "    # Convert final results to the expected format\n",
    "    transfermer_samples = accumulated_samples\n",
    "\n",
    "    # Debugging prints\n",
    "    num_items = len(transfermer_samples)\n",
    "    print(f\"Number of elements per sample: {num_items}\")\n",
    "    for i, sample in enumerate(transfermer_samples):\n",
    "        print(f\"Sample {i}: {sample.shape}\")\n",
    "\n",
    "    # Save the concatenated samples\n",
    "    save_samples(transfermer_samples, transfermer_samples_file)\n",
    "\n",
    "# 3mins 27s, 10 events, 10000 samples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
