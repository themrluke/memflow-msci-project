{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a7bc2a-42e5-48ff-8ddd-a01ed240d9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU : True\n",
      "Accelerator : gpu\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"  # Change \"<n>\" to the index of the GPU you want to use on node\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import dask\n",
    "import vector\n",
    "import particle\n",
    "import hepunits\n",
    "\n",
    "import zuko\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from memflow.dataset.data import ParquetData # Class to handle Parquet files\n",
    "from memflow.dataset.dataset import CombinedDataset\n",
    "#from memflow.HH.ttH import ttHHardDataset, ttHRecoDataset\n",
    "from memflow.dataset.tth import ttHHardDataset, ttHRecoDataset\n",
    "from memflow.callbacks.transfer_flow_callbacks import *\n",
    "from memflow.models.custom_flows import *\n",
    "vector.register_awkward() # Enables vector operations on awkward arrays\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 100})\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "print (f\"Running on GPU : {torch.cuda.is_available()}\")\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "print (f\"Accelerator : {accelerator}\")\n",
    "torch.set_float32_matmul_precision('medium')  \n",
    "if accelerator =='gpu':\n",
    "    torch.cuda.empty_cache()\n",
    "    print (torch.cuda.memory_summary(device=None, abbreviated=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da133f-7075-4028-a2b0-86d1b7e82917",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "In this section, we will load and prepare the datasets for feeding through the transfer model.\n",
    "\n",
    "Some of the steps of making particles and selecting events is already done in the classes\n",
    "- HHbbWWDoubleLeptonHardDataset\n",
    "- HHbbWWDoubleLeptonRecoDataset\n",
    "\n",
    "We will first get the gen and reco data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974efcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object\n",
      "Loaded branches:\n",
      "   ... file: 100000\n",
      "   ... sample: 100000\n",
      "   ... tree: 100000\n",
      "Branch in files not loaded:\n",
      "   ... Generator_scalePDF\n",
      "   ... Generator_weight\n",
      "   ... Generator_x1\n",
      "   ... Generator_x2\n",
      "   ... Generator_xpdf1\n",
      "   ... Generator_xpdf2\n",
      "   ... W_minus_from_antitop_eta\n",
      "   ... W_minus_from_antitop_genPartIdxMother\n",
      "   ... W_minus_from_antitop_idx\n",
      "   ... W_minus_from_antitop_mass\n",
      "   ... W_minus_from_antitop_pdgId\n",
      "   ... W_minus_from_antitop_phi\n",
      "   ... W_minus_from_antitop_pt\n",
      "   ... W_minus_from_antitop_status\n",
      "   ... W_minus_from_antitop_statusFlags\n",
      "   ... W_plus_from_top_eta\n",
      "   ... W_plus_from_top_genPartIdxMother\n",
      "   ... W_plus_from_top_idx\n",
      "   ... W_plus_from_top_mass\n",
      "   ... W_plus_from_top_pdgId\n",
      "   ... W_plus_from_top_phi\n",
      "   ... W_plus_from_top_pt\n",
      "   ... W_plus_from_top_status\n",
      "   ... W_plus_from_top_statusFlags\n",
      "   ... Z_from_higgs_eta\n",
      "   ... Z_from_higgs_genPartIdxMother\n",
      "   ... Z_from_higgs_idx\n",
      "   ... Z_from_higgs_mass\n",
      "   ... Z_from_higgs_pdgId\n",
      "   ... Z_from_higgs_phi\n",
      "   ... Z_from_higgs_pt\n",
      "   ... Z_from_higgs_status\n",
      "   ... Z_from_higgs_statusFlags\n",
      "   ... antibottom_eta\n",
      "   ... antibottom_genPartIdxMother\n",
      "   ... antibottom_idx\n",
      "   ... antibottom_mass\n",
      "   ... antibottom_pdgId\n",
      "   ... antibottom_phi\n",
      "   ... antibottom_pt\n",
      "   ... antibottom_status\n",
      "   ... antibottom_statusFlags\n",
      "   ... antineutrino_from_W_minus_eta\n",
      "   ... antineutrino_from_W_minus_genPartIdxMother\n",
      "   ... antineutrino_from_W_minus_idx\n",
      "   ... antineutrino_from_W_minus_mass\n",
      "   ... antineutrino_from_W_minus_pdgId\n",
      "   ... antineutrino_from_W_minus_phi\n",
      "   ... antineutrino_from_W_minus_pt\n",
      "   ... antineutrino_from_W_minus_status\n",
      "   ... antineutrino_from_W_minus_statusFlags\n",
      "   ... antiquark_from_W_minus_eta\n",
      "   ... antiquark_from_W_minus_genPartIdxMother\n",
      "   ... antiquark_from_W_minus_idx\n",
      "   ... antiquark_from_W_minus_mass\n",
      "   ... antiquark_from_W_minus_pdgId\n",
      "   ... antiquark_from_W_minus_phi\n",
      "   ... antiquark_from_W_minus_pt\n",
      "   ... antiquark_from_W_minus_status\n",
      "   ... antiquark_from_W_minus_statusFlags\n",
      "   ... antiquark_from_W_plus_eta\n",
      "   ... antiquark_from_W_plus_genPartIdxMother\n",
      "   ... antiquark_from_W_plus_idx\n",
      "   ... antiquark_from_W_plus_mass\n",
      "   ... antiquark_from_W_plus_pdgId\n",
      "   ... antiquark_from_W_plus_phi\n",
      "   ... antiquark_from_W_plus_pt\n",
      "   ... antiquark_from_W_plus_status\n",
      "   ... antiquark_from_W_plus_statusFlags\n",
      "   ... antitop_eta\n",
      "   ... antitop_genPartIdxMother\n",
      "   ... antitop_idx\n",
      "   ... antitop_mass\n",
      "   ... antitop_pdgId\n",
      "   ... antitop_phi\n",
      "   ... antitop_pt\n",
      "   ... antitop_status\n",
      "   ... antitop_statusFlags\n",
      "   ... bottom_eta\n",
      "   ... bottom_genPartIdxMother\n",
      "   ... bottom_idx\n",
      "   ... bottom_mass\n",
      "   ... bottom_pdgId\n",
      "   ... bottom_phi\n",
      "   ... bottom_pt\n",
      "   ... bottom_status\n",
      "   ... bottom_statusFlags\n",
      "   ... event\n",
      "   ... higgs_eta\n",
      "   ... higgs_genPartIdxMother\n",
      "   ... higgs_idx\n",
      "   ... higgs_mass\n",
      "   ... higgs_pdgId\n",
      "   ... higgs_phi\n",
      "   ... higgs_pt\n",
      "   ... higgs_status\n",
      "   ... higgs_statusFlags\n",
      "   ... lep_minus_from_W_minus_eta\n",
      "   ... lep_minus_from_W_minus_genPartIdxMother\n",
      "   ... lep_minus_from_W_minus_idx\n",
      "   ... lep_minus_from_W_minus_mass\n",
      "   ... lep_minus_from_W_minus_pdgId\n",
      "   ... lep_minus_from_W_minus_phi\n",
      "   ... lep_minus_from_W_minus_pt\n",
      "   ... lep_minus_from_W_minus_status\n",
      "   ... lep_minus_from_W_minus_statusFlags\n",
      "   ... lep_plus_from_W_plus_eta\n",
      "   ... lep_plus_from_W_plus_genPartIdxMother\n",
      "   ... lep_plus_from_W_plus_idx\n",
      "   ... lep_plus_from_W_plus_mass\n",
      "   ... lep_plus_from_W_plus_pdgId\n",
      "   ... lep_plus_from_W_plus_phi\n",
      "   ... lep_plus_from_W_plus_pt\n",
      "   ... lep_plus_from_W_plus_status\n",
      "   ... lep_plus_from_W_plus_statusFlags\n",
      "   ... neutrino_from_W_plus_eta\n",
      "   ... neutrino_from_W_plus_genPartIdxMother\n",
      "   ... neutrino_from_W_plus_idx\n",
      "   ... neutrino_from_W_plus_mass\n",
      "   ... neutrino_from_W_plus_pdgId\n",
      "   ... neutrino_from_W_plus_phi\n",
      "   ... neutrino_from_W_plus_pt\n",
      "   ... neutrino_from_W_plus_status\n",
      "   ... neutrino_from_W_plus_statusFlags\n",
      "   ... neutrinos_from_Z_eta\n",
      "   ... neutrinos_from_Z_genPartIdxMother\n",
      "   ... neutrinos_from_Z_idx\n",
      "   ... neutrinos_from_Z_mass\n",
      "   ... neutrinos_from_Z_pdgId\n",
      "   ... neutrinos_from_Z_phi\n",
      "   ... neutrinos_from_Z_pt\n",
      "   ... neutrinos_from_Z_status\n",
      "   ... neutrinos_from_Z_statusFlags\n",
      "   ... quark_from_W_minus_eta\n",
      "   ... quark_from_W_minus_genPartIdxMother\n",
      "   ... quark_from_W_minus_idx\n",
      "   ... quark_from_W_minus_mass\n",
      "   ... quark_from_W_minus_pdgId\n",
      "   ... quark_from_W_minus_phi\n",
      "   ... quark_from_W_minus_pt\n",
      "   ... quark_from_W_minus_status\n",
      "   ... quark_from_W_minus_statusFlags\n",
      "   ... quark_from_W_plus_eta\n",
      "   ... quark_from_W_plus_genPartIdxMother\n",
      "   ... quark_from_W_plus_idx\n",
      "   ... quark_from_W_plus_mass\n",
      "   ... quark_from_W_plus_pdgId\n",
      "   ... quark_from_W_plus_phi\n",
      "   ... quark_from_W_plus_pt\n",
      "   ... quark_from_W_plus_status\n",
      "   ... quark_from_W_plus_statusFlags\n",
      "   ... top_eta\n",
      "   ... top_genPartIdxMother\n",
      "   ... top_idx\n",
      "   ... top_mass\n",
      "   ... top_pdgId\n",
      "   ... top_phi\n",
      "   ... top_pt\n",
      "   ... top_status\n",
      "   ... top_statusFlags\n"
     ]
    }
   ],
   "source": [
    "data_hard = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/hard/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ], # these are the files we want to load, there can be several\n",
    "    # in parquet data there is no tree\n",
    "    lazy = True, # Explained above.\n",
    "    N = 100000, # this is to load only the N first events in the tree, \n",
    "    # in case you are just playing/debugging and don't need to load all the data (can be slow)\n",
    "    # to load all, just comment out\n",
    ")\n",
    "\n",
    "print(data_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50bb6b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object\n",
      "Loaded branches:\n",
      "   ... file: 100000\n",
      "   ... sample: 100000\n",
      "   ... tree: 100000\n",
      "Branch in files not loaded:\n",
      "   ... Generator_scalePDF\n",
      "   ... Generator_weight\n",
      "   ... Generator_x1\n",
      "   ... Generator_x2\n",
      "   ... Generator_xpdf1\n",
      "   ... Generator_xpdf2\n",
      "   ... InputMet_phi\n",
      "   ... InputMet_pt\n",
      "   ... cleanedJet_btagDeepFlavB\n",
      "   ... cleanedJet_eta\n",
      "   ... cleanedJet_mass\n",
      "   ... cleanedJet_phi\n",
      "   ... cleanedJet_pt\n",
      "   ... event\n",
      "   ... ncleanedBJet\n",
      "   ... ncleanedJet\n",
      "   ... region\n",
      "   ... weight_nominal\n",
      "   ... xs_weight\n"
     ]
    }
   ],
   "source": [
    "data_reco = ParquetData(\n",
    "    files = [\n",
    "        '/cephfs/dice/users/sa21722/datasets/MEM_data/ttH/TF_v6/reco/2018/ttH/ttH_HToInvisible_M125.parquet',\n",
    "    ],\n",
    "    lazy = True, # Explained above.\n",
    "    N = data_hard.N,\n",
    ")\n",
    "print(data_reco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ffff8e",
   "metadata": {},
   "source": [
    "Apply Mask\n",
    "\n",
    "Use mask to cut our events, only keeping the ones passing our selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63bde1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 39842 events out of 100000\n",
      "Before cut : 100000\n",
      "After cut : 39842\n"
     ]
    }
   ],
   "source": [
    "mask_hard = np.logical_and.reduce( # All conditions must be True for an event to pass the mask\n",
    "    # Each element inside the list below is a condition that needs to be met\n",
    "    [\n",
    "        # Higgs decay : H->ZZ->4nu #\n",
    "        ak.num(data_hard['higgs_idx']) == 1, # Events where there is exactly 1 Higgs particle\n",
    "        ak.num(data_hard['Z_from_higgs_idx']) == 2, # 2 Z bosons from Higgs\n",
    "        ak.num(data_hard['neutrinos_from_Z_idx']) == 4, # 2 neutrinos from each Z boson\n",
    "        # top decay : t->b q qbar #\n",
    "        ak.num(data_hard['top_idx']) == 1,\n",
    "        ak.num(data_hard['W_plus_from_top_idx']) == 1,\n",
    "        ak.num(data_hard['quark_from_W_plus_idx']) == 1,\n",
    "        ak.num(data_hard['antiquark_from_W_plus_idx']) == 1,\n",
    "        # antitop decay : tbar->bbar q qbar #\n",
    "        ak.num(data_hard['antitop_idx']) == 1,\n",
    "        ak.num(data_hard['W_minus_from_antitop_idx']) == 1,\n",
    "        ak.num(data_hard['quark_from_W_minus_idx']) == 1,\n",
    "        ak.num(data_hard['antiquark_from_W_minus_idx']) == 1,\n",
    "    ]\n",
    ")\n",
    "print (f'Selecting {mask_hard.sum()} events out of {len(mask_hard)}')\n",
    "print (f'Before cut : {data_hard.events}')\n",
    "\n",
    "data_hard.cut(mask_hard) # Apply the mask\n",
    "\n",
    "print (f'After cut : {data_hard.events}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef56a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cut:  100000 events\n",
      "After cut:  49446 events\n"
     ]
    }
   ],
   "source": [
    "mask_reco = data_reco['region'] == 0 # 0 is signal region\n",
    "print('Before cut: ', data_reco.events, 'events')\n",
    "data_reco.cut(mask_reco)\n",
    "print('After cut: ', data_reco.events, 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba90b928",
   "metadata": {},
   "outputs": [
    {
     "ename": "FieldNotFoundError",
     "evalue": "no field 'generator_info' in record with 160 fields\n\nThis error occurred while attempting to slice\n\n    <Array-typetracer [...] type='## * {event: ?uint64, Generator_weight: ?...'>\n\nwith\n\n    'generator_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFieldNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/awkward/highlevel.py:1066\u001b[0m, in \u001b[0;36mArray.__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ak\u001b[38;5;241m.\u001b[39m_errors\u001b[38;5;241m.\u001b[39mSlicingErrorContext(\u001b[38;5;28mself\u001b[39m, where):\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap_layout(\n\u001b[0;32m-> 1066\u001b[0m         prepare_layout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layout\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m]\u001b[49m),\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_behavior,\n\u001b[1;32m   1068\u001b[0m         allow_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1069\u001b[0m         attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs,\n\u001b[1;32m   1070\u001b[0m     )\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/awkward/contents/content.py:512\u001b[0m, in \u001b[0;36mContent.__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, where):\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/awkward/contents/content.py:529\u001b[0m, in \u001b[0;36mContent._getitem\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(where, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnewaxis:\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/awkward/contents/recordarray.py:458\u001b[0m, in \u001b[0;36mRecordArray._getitem_field\u001b[0;34m(self, where, only_fields)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(only_fields) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/awkward/contents/recordarray.py:394\u001b[0m, in \u001b[0;36mRecordArray.content\u001b[0;34m(self, index_or_field)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontent\u001b[39m(\u001b[38;5;28mself\u001b[39m, index_or_field: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m SupportsIndex) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Content:\n\u001b[0;32m--> 394\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_or_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length \u001b[38;5;129;01mis\u001b[39;00m unknown_length\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m unknown_length\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length\n\u001b[1;32m    399\u001b[0m     ):\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/awkward/_meta/recordmeta.py:136\u001b[0m, in \u001b[0;36mRecordMeta.content\u001b[0;34m(self, index_or_field)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index_or_field, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 136\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield_to_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_or_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/awkward/_meta/recordmeta.py:117\u001b[0m, in \u001b[0;36mRecordMeta.field_to_index\u001b[0;34m(self, field)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m i\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m FieldNotFoundError(\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno field \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m in record with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_contents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fields\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m )\n",
      "\u001b[0;31mFieldNotFoundError\u001b[0m: no field 'generator_info' in record with 160 fields",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFieldNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize HardDataset and RecoDataset for ttH process\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hard_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mttHHardDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_hard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Specify particles to be fed to ML model\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhiggs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtops\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbottoms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquarks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mZs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneutrinos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcylindrical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Choose 'cylindrical' for [pt, eta, phi, m] or 'cartesian' for [px, py, pz, E]\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply_boost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set whether or not to boost to the center of mass (CM) frame\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply_preprocessing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Re-save processed data to the specified directory\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(hard_dataset)\n",
      "File \u001b[0;32m/cephfs/dice/users/sa21722/projects/MEM/memflow/dataset/tth.py:70\u001b[0m, in \u001b[0;36mttHHardDataset.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     69\u001b[0m     ttHBase\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 70\u001b[0m     \u001b[43mHardDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cephfs/dice/users/sa21722/projects/MEM/memflow/dataset/dataset.py:798\u001b[0m, in \u001b[0;36mHardDataset.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cephfs/dice/users/sa21722/projects/MEM/memflow/dataset/dataset.py:317\u001b[0m, in \u001b[0;36mAbsDataset.__init__\u001b[0;34m(self, data, selection, default_features, build, device, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing()\n",
      "File \u001b[0;32m/cephfs/dice/users/sa21722/projects/MEM/memflow/dataset/tth.py:97\u001b[0m, in \u001b[0;36mttHHardDataset.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Make generator info #\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerator_info\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     98\u001b[0m     boost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_boost(generator\u001b[38;5;241m.\u001b[39mx1,generator\u001b[38;5;241m.\u001b[39mx2)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_object(\n\u001b[1;32m    100\u001b[0m         name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboost\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    101\u001b[0m         obj \u001b[38;5;241m=\u001b[39m boost,\n\u001b[1;32m    102\u001b[0m     )\n",
      "File \u001b[0;32m/cephfs/dice/users/sa21722/projects/MEM/memflow/dataset/data.py:141\u001b[0m, in \u001b[0;36mAbsData.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    Magic getitem method\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Looks for the keys in already loaded ones, if not found loads the branch\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    If a cut has been done before, only select the corresponding indices\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx]\n",
      "File \u001b[0;32m/cephfs/dice/users/sa21722/projects/MEM/memflow/dataset/data.py:120\u001b[0m, in \u001b[0;36mAbsData._getitem\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    118\u001b[0m arrays \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entries,tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees):\n\u001b[0;32m--> 120\u001b[0m     arrays\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentries\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    121\u001b[0m array \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mconcatenate(arrays,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array\u001b[38;5;241m.\u001b[39mlayout,ak\u001b[38;5;241m.\u001b[39mcontents\u001b[38;5;241m.\u001b[39mlistoffsetarray\u001b[38;5;241m.\u001b[39mListOffsetArray):\n",
      "File \u001b[0;32m/cephfs/dice/users/sa21722/projects/MEM/memflow/dataset/data.py:505\u001b[0m, in \u001b[0;36mParquetData.getitem\u001b[0;34m(self, entries, tree, key)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m[:entries]\u001b[38;5;241m.\u001b[39mcompute()\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/dask_awkward/lib/core.py:1588\u001b[0m, in \u001b[0;36mArray.__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(where, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(where)\n\u001b[0;32m-> 1588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/dask_awkward/lib/core.py:1527\u001b[0m, in \u001b[0;36mArray._getitem_single\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, where):\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;66;03m# a single string\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(where, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1527\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_outer_str_or_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;66;03m# an empty slice\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_empty_slice(where):\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/dask_awkward/lib/core.py:1337\u001b[0m, in \u001b[0;36mArray._getitem_outer_str_or_list\u001b[0;34m(self, where, label)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         new_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta[metad]\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(where, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m-> 1337\u001b[0m         new_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_trivial_map_partitions(where, meta\u001b[38;5;241m=\u001b[39mnew_meta, label\u001b[38;5;241m=\u001b[39mlabel)\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/awkward/highlevel.py:1064\u001b[0m, in \u001b[0;36mArray.__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, where):\n\u001b[1;32m    636\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m        where (many types supported; see below): Index of positions to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m    have the same dimension as the array being indexed.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ak\u001b[38;5;241m.\u001b[39m_errors\u001b[38;5;241m.\u001b[39mSlicingErrorContext(\u001b[38;5;28mself\u001b[39m, where):\n\u001b[1;32m   1065\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m wrap_layout(\n\u001b[1;32m   1066\u001b[0m             prepare_layout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout[where]),\n\u001b[1;32m   1067\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_behavior,\n\u001b[1;32m   1068\u001b[0m             allow_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1069\u001b[0m             attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs,\n\u001b[1;32m   1070\u001b[0m         )\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/awkward/_errors.py:85\u001b[0m, in \u001b[0;36mErrorContext.__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Handle caught exception\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m         exception_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(exception_type, \u001b[38;5;167;01mException\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     84\u001b[0m     ):\n\u001b[0;32m---> 85\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexception_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Step out of the way so that another ErrorContext can become primary.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n",
      "File \u001b[0;32m/software/sa21722/miniconda3/envs/mem-flow/lib/python3.10/site-packages/awkward/_errors.py:95\u001b[0m, in \u001b[0;36mErrorContext.handle_exception\u001b[0;34m(self, cls, exception)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecorate_exception(\u001b[38;5;28mcls\u001b[39m, exception)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecorate_exception(\u001b[38;5;28mcls\u001b[39m, exception)\n",
      "\u001b[0;31mFieldNotFoundError\u001b[0m: no field 'generator_info' in record with 160 fields\n\nThis error occurred while attempting to slice\n\n    <Array-typetracer [...] type='## * {event: ?uint64, Generator_weight: ?...'>\n\nwith\n\n    'generator_info'"
     ]
    }
   ],
   "source": [
    "# Initialize HardDataset and RecoDataset for ttH process\n",
    "hard_dataset = ttHHardDataset(\n",
    "    data = data_hard,\n",
    "    selection=[     # Specify particles to be fed to ML model\n",
    "        'higgs',\n",
    "        'tops',\n",
    "        'bottoms',\n",
    "        'Ws',\n",
    "        'quarks',\n",
    "        'Zs',\n",
    "        'neutrinos'\n",
    "    ],\n",
    "    coordinates = 'cylindrical', # Choose 'cylindrical' for [pt, eta, phi, m] or 'cartesian' for [px, py, pz, E]\n",
    "    apply_boost = False,  # Set whether or not to boost to the center of mass (CM) frame\n",
    "    apply_preprocessing = True,\n",
    "    build = True,  # Re-save processed data to the specified directory\n",
    "    dtype = torch.float32,\n",
    ")\n",
    "\n",
    "print(hard_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_dataset = ttHRecoDataset(\n",
    "    data = data_reco,\n",
    "    selection=[     # Include reconstructed particles: jets and MET\n",
    "        'jets',\n",
    "        'met'\n",
    "    ],\n",
    "    coordinates = 'cylindrical',\n",
    "    apply_boost = False,\n",
    "    apply_preprocessing = True,\n",
    "    build = True,\n",
    "    dtype = torch.float32,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99adb682",
   "metadata": {},
   "source": [
    "Just to understand a bit better what is in our dataset, we can plot the different quantities for each partycle/type.\n",
    "\n",
    "We can plot both the raw values, and the ones after pre-processing.\n",
    "\n",
    "About preprocessing, for several reasons we do not want to feed the raw values to the ML model. \n",
    "So we apply different preprocessing schemes (apply a log, standard scaling, min-max scaling, etc), but we still want to check their effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Hard-level')\n",
    "print ('Before preprocessing')\n",
    "\n",
    "hard_dataset.plot(selection=True, raw=True)\n",
    "print ('After preprocessing')\n",
    "hard_dataset.plot(selection=True,raw=False)\n",
    "\n",
    "print ('Reco-level')\n",
    "print ('Before preprocessing')\n",
    "reco_dataset.plot(selection=True,raw=True)\n",
    "print ('After preprocessing')\n",
    "reco_dataset.plot(selection=True,raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd8d60-f3d9-4a88-9b44-f8450e18b70c",
   "metadata": {},
   "source": [
    "Now, we want to combine the reco and gen information.\n",
    "\n",
    "To avoid doing it by hand, a combined dataset has been created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a63d7-eafc-43ef-8147-d0887d4bceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_dataset = CombinedDataset(\n",
    "    hard_dataset = hard_dataset,\n",
    "    reco_dataset = reco_dataset,\n",
    "    intersection_branch = 'event',\n",
    ")\n",
    "print (comb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acc7c1-02e8-4c7f-8354-69090de0a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check what is contained in all our batches, we make a classic pytorch loader, and print the content\n",
    "loader_comb = DataLoader(\n",
    "    comb_dataset,\n",
    "    batch_size = 256,\n",
    ")\n",
    "batch = next(iter(loader_comb))\n",
    "\n",
    "print ('Reco')\n",
    "for obj,mask,sel in zip(batch['reco']['data'],batch['reco']['mask'],reco_dataset.selection):\n",
    "    print ('\\t',sel,obj.shape,mask.shape)\n",
    "print ('Hard')\n",
    "for obj,mask,sel in zip(batch['hard']['data'],batch['hard']['mask'],hard_dataset.selection):\n",
    "    print ('\\t',sel,obj.shape,mask.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5ba62-3a9a-41d8-aa38-5f99bf8abbc5",
   "metadata": {},
   "source": [
    "The batches that we provide to the loader contain the different types of particles for reco and gen level.\n",
    "The shape is `[N,S,F]`\n",
    "- `N` : number of events in the batch (this is the batch size that you decide yourself)\n",
    "- `S` : sequence length = number of particles (eg, number of electrons, or jets) (see explanation of mask below)\n",
    "- `F` : number of features (eg, pt, eta, pdgid, btag score, etc) for each particle\n",
    "\n",
    "For each type of particle we provide the data (actual values in the training) with shape above, but also the mask.\n",
    "This mask comes from the fact that we need to feed a fixed-size tensor to the ML model, but we do not have fixed size inputs (number of jets can vary for example). So we \"pad\" the tensors with 0s, but the fact that these particles are missing are saved in the mask that has False's. In the transformer this will be used in the self-attention. The mask has a shape `[N,S]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6d738-34cd-4ea4-9125-ca0e82dea403",
   "metadata": {},
   "source": [
    "Now we need to split the dataset into a training and validation dataset, and make them into dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6a92f-5b59-4062-bedb-fb3e7078356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation\n",
    "# Not randomly for reproducilibility, but just based on number\n",
    "\n",
    "train_frac = 0.9\n",
    "indices = torch.arange(len(comb_dataset))\n",
    "sep = int(train_frac*len(comb_dataset))\n",
    "train_indices = indices[:sep]\n",
    "valid_indices = indices[sep:]\n",
    "\n",
    "comb_dataset_train = torch.utils.data.Subset(comb_dataset,train_indices)\n",
    "comb_dataset_valid = torch.utils.data.Subset(comb_dataset,valid_indices)\n",
    "\n",
    "print (f'There will be {len(comb_dataset_train)} events in the training dataset, and {len(comb_dataset_valid)} in the validation set')\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "loader_comb_train = DataLoader(\n",
    "    comb_dataset_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    ")\n",
    "loader_comb_valid = DataLoader(\n",
    "    comb_dataset_valid,\n",
    "    batch_size = 10000,\n",
    "    shuffle = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc95f0-3ab1-474e-ae5a-a7b83d092139",
   "metadata": {},
   "source": [
    "# Creating the model\n",
    "\n",
    "A specific pytorch class has been coded for the transfer flow\n",
    "\n",
    "Most of these arguments would require a lot of text, so will give some cues but can explain later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42338669-b470-453c-a0ad-9c02de2ba6ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from memflow.models.transfer_flow_model import TransferFlow\n",
    "\n",
    "model = TransferFlow(\n",
    "    # Embedding arguments #\n",
    "    embed_dims = [32,64], # number of neurons per layer to embed the particle components to higher dimension\n",
    "    embed_act = nn.GELU,\n",
    "    # Particle features, names, masks, and number for printouts and logging #\n",
    "    n_hard_particles_per_type = hard_dataset.number_particles_per_type,\n",
    "    hard_particle_type_names = hard_dataset.selection,\n",
    "    hard_input_features_per_type = hard_dataset.input_features,\n",
    "    n_reco_particles_per_type = reco_dataset.number_particles_per_type,\n",
    "    reco_particle_type_names = reco_dataset.selection,\n",
    "    reco_input_features_per_type = reco_dataset.input_features,\n",
    "    flow_input_features = [ # features to be used in the flows (different from the tranformer)\n",
    "        ['pt','phi'],       # met\n",
    "        ['pt','eta','phi'], # jets\n",
    "    ],\n",
    "    reco_mask_attn = reco_dataset.attention_mask,\n",
    "    hard_mask_attn = hard_dataset.attention_mask,\n",
    "    # Transformer arguments #\n",
    "    onehot_encoding = True, # add onehot encoded position vector to features\n",
    "    transformer_args = { # to be passed to the Transformer pytorch class\n",
    "        'nhead' : 8,\n",
    "        'num_encoder_layers' : 8, \n",
    "        'num_decoder_layers' : 8, \n",
    "        'dim_feedforward' : 256, \n",
    "        'dropout' : 0., \n",
    "        'activation' : 'gelu', \n",
    "    },\n",
    "    # Flow args #\n",
    "    flow_common_args = { # common args for all flows\n",
    "        'bins' : 32,\n",
    "        'transforms' : 5,\n",
    "        'randperm' : True,\n",
    "        'passes' : None,\n",
    "        'hidden_features' : [128] * 3,   \n",
    "    },\n",
    "    flow_classes = { # classes for each feature\n",
    "        'pt'  : zuko.flows.NSF,\n",
    "        'eta' : UniformNSF,\n",
    "        'phi' : UniformNCSF,\n",
    "        'pt'  : zuko.flows.NSF,\n",
    "    },\n",
    "    flow_specific_args = { # specific args for each class above\n",
    "        'eta' : {'bound' : 1.},\n",
    "        'phi' : {'bound' : 1.},\n",
    "    },\n",
    "    flow_mode = 'global', # 'global', 'type' or 'particle'\n",
    ")\n",
    "model = model.cpu()\n",
    "\n",
    "# Below are some testing of individual workings of the model\n",
    "# This is just to make sure there is no bug before running the training loop\n",
    "\n",
    "# Testing the model #\n",
    "# We get a single batch from the loader\n",
    "batch = next(iter(loader_comb))\n",
    "# This returns the -log(prob), the combined mask and weights\n",
    "# They all have shape [N,S], as we have a log prob per particle\n",
    "log_probs, mask, weights = model(batch)\n",
    "print ('log_probs',log_probs,log_probs.shape)\n",
    "print ('mask',mask,mask.shape)\n",
    "print ('weights',weights,weights.shape)\n",
    "\n",
    "# Separately test to get the total log prob #\n",
    "log_probs_tot = model.shared_eval(batch,0,'test')\n",
    "print ('tot log probs',log_probs_tot)\n",
    "\n",
    "# Separately test sampling the model #\n",
    "samples = model.sample(batch['hard']['data'],batch['hard']['mask'],batch['reco']['data'],batch['reco']['mask'],N=100)\n",
    "print ('samples')\n",
    "for sample in samples:\n",
    "    print ('\\t',sample.shape)\n",
    "\n",
    "print (model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141d655-7c98-4c7c-b3f7-95c556db5c88",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "We use [lightning](https://lightning.ai/docs/pytorch/stable/) to remove some boilerplate code, mostly during training.\n",
    "\n",
    "The training is done through a trainer (see below). To log all the interesting quantities (and plots, will do that later), we use [comet](https://www.comet.com) (you will probably have to create an account, or link your github account).\n",
    "\n",
    "This tool is pretty cool to plot the different metrics, I can pass to you a setup later. If you want to avoid this, comment out the `logger` part.\n",
    "\n",
    "All the following code are tools for the training, will dedicate a discussion later on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f1a44-c9ba-4e45-a354-5d95bb3b5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Parameters #####\n",
    "epochs = 10 # use small number for testing here, in practice would use in the hundreds\n",
    "steps_per_epoch_train = math.ceil(len(comb_dataset_train)/loader_comb_train.batch_size)\n",
    "\n",
    "print (f'Training   : Batch size = {loader_comb_train.batch_size} => {steps_per_epoch_train} steps per epoch')\n",
    "\n",
    "##### Optimizer #####\n",
    "optimizer = optim.RAdam(model.parameters(), lr=1e-3)\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "##### Scheduler #####\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer = optimizer,\n",
    "    mode='min', \n",
    "    factor=0.1, \n",
    "    patience=10, \n",
    "    threshold=0.001, \n",
    "    threshold_mode='rel', \n",
    "    cooldown=5, \n",
    "    min_lr=1e-7\n",
    ")\n",
    "model.set_scheduler_config(\n",
    "    {\n",
    "        'scheduler' : scheduler,\n",
    "        'interval' : 'step' if isinstance(scheduler,optim.lr_scheduler.OneCycleLR) else 'epoch',\n",
    "        'frequency' : 1,\n",
    "        'monitor' : 'val/loss',\n",
    "        'strict' : True,\n",
    "        'name' : 'scheduler',\n",
    "    }\n",
    ")\n",
    "\n",
    "##### Callbacks #####\n",
    "callbacks = [\n",
    "    L.pytorch.callbacks.LearningRateMonitor(logging_interval = 'epoch'),\n",
    "    L.pytorch.callbacks.ModelSummary(max_depth=2),\n",
    "] \n",
    "\n",
    "# ##### Logger #####\n",
    "# logger = pl_loggers.CometLogger(\n",
    "#     save_dir = '../comet_logs',\n",
    "#     project_name = 'mem-flow-HH',\n",
    "#     experiment_name = 'HH',\n",
    "#     offline = False,\n",
    "# ) \n",
    "# logger.log_graph(model)\n",
    "# logger.experiment.log_notebook(filename=globals()['__session__'],overwrite=True)\n",
    "\n",
    "##### Trainer #####\n",
    "trainer = L.Trainer(    \n",
    "    min_epochs = 5,\n",
    "    max_epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    devices = 'auto',\n",
    "    accelerator = accelerator,\n",
    "    #logger = logger,\n",
    "    log_every_n_steps = steps_per_epoch_train,\n",
    ")\n",
    "##### Fit #####\n",
    "trainer.fit(\n",
    "    model = model, \n",
    "    train_dataloaders = loader_comb_train,\n",
    "    val_dataloaders = loader_comb_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337aca6-4faa-47f6-aa45-18e19bdb95df",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "We want to make sure the model is doing a good job.\n",
    "\n",
    "Because our model is used to compute the log probability, it can be a bit hard to represent. Instead what we can use is a sampling of this pdf through sampling with the flow. \n",
    "\n",
    "We will plot two cases\n",
    "- Sampling per event : we select one event specifically, and sample N times and show the distribution\n",
    "- Sampling bias : for all events, we sample N times, and investigate the bias\n",
    "\n",
    "We will show both cases below.\n",
    "\n",
    "Note : here we do plots post training, but these classes below are made to be used as callbacks. This means they can be called automatically during training (typially at the end of the validation for each epoch). The resulting plots will then be logged to comet and be saved online.\n",
    "\n",
    "Here we will just use them to plot after the training, but you can pass the class instances in the training callbacks above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76563be6-84f7-4c6f-b943-b8133b6cdda6",
   "metadata": {},
   "source": [
    "## Per-event sampling\n",
    "\n",
    "We want to see what the pdf looks like for event.\n",
    "\n",
    "To do that we sample N times for a specific event, and compare the sampled values to the true ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684f06a-f33f-41bb-866f-0729c8bee9cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model.cuda() # we make sure model is on GPU (to make it faster)\n",
    "sampling = SamplingCallback(\n",
    "    dataset = comb_dataset,\n",
    "    idx_to_monitor = torch.arange(5), # show the first five events (you can use specific events you want to look at)\n",
    "    N_sample = 100000,  # number of samples per event (this should be high, but also takes longer)\n",
    "    frequency = 10, # callback parameter : frequency of epochs to record (1 = all epochs)\n",
    "    raw = True, # if True, will undo the preprocessing\n",
    "    bins = 101, # bins for the plot\n",
    "    log_scale = True, # log scale for the plot\n",
    ")\n",
    "figures = sampling.make_sampling_plots(model,show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80779df3-91a1-4ebd-90d7-1fe717144e8e",
   "metadata": {},
   "source": [
    "## Bias plotting\n",
    "\n",
    "We want to make sure the sampling (and therefore the pdf) is not biased.\n",
    "\n",
    "To do so, we sample N times for each event, and compute the different sample-true.\n",
    "\n",
    "This will be plotted in 1D, as a function of the true value, and then estimating the bias coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672e84e-ca2f-42d2-ab87-8e2b57aa67c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model.cuda() # we make sure model is on GPU (to make it faster)\n",
    "bias = BiasCallback(\n",
    "    dataset = comb_dataset,\n",
    "    N_sample = 50, # if very high, will be very slow\n",
    "    batch_size = 10000, # compute certain number of events at a time (all events would break RAM)\n",
    "    # N_batch = 1, # Limits the number of batches to process (eg =1 to make it faster), comment out to use all of them\n",
    "    frequency = 10, # callback parameter : frequency of epochs to record (1 = all epochs)\n",
    "    raw = True, # if True, will undo the preprocessing\n",
    "    bins = 101, # plot bins\n",
    "    points = 20, # number of points for the bias plot\n",
    "    log_scale = True, # log scale for the plot\n",
    ")\n",
    "figs = bias.make_bias_plots(model,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b1fd2-37b0-42a4-aa87-cfa8a12165b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
